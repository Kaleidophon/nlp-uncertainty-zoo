<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>nlp_uncertainty_zoo.utils.task_eval &#8212; nlp-uncertainty-zoo 0.9.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/bootstrap-sphinx.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="nlp_uncertainty_zoo.utils.uncertainty_eval" href="nlp_uncertainty_zoo.utils.uncertainty_eval.html" />
    <link rel="prev" title="nlp_uncertainty_zoo.utils.samplers" href="nlp_uncertainty_zoo.utils.samplers.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="_static/js/jquery-1.12.4.min.js"></script>
<script type="text/javascript" src="_static/js/jquery-fix.js"></script>
<script type="text/javascript" src="_static/bootstrap-3.4.1/js/bootstrap.min.js"></script>
<script type="text/javascript" src="_static/bootstrap-sphinx.js"></script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="index.html">
          nlp-uncertainty-zoo</a>
        <span class="navbar-text navbar-version pull-left"><b>0.9.0</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.html">nlp_uncertainty_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.html">Model package</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.bayesian_lstm.html">Bayesian LSTM</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.bayesian_lstm.html#bayesian-lstm-module-documentation">Bayesian LSTM Module Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.bert.html#module-nlp_uncertainty_zoo.models.bert">BERT Module Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.ddu_transformer.html">DDU Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.ddu_transformer.html#module-nlp_uncertainty_zoo.models.ddu_transformer">DDU Transformer Module Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.dpp_transformer.html">DPP Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.dpp_transformer.html#module-nlp_uncertainty_zoo.models.dpp_transformer">DPP Transformer Module Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.lstm.html">LSTM</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.lstm.html#module-nlp_uncertainty_zoo.models.lstm">LSTM Module Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.lstm_ensemble.html">LSTM Ensemble</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.lstm_ensemble.html#module-nlp_uncertainty_zoo.models.lstm_ensemble">LSTM Ensemble Module Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.model.html#the-module-class">The <cite>Module</cite> class</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.model.html#the-model-class">The <cite>Model</cite> class</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.model.html#module-nlp_uncertainty_zoo.models">Models Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.sngp_transformer.html">nlp_uncertainty_zoo.models.sngp_transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.spectral.html">nlp_uncertainty_zoo.models.spectral</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.st_tau_lstm.html">nlp_uncertainty_zoo.models.st_tau_lstm</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.transformer.html">nlp_uncertainty_zoo.models.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.st_tau_lstm.html">nlp_uncertainty_zoo.models.st_tau_lstm</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.transformer.html">nlp_uncertainty_zoo.models.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.variational_lstm.html">nlp_uncertainty_zoo.models.variational_lstm</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.variational_transformer.html">nlp_uncertainty_zoo.models.variational_transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.utils.html">nlp_uncertainty_zoo.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.utils.custom_types.html">nlp_uncertainty_zoo.utils.custom_types</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.utils.data.html">nlp_uncertainty_zoo.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.utils.metrics.html">nlp_uncertainty_zoo.utils.metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html">nlp_uncertainty_zoo.utils.samplers</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">nlp_uncertainty_zoo.utils.task_eval</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.utils.uncertainty_eval.html">nlp_uncertainty_zoo.utils.uncertainty_eval</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">nlp_uncertainty_zoo.utils.task_eval</a><ul>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader"><code class="docutils literal notranslate"><span class="pre">DataLoader</span></code></a><ul>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.batch_size"><code class="docutils literal notranslate"><span class="pre">DataLoader.batch_size</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.check_worker_number_rationality"><code class="docutils literal notranslate"><span class="pre">DataLoader.check_worker_number_rationality()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.dataset"><code class="docutils literal notranslate"><span class="pre">DataLoader.dataset</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.drop_last"><code class="docutils literal notranslate"><span class="pre">DataLoader.drop_last</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.multiprocessing_context"><code class="docutils literal notranslate"><span class="pre">DataLoader.multiprocessing_context</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.num_workers"><code class="docutils literal notranslate"><span class="pre">DataLoader.num_workers</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.pin_memory"><code class="docutils literal notranslate"><span class="pre">DataLoader.pin_memory</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.prefetch_factor"><code class="docutils literal notranslate"><span class="pre">DataLoader.prefetch_factor</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.sampler"><code class="docutils literal notranslate"><span class="pre">DataLoader.sampler</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.timeout"><code class="docutils literal notranslate"><span class="pre">DataLoader.timeout</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.Dict"><code class="docutils literal notranslate"><span class="pre">Dict</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase</span></code></a><ul>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.as_target_tokenizer"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.as_target_tokenizer()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.batch_decode"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.batch_decode()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.batch_encode_plus"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.batch_encode_plus()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.build_inputs_with_special_tokens"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.build_inputs_with_special_tokens()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.clean_up_tokenization"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.clean_up_tokenization()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.convert_tokens_to_string"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.convert_tokens_to_string()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.create_token_type_ids_from_sequences"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.create_token_type_ids_from_sequences()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.decode"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.decode()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.encode"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.encode()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.encode_plus"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.encode_plus()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.from_pretrained"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.from_pretrained()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.get_special_tokens_mask"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.get_special_tokens_mask()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.get_vocab"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.get_vocab()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.max_len_sentences_pair"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.max_len_sentences_pair</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.max_len_single_sentence"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.max_len_single_sentence</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.max_model_input_sizes"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.max_model_input_sizes</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.model_input_names"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.model_input_names</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.num_special_tokens_to_add"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.num_special_tokens_to_add()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.pad"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.pad()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.padding_side"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.padding_side</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.prepare_for_model"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.prepare_for_model()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.prepare_seq2seq_batch"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.prepare_seq2seq_batch()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.pretrained_init_configuration"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.pretrained_init_configuration</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.pretrained_vocab_files_map"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.pretrained_vocab_files_map</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.save_pretrained"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.save_pretrained()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.save_vocabulary"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.save_vocabulary()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.slow_tokenizer_class"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.slow_tokenizer_class</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.tokenize"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.tokenize()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.truncate_sequences"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.truncate_sequences()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.vocab_files_names"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.vocab_files_names</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.accuracy_score"><code class="docutils literal notranslate"><span class="pre">accuracy_score()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.defaultdict"><code class="docutils literal notranslate"><span class="pre">defaultdict</span></code></a><ul>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.defaultdict.copy"><code class="docutils literal notranslate"><span class="pre">defaultdict.copy()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.defaultdict.default_factory"><code class="docutils literal notranslate"><span class="pre">defaultdict.default_factory</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.evaluate"><code class="docutils literal notranslate"><span class="pre">evaluate()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.f1_score"><code class="docutils literal notranslate"><span class="pre">f1_score()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.nll_loss"><code class="docutils literal notranslate"><span class="pre">nll_loss()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.rearrange"><code class="docutils literal notranslate"><span class="pre">rearrange()</span></code></a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
              
                
  <li>
    <a href="nlp_uncertainty_zoo.utils.samplers.html" title="Previous Chapter: nlp_uncertainty_zoo.utils.samplers"><span class="glyphicon glyphicon-chevron-left visible-sm"></span><span class="hidden-sm hidden-tablet">&laquo; nlp_uncertain...</span>
    </a>
  </li>
  <li>
    <a href="nlp_uncertainty_zoo.utils.uncertainty_eval.html" title="Next Chapter: nlp_uncertainty_zoo.utils.uncertainty_eval"><span class="glyphicon glyphicon-chevron-right visible-sm"></span><span class="hidden-sm hidden-tablet">nlp_uncertain... &raquo;</span>
    </a>
  </li>
              
            
            
            
            
              <li class="hidden-sm">
<div id="sourcelink">
  <a href="_sources/nlp_uncertainty_zoo.utils.task_eval.rst.txt"
     rel="nofollow">Source</a>
</div></li>
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <section id="nlp-uncertainty-zoo-utils-task-eval">
<h1>nlp_uncertainty_zoo.utils.task_eval<a class="headerlink" href="#nlp-uncertainty-zoo-utils-task-eval" title="Permalink to this heading">¶</a></h1>
<span class="target" id="module-nlp_uncertainty_zoo.utils.task_eval"></span><p>Implementation of evaluation logic.</p>
<dl class="py class">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.DataLoader">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.task_eval.</span></span><span class="sig-name descname"><span class="pre">DataLoader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dataset</span><span class="p"><span class="pre">[</span></span><span class="pre">T_co</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampler</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.Sampler" title="torch.utils.data.sampler.Sampler"><span class="pre">Sampler</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_sampler</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.Sampler" title="torch.utils.data.sampler.Sampler"><span class="pre">Sampler</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">collate_fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">T</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_last</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">worker_init_fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multiprocessing_context</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefetch_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">persistent_workers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Generic</span></code>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">T_co</span></code>]</p>
<p>Data loader. Combines a dataset and a sampler, and provides an iterable over
the given dataset.</p>
<p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code> supports both map-style and
iterable-style datasets with single- or multi-process loading, customizing
loading order and optional automatic batching (collation) and memory pinning.</p>
<p>See <code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.utils.data</span></code> documentation page for more details.</p>
<dl>
<dt>Args:</dt><dd><p>dataset (Dataset): dataset from which to load the data.
batch_size (int, optional): how many samples per batch to load</p>
<blockquote>
<div><p>(default: <code class="docutils literal notranslate"><span class="pre">1</span></code>).</p>
</div></blockquote>
<dl class="simple">
<dt>shuffle (bool, optional): set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to have the data reshuffled</dt><dd><p>at every epoch (default: <code class="docutils literal notranslate"><span class="pre">False</span></code>).</p>
</dd>
<dt>sampler (Sampler or Iterable, optional): defines the strategy to draw</dt><dd><p>samples from the dataset. Can be any <code class="docutils literal notranslate"><span class="pre">Iterable</span></code> with <code class="docutils literal notranslate"><span class="pre">__len__</span></code>
implemented. If specified, <code class="xref py py-attr docutils literal notranslate"><span class="pre">shuffle</span></code> must not be specified.</p>
</dd>
<dt>batch_sampler (Sampler or Iterable, optional): like <a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.sampler" title="nlp_uncertainty_zoo.utils.task_eval.DataLoader.sampler"><code class="xref py py-attr docutils literal notranslate"><span class="pre">sampler</span></code></a>, but</dt><dd><p>returns a batch of indices at a time. Mutually exclusive with
<a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.batch_size" title="nlp_uncertainty_zoo.utils.task_eval.DataLoader.batch_size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">batch_size</span></code></a>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">shuffle</span></code>, <a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.sampler" title="nlp_uncertainty_zoo.utils.task_eval.DataLoader.sampler"><code class="xref py py-attr docutils literal notranslate"><span class="pre">sampler</span></code></a>,
and <a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.drop_last" title="nlp_uncertainty_zoo.utils.task_eval.DataLoader.drop_last"><code class="xref py py-attr docutils literal notranslate"><span class="pre">drop_last</span></code></a>.</p>
</dd>
<dt>num_workers (int, optional): how many subprocesses to use for data</dt><dd><p>loading. <code class="docutils literal notranslate"><span class="pre">0</span></code> means that the data will be loaded in the main process.
(default: <code class="docutils literal notranslate"><span class="pre">0</span></code>)</p>
</dd>
<dt>collate_fn (callable, optional): merges a list of samples to form a</dt><dd><p>mini-batch of Tensor(s).  Used when using batched loading from a
map-style dataset.</p>
</dd>
<dt>pin_memory (bool, optional): If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the data loader will copy Tensors</dt><dd><p>into CUDA pinned memory before returning them.  If your data elements
are a custom type, or your <code class="xref py py-attr docutils literal notranslate"><span class="pre">collate_fn</span></code> returns a batch that is a custom type,
see the example below.</p>
</dd>
<dt>drop_last (bool, optional): set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to drop the last incomplete batch,</dt><dd><p>if the dataset size is not divisible by the batch size. If <code class="docutils literal notranslate"><span class="pre">False</span></code> and
the size of dataset is not divisible by the batch size, then the last batch
will be smaller. (default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p>
</dd>
<dt>timeout (numeric, optional): if positive, the timeout value for collecting a batch</dt><dd><p>from workers. Should always be non-negative. (default: <code class="docutils literal notranslate"><span class="pre">0</span></code>)</p>
</dd>
<dt>worker_init_fn (callable, optional): If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, this will be called on each</dt><dd><p>worker subprocess with the worker id (an int in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">num_workers</span> <span class="pre">-</span> <span class="pre">1]</span></code>) as
input, after seeding and before data loading. (default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p>
</dd>
<dt>generator (torch.Generator, optional): If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, this RNG will be used</dt><dd><p>by RandomSampler to generate random indexes and multiprocessing to generate
<cite>base_seed</cite> for workers. (default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p>
</dd>
<dt>prefetch_factor (int, optional, keyword-only arg): Number of samples loaded</dt><dd><p>in advance by each worker. <code class="docutils literal notranslate"><span class="pre">2</span></code> means there will be a total of
2 * num_workers samples prefetched across all workers. (default: <code class="docutils literal notranslate"><span class="pre">2</span></code>)</p>
</dd>
<dt>persistent_workers (bool, optional): If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the data loader will not shutdown</dt><dd><p>the worker processes after a dataset has been consumed once. This allows to
maintain the workers <cite>Dataset</cite> instances alive. (default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">spawn</span></code> start method is used, <code class="xref py py-attr docutils literal notranslate"><span class="pre">worker_init_fn</span></code>
cannot be an unpicklable object, e.g., a lambda function. See
<span class="xref std std-ref">multiprocessing-best-practices</span> on more details related
to multiprocessing in PyTorch.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">len(dataloader)</span></code> heuristic is based on the length of the sampler used.
When <a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.dataset" title="nlp_uncertainty_zoo.utils.task_eval.DataLoader.dataset"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dataset</span></code></a> is an <code class="xref py py-class docutils literal notranslate"><span class="pre">IterableDataset</span></code>,
it instead returns an estimate based on <code class="docutils literal notranslate"><span class="pre">len(dataset)</span> <span class="pre">/</span> <span class="pre">batch_size</span></code>, with proper
rounding depending on <a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.drop_last" title="nlp_uncertainty_zoo.utils.task_eval.DataLoader.drop_last"><code class="xref py py-attr docutils literal notranslate"><span class="pre">drop_last</span></code></a>, regardless of multi-process loading
configurations. This represents the best guess PyTorch can make because PyTorch
trusts user <a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.dataset" title="nlp_uncertainty_zoo.utils.task_eval.DataLoader.dataset"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dataset</span></code></a> code in correctly handling multi-process
loading to avoid duplicate data.</p>
<p>However, if sharding results in multiple workers having incomplete last batches,
this estimate can still be inaccurate, because (1) an otherwise complete batch can
be broken into multiple ones and (2) more than one batch worth of samples can be
dropped when <a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.drop_last" title="nlp_uncertainty_zoo.utils.task_eval.DataLoader.drop_last"><code class="xref py py-attr docutils literal notranslate"><span class="pre">drop_last</span></code></a> is set. Unfortunately, PyTorch can not detect such
cases in general.</p>
<p>See <a href="#id12"><span class="problematic" id="id13">`Dataset Types`_</span></a> for more details on these two types of datasets and how
<code class="xref py py-class docutils literal notranslate"><span class="pre">IterableDataset</span></code> interacts with
<a href="#id14"><span class="problematic" id="id15">`Multi-process data loading`_</span></a>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>See <span class="xref std std-ref">reproducibility</span>, and <span class="xref std std-ref">dataloader-workers-random-seed</span>, and
<span class="xref std std-ref">data-loading-randomness</span> notes for random seed related questions.</p>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.DataLoader.batch_size">
<span class="sig-name descname"><span class="pre">batch_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.batch_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.DataLoader.check_worker_number_rationality">
<span class="sig-name descname"><span class="pre">check_worker_number_rationality</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.check_worker_number_rationality" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.DataLoader.dataset">
<span class="sig-name descname"><span class="pre">dataset</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dataset</span><span class="p"><span class="pre">[</span></span><span class="pre">T_co</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.dataset" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.DataLoader.drop_last">
<span class="sig-name descname"><span class="pre">drop_last</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.drop_last" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.DataLoader.multiprocessing_context">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">multiprocessing_context</span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.multiprocessing_context" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.DataLoader.num_workers">
<span class="sig-name descname"><span class="pre">num_workers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.num_workers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.DataLoader.pin_memory">
<span class="sig-name descname"><span class="pre">pin_memory</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.pin_memory" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.DataLoader.prefetch_factor">
<span class="sig-name descname"><span class="pre">prefetch_factor</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.prefetch_factor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.DataLoader.sampler">
<span class="sig-name descname"><span class="pre">sampler</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.Sampler" title="torch.utils.data.sampler.Sampler"><span class="pre">Sampler</span></a></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.sampler" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.DataLoader.timeout">
<span class="sig-name descname"><span class="pre">timeout</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader.timeout" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.Dict">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.task_eval.</span></span><span class="sig-name descname"><span class="pre">Dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">)</span> <span class="pre">-&gt;</span> <span class="pre">new</span> <span class="pre">empty</span> <span class="pre">dictionary</span> <span class="pre">dict(mapping)</span> <span class="pre">-&gt;</span> <span class="pre">new</span> <span class="pre">dictionary</span> <span class="pre">initialized</span> <span class="pre">from</span> <span class="pre">a</span> <span class="pre">mapping</span> <span class="pre">object's</span> <span class="pre">(key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value)</span> <span class="pre">pairs</span> <span class="pre">dict(iterable)</span> <span class="pre">-&gt;</span> <span class="pre">new</span> <span class="pre">dictionary</span> <span class="pre">initialized</span> <span class="pre">as</span> <span class="pre">if</span> <span class="pre">via:</span> <span class="pre">d</span> <span class="pre">=</span> <span class="pre">{}</span> <span class="pre">for</span> <span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span> <span class="pre">in</span> <span class="pre">iterable:</span> <span class="pre">d[k]</span> <span class="pre">=</span> <span class="pre">v</span> <span class="pre">dict(**kwargs)</span> <span class="pre">-&gt;</span> <span class="pre">new</span> <span class="pre">dictionary</span> <span class="pre">initialized</span> <span class="pre">with</span> <span class="pre">the</span> <span class="pre">name=value</span> <span class="pre">pairs</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">keyword</span> <span class="pre">argument</span> <span class="pre">list.</span>&#160; <span class="pre">For</span> <span class="pre">example:</span>&#160; <span class="pre">dict(one=1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">two=2</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.Dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">MutableMapping</span></code>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">KT</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">VT</span></code>]</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.task_eval.</span></span><span class="sig-name descname"><span class="pre">PreTrainedTokenizerBase</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">SpecialTokensMixin</span></code></p>
<p>Base class for <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code>.</p>
<p>Handles shared (mostly boiler plate) methods for those two classes.</p>
<p>Class attributes (overridden by derived classes)</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>vocab_files_names</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str]</span></code>) – A dictionary with, as keys, the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> keyword name of
each vocabulary file required by the model, and as associated values, the filename for saving the associated
file (string).</p></li>
<li><p><strong>pretrained_vocab_files_map</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Dict[str,</span> <span class="pre">str]]</span></code>) – A dictionary of dictionaries, with the
high-level keys being the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> keyword name of each vocabulary file required by the model, the
low-level being the <code class="xref py py-obj docutils literal notranslate"><span class="pre">short-cut-names</span></code> of the pretrained models with, as associated values, the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">url</span></code> to the associated pretrained vocabulary file.</p></li>
<li><p><strong>max_model_input_sizes</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Optinal[int]]</span></code>) – A dictionary with, as keys, the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">short-cut-names</span></code> of the pretrained models, and as associated values, the maximum length of the sequence
inputs of this model, or <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> if the model has no maximum input size.</p></li>
<li><p><strong>pretrained_init_configuration</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Dict[str,</span> <span class="pre">Any]]</span></code>) – A dictionary with, as keys, the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">short-cut-names</span></code> of the pretrained models, and as associated values, a dictionary of specific arguments
to pass to the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method of the tokenizer class for this pretrained model when loading the
tokenizer with the <code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code>
method.</p></li>
<li><p><strong>model_input_names</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>) – A list of inputs expected in the forward pass of the model.</p></li>
<li><p><strong>padding_side</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) – The default value for the side on which the model should have padding
applied. Should be <code class="xref py py-obj docutils literal notranslate"><span class="pre">'right'</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'left'</span></code>.</p></li>
</ul>
</div></blockquote>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>model_max_length (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>The maximum length (in number of tokens) for the inputs to the transformer model. When the tokenizer is
loaded with <code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code>, this
will be set to the value stored for the associated model in <code class="docutils literal notranslate"><span class="pre">max_model_input_sizes</span></code> (see above). If no
value is provided, will default to VERY_LARGE_INTEGER (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int(1e30)</span></code>).</p>
</dd>
<dt>padding_side: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>):</dt><dd><p>The side on which the model should have padding applied. Should be selected between [‘right’, ‘left’].
Default value is picked from the class attribute of the same name.</p>
</dd>
<dt>model_input_names (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[string]</span></code>, <cite>optional</cite>):</dt><dd><p>The list of inputs accepted by the forward pass of the model (like <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;token_type_ids&quot;</span></code> or
<code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;attention_mask&quot;</span></code>). Default value is picked from the class attribute of the same name.</p>
</dd>
<dt>bos_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>):</dt><dd><p>A special token representing the beginning of a sentence. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.bos_token</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.bos_token_id</span></code>.</p>
</dd>
<dt>eos_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>):</dt><dd><p>A special token representing the end of a sentence. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.eos_token</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.eos_token_id</span></code>.</p>
</dd>
<dt>unk_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>):</dt><dd><p>A special token representing an out-of-vocabulary token. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.unk_token</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.unk_token_id</span></code>.</p>
</dd>
<dt>sep_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>):</dt><dd><p>A special token separating two different sentences in the same input (used by BERT for instance). Will be
associated to <code class="docutils literal notranslate"><span class="pre">self.sep_token</span></code> and <code class="docutils literal notranslate"><span class="pre">self.sep_token_id</span></code>.</p>
</dd>
<dt>pad_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>):</dt><dd><p>A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by
attention mechanisms or loss computation. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.pad_token</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.pad_token_id</span></code>.</p>
</dd>
<dt>cls_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>):</dt><dd><p>A special token representing the class of the input (used by BERT for instance). Will be associated to
<code class="docutils literal notranslate"><span class="pre">self.cls_token</span></code> and <code class="docutils literal notranslate"><span class="pre">self.cls_token_id</span></code>.</p>
</dd>
<dt>mask_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>):</dt><dd><p>A special token representing a masked token (used by masked-language modeling pretraining objectives, like
BERT). Will be associated to <code class="docutils literal notranslate"><span class="pre">self.mask_token</span></code> and <code class="docutils literal notranslate"><span class="pre">self.mask_token_id</span></code>.</p>
</dd>
<dt>additional_special_tokens (tuple or list of <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>):</dt><dd><p>A tuple or a list of additional special tokens. Add them here to ensure they won’t be split by the
tokenization process. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.additional_special_tokens</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.additional_special_tokens_ids</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.as_target_tokenizer">
<span class="sig-name descname"><span class="pre">as_target_tokenizer</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.as_target_tokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to
sequence-to-sequence models that need a slightly different processing for the labels.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.batch_decode">
<span class="sig-name descname"><span class="pre">batch_decode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sequences</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">np.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_special_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clean_up_tokenization_spaces</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.batch_decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert a list of lists of token ids into a list of strings by calling decode.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>sequences (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[List[int],</span> <span class="pre">List[List[int]],</span> <span class="pre">np.ndarray,</span> <span class="pre">torch.Tensor,</span> <span class="pre">tf.Tensor]</span></code>):</dt><dd><p>List of tokenized input ids. Can be obtained using the <code class="docutils literal notranslate"><span class="pre">__call__</span></code> method.</p>
</dd>
<dt>skip_special_tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to remove special tokens in the decoding.</p>
</dd>
<dt>clean_up_tokenization_spaces (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to clean up the tokenization spaces.</p>
</dd>
<dt>kwargs (additional keyword arguments, <cite>optional</cite>):</dt><dd><p>Will be passed to the underlying model specific decode method.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>: The list of decoded sentences.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.batch_encode_plus">
<span class="sig-name descname"><span class="pre">batch_encode_plus</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_text_or_text_pairs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.Tuple" title="typing.Tuple"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.Tuple" title="typing.Tuple"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.Tuple" title="typing.Tuple"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_special_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">PaddingStrategy</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truncation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">TruncationStrategy</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_split_into_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_to_multiple_of</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">None</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">TensorType</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_token_type_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_attention_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_overflowing_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_special_tokens_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_offsets_mapping</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.data.html#nlp_uncertainty_zoo.utils.data.BatchEncoding" title="transformers.tokenization_utils_base.BatchEncoding"><span class="pre">BatchEncoding</span></a></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.batch_encode_plus" title="Permalink to this definition">¶</a></dt>
<dd><p>Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This method is deprecated, <code class="docutils literal notranslate"><span class="pre">__call__</span></code> should be used instead.</p>
</div>
<dl>
<dt>Args:</dt><dd><dl>
<dt>batch_text_or_text_pairs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Tuple[str,</span> <span class="pre">str]]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[List[str]]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Tuple[List[str],</span> <span class="pre">List[str]]]</span></code>, and for not-fast tokenizers, also <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[List[int]]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Tuple[List[int],</span> <span class="pre">List[int]]]</span></code>):</dt><dd><p>Batch of sequences or pair of sequences to be encoded. This can be a list of
string/string-sequences/int-sequences or a list of pair of string/string-sequences/int-sequence (see
details in <code class="docutils literal notranslate"><span class="pre">encode_plus</span></code>).</p>
</dd>
<dt>add_special_tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to encode the sequences with the special tokens relative to their model.</p>
</dd>
<dt>padding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Activates and controls padding. Accepts the following values:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest'</span></code>: Pad to the longest sequence in the batch (or no padding if only a
single sequence if provided).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'max_length'</span></code>: Pad to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_pad'</span></code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</p></li>
</ul>
</dd>
<dt>truncation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TruncationStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Activates and controls truncation. Accepts the following values:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest_first'</span></code>: Truncate to a maximum length specified with the argument
<code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the maximum acceptable input length for the model if that argument is not
provided. This will truncate token by token, removing a token from the longest sequence in the pair
if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to
the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_second'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
to the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_truncate'</span></code> (default): No truncation (i.e., can output batch with
sequence lengths greater than the model maximum admissible input size).</p></li>
</ul>
</dd>
<dt>max_length (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this will use the predefined model maximum length if a maximum
length is required by one of the truncation/padding parameters. If the model has no specific maximum
input length (like XLNet) truncation/padding to a maximum length will be deactivated.</p>
</dd>
<dt>stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0):</dt><dd><p>If set to a number along with <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code>, the overflowing tokens returned when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code> will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.</p>
</dd>
<dt>is_split_into_words (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer
will skip the pre-tokenization step. This is useful for NER or token classification.</p>
</dd>
<dt>pad_to_multiple_of (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
the use of Tensor Cores on NVIDIA hardware with compute capability &gt;= 7.5 (Volta).</p>
</dd>
<dt>return_tensors (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code>, <cite>optional</cite>):</dt><dd><p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'tf'</span></code>: Return TensorFlow <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.constant</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'pt'</span></code>: Return PyTorch <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'np'</span></code>: Return Numpy <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> objects.</p></li>
</ul>
</dd>
<dt>return_token_type_ids (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>):</dt><dd><p>Whether to return token type IDs. If left to the default, will return the token type IDs according to
the specific tokenizer’s default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</dd>
<dt>return_attention_mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>):</dt><dd><p>Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific tokenizer’s default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</dd>
<dt>return_overflowing_tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return overflowing token sequences.</p>
</dd>
<dt>return_special_tokens_mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return special tokens mask information.</p>
</dd>
<dt>return_offsets_mapping (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return <code class="xref py py-obj docutils literal notranslate"><span class="pre">(char_start,</span> <span class="pre">char_end)</span></code> for each token.</p>
<p>This is only available on fast tokenizers inheriting from
<code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code>, if using Python’s tokenizer, this method will raise
<code class="xref py py-obj docutils literal notranslate"><span class="pre">NotImplementedError</span></code>.</p>
</dd>
<dt>return_length  (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return the lengths of the encoded inputs.</p>
</dd>
<dt>verbose (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to print more information and warnings.</p>
</dd>
</dl>
<p><a href="#id1"><span class="problematic" id="id2">**</span></a>kwargs: passed to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.tokenize()</span></code> method</p>
</dd>
<dt>Return:</dt><dd><p><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code>: A <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code> with the following fields:</p>
<ul>
<li><p><strong>input_ids</strong> – List of token ids to be fed to a model.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><p><strong>token_type_ids</strong> – List of token type ids to be fed to a model (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_token_type_ids=True</span></code>
or if <cite>“token_type_ids”</cite> is in <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.model_input_names</span></code>).</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</li>
<li><p><strong>attention_mask</strong> – List of indices specifying which tokens should be attended to by the model (when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_attention_mask=True</span></code> or if <cite>“attention_mask”</cite> is in <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.model_input_names</span></code>).</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</li>
<li><p><strong>overflowing_tokens</strong> – List of overflowing tokens sequences (when a <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> is specified and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code>).</p></li>
<li><p><strong>num_truncated_tokens</strong> – Number of tokens truncated (when a <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> is specified and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code>).</p></li>
<li><p><strong>special_tokens_mask</strong> – List of 0s and 1s, with 1 specifying added special tokens and 0 specifying
regular sequence tokens (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">add_special_tokens=True</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_special_tokens_mask=True</span></code>).</p></li>
<li><p><strong>length</strong> – The length of the inputs (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_length=True</span></code>)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.build_inputs_with_special_tokens">
<span class="sig-name descname"><span class="pre">build_inputs_with_special_tokens</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">token_ids_0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_ids_1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.build_inputs_with_special_tokens" title="Permalink to this definition">¶</a></dt>
<dd><p>Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens.</p>
<p>This implementation does not add special tokens and this method should be overridden in a subclass.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>token_ids_0 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>): The first tokenized sequence.
token_ids_1 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>): The second tokenized sequence.</p>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>: The model input with special tokens.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.clean_up_tokenization">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">clean_up_tokenization</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">out_string</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.clean_up_tokenization" title="Permalink to this definition">¶</a></dt>
<dd><p>Clean up a list of simple English tokenization artifacts like spaces before punctuations and abbreviated forms.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>out_string (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): The text to clean up.</p>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>: The cleaned-up string.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.convert_tokens_to_string">
<span class="sig-name descname"><span class="pre">convert_tokens_to_string</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.convert_tokens_to_string" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a sequence of tokens in a single string. The most simple way to do it is <code class="docutils literal notranslate"><span class="pre">&quot;</span> <span class="pre">&quot;.join(tokens)</span></code> but we
often want to remove sub-word tokenization artifacts at the same time.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): The token to join in a string.</p>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>: The joined tokens.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.create_token_type_ids_from_sequences">
<span class="sig-name descname"><span class="pre">create_token_type_ids_from_sequences</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">token_ids_0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_ids_1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.create_token_type_ids_from_sequences" title="Permalink to this definition">¶</a></dt>
<dd><p>Create the token type IDs corresponding to the sequences passed. <a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
<p>Should be overridden in a subclass if the model has a special way of building those.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>token_ids_0 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>): The first tokenized sequence.
token_ids_1 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>): The second tokenized sequence.</p>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>: The token type ids.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.decode">
<span class="sig-name descname"><span class="pre">decode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">token_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">np.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_special_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clean_up_tokenization_spaces</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special
tokens and clean up tokenization spaces.</p>
<p>Similar to doing <code class="docutils literal notranslate"><span class="pre">self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))</span></code>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>token_ids (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[int,</span> <span class="pre">List[int],</span> <span class="pre">np.ndarray,</span> <span class="pre">torch.Tensor,</span> <span class="pre">tf.Tensor]</span></code>):</dt><dd><p>List of tokenized input ids. Can be obtained using the <code class="docutils literal notranslate"><span class="pre">__call__</span></code> method.</p>
</dd>
<dt>skip_special_tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to remove special tokens in the decoding.</p>
</dd>
<dt>clean_up_tokenization_spaces (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to clean up the tokenization spaces.</p>
</dd>
<dt>kwargs (additional keyword arguments, <cite>optional</cite>):</dt><dd><p>Will be passed to the underlying model specific decode method.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>: The decoded sentence.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.encode">
<span class="sig-name descname"><span class="pre">encode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">text_pair</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_special_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">PaddingStrategy</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truncation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">TruncationStrategy</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">None</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">TensorType</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.encode" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.</p>
<p>Same as doing <code class="docutils literal notranslate"><span class="pre">self.convert_tokens_to_ids(self.tokenize(text))</span></code>.</p>
<dl>
<dt>Args:</dt><dd><dl>
<dt>text (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>):</dt><dd><p>The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the
<code class="docutils literal notranslate"><span class="pre">tokenize</span></code> method) or a list of integers (tokenized string ids using the <code class="docutils literal notranslate"><span class="pre">convert_tokens_to_ids</span></code>
method).</p>
</dd>
<dt>text_pair (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>):</dt><dd><p>Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using
the <code class="docutils literal notranslate"><span class="pre">tokenize</span></code> method) or a list of integers (tokenized string ids using the
<code class="docutils literal notranslate"><span class="pre">convert_tokens_to_ids</span></code> method).</p>
</dd>
<dt>add_special_tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to encode the sequences with the special tokens relative to their model.</p>
</dd>
<dt>padding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Activates and controls padding. Accepts the following values:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest'</span></code>: Pad to the longest sequence in the batch (or no padding if only a
single sequence if provided).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'max_length'</span></code>: Pad to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_pad'</span></code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</p></li>
</ul>
</dd>
<dt>truncation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TruncationStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Activates and controls truncation. Accepts the following values:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest_first'</span></code>: Truncate to a maximum length specified with the argument
<code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the maximum acceptable input length for the model if that argument is not
provided. This will truncate token by token, removing a token from the longest sequence in the pair
if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to
the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_second'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
to the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_truncate'</span></code> (default): No truncation (i.e., can output batch with
sequence lengths greater than the model maximum admissible input size).</p></li>
</ul>
</dd>
<dt>max_length (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this will use the predefined model maximum length if a maximum
length is required by one of the truncation/padding parameters. If the model has no specific maximum
input length (like XLNet) truncation/padding to a maximum length will be deactivated.</p>
</dd>
<dt>stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0):</dt><dd><p>If set to a number along with <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code>, the overflowing tokens returned when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code> will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.</p>
</dd>
<dt>is_split_into_words (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer
will skip the pre-tokenization step. This is useful for NER or token classification.</p>
</dd>
<dt>pad_to_multiple_of (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
the use of Tensor Cores on NVIDIA hardware with compute capability &gt;= 7.5 (Volta).</p>
</dd>
<dt>return_tensors (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code>, <cite>optional</cite>):</dt><dd><p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'tf'</span></code>: Return TensorFlow <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.constant</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'pt'</span></code>: Return PyTorch <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'np'</span></code>: Return Numpy <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> objects.</p></li>
</ul>
</dd>
</dl>
<p><a href="#id3"><span class="problematic" id="id4">**</span></a>kwargs: Passed along to the <cite>.tokenize()</cite> method.</p>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code>: The tokenized ids of the
text.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.encode_plus">
<span class="sig-name descname"><span class="pre">encode_plus</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">text_pair</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_special_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">PaddingStrategy</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truncation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">TruncationStrategy</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_split_into_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_to_multiple_of</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">None</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">TensorType</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_token_type_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_attention_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_overflowing_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_special_tokens_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_offsets_mapping</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.data.html#nlp_uncertainty_zoo.utils.data.BatchEncoding" title="transformers.tokenization_utils_base.BatchEncoding"><span class="pre">BatchEncoding</span></a></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.encode_plus" title="Permalink to this definition">¶</a></dt>
<dd><p>Tokenize and prepare for the model a sequence or a pair of sequences.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This method is deprecated, <code class="docutils literal notranslate"><span class="pre">__call__</span></code> should be used instead.</p>
</div>
<dl>
<dt>Args:</dt><dd><dl>
<dt>text (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code> (the latter only for not-fast tokenizers)):</dt><dd><p>The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the
<code class="docutils literal notranslate"><span class="pre">tokenize</span></code> method) or a list of integers (tokenized string ids using the <code class="docutils literal notranslate"><span class="pre">convert_tokens_to_ids</span></code>
method).</p>
</dd>
<dt>text_pair (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>):</dt><dd><p>Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using
the <code class="docutils literal notranslate"><span class="pre">tokenize</span></code> method) or a list of integers (tokenized string ids using the
<code class="docutils literal notranslate"><span class="pre">convert_tokens_to_ids</span></code> method).</p>
</dd>
<dt>add_special_tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to encode the sequences with the special tokens relative to their model.</p>
</dd>
<dt>padding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Activates and controls padding. Accepts the following values:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest'</span></code>: Pad to the longest sequence in the batch (or no padding if only a
single sequence if provided).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'max_length'</span></code>: Pad to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_pad'</span></code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</p></li>
</ul>
</dd>
<dt>truncation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TruncationStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Activates and controls truncation. Accepts the following values:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest_first'</span></code>: Truncate to a maximum length specified with the argument
<code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the maximum acceptable input length for the model if that argument is not
provided. This will truncate token by token, removing a token from the longest sequence in the pair
if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to
the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_second'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
to the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_truncate'</span></code> (default): No truncation (i.e., can output batch with
sequence lengths greater than the model maximum admissible input size).</p></li>
</ul>
</dd>
<dt>max_length (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this will use the predefined model maximum length if a maximum
length is required by one of the truncation/padding parameters. If the model has no specific maximum
input length (like XLNet) truncation/padding to a maximum length will be deactivated.</p>
</dd>
<dt>stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0):</dt><dd><p>If set to a number along with <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code>, the overflowing tokens returned when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code> will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.</p>
</dd>
<dt>is_split_into_words (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer
will skip the pre-tokenization step. This is useful for NER or token classification.</p>
</dd>
<dt>pad_to_multiple_of (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
the use of Tensor Cores on NVIDIA hardware with compute capability &gt;= 7.5 (Volta).</p>
</dd>
<dt>return_tensors (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code>, <cite>optional</cite>):</dt><dd><p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'tf'</span></code>: Return TensorFlow <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.constant</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'pt'</span></code>: Return PyTorch <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'np'</span></code>: Return Numpy <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> objects.</p></li>
</ul>
</dd>
<dt>return_token_type_ids (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>):</dt><dd><p>Whether to return token type IDs. If left to the default, will return the token type IDs according to
the specific tokenizer’s default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</dd>
<dt>return_attention_mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>):</dt><dd><p>Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific tokenizer’s default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</dd>
<dt>return_overflowing_tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return overflowing token sequences.</p>
</dd>
<dt>return_special_tokens_mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return special tokens mask information.</p>
</dd>
<dt>return_offsets_mapping (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return <code class="xref py py-obj docutils literal notranslate"><span class="pre">(char_start,</span> <span class="pre">char_end)</span></code> for each token.</p>
<p>This is only available on fast tokenizers inheriting from
<code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code>, if using Python’s tokenizer, this method will raise
<code class="xref py py-obj docutils literal notranslate"><span class="pre">NotImplementedError</span></code>.</p>
</dd>
<dt>return_length  (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return the lengths of the encoded inputs.</p>
</dd>
<dt>verbose (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to print more information and warnings.</p>
</dd>
</dl>
<p><a href="#id5"><span class="problematic" id="id6">**</span></a>kwargs: passed to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.tokenize()</span></code> method</p>
</dd>
<dt>Return:</dt><dd><p><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code>: A <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code> with the following fields:</p>
<ul>
<li><p><strong>input_ids</strong> – List of token ids to be fed to a model.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><p><strong>token_type_ids</strong> – List of token type ids to be fed to a model (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_token_type_ids=True</span></code>
or if <cite>“token_type_ids”</cite> is in <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.model_input_names</span></code>).</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</li>
<li><p><strong>attention_mask</strong> – List of indices specifying which tokens should be attended to by the model (when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_attention_mask=True</span></code> or if <cite>“attention_mask”</cite> is in <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.model_input_names</span></code>).</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</li>
<li><p><strong>overflowing_tokens</strong> – List of overflowing tokens sequences (when a <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> is specified and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code>).</p></li>
<li><p><strong>num_truncated_tokens</strong> – Number of tokens truncated (when a <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> is specified and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code>).</p></li>
<li><p><strong>special_tokens_mask</strong> – List of 0s and 1s, with 1 specifying added special tokens and 0 specifying
regular sequence tokens (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">add_special_tokens=True</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_special_tokens_mask=True</span></code>).</p></li>
<li><p><strong>length</strong> – The length of the inputs (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_length=True</span></code>)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.from_pretrained">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pretrained_model_name_or_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">PathLike</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">init_inputs</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.from_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiate a <a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase</span></code></a> (or a derived class) from
a predefined tokenizer.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>pretrained_model_name_or_path (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>):</dt><dd><p>Can be either:</p>
<ul class="simple">
<li><p>A string, the <cite>model id</cite> of a predefined tokenizer hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under a
user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing vocabulary files required by the tokenizer, for instance saved
using the <code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code>
method, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>(<strong>Deprecated</strong>, not applicable to all derived classes) A path or url to a single saved vocabulary
file (if and only if the tokenizer only requires a single vocabulary file like Bert or XLNet), e.g.,
<code class="docutils literal notranslate"><span class="pre">./my_model_directory/vocab.txt</span></code>.</p></li>
</ul>
</dd>
<dt>cache_dir (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>, <cite>optional</cite>):</dt><dd><p>Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the
standard cache should not be used.</p>
</dd>
<dt>force_download (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to force the (re-)download the vocabulary files and override the cached versions if they
exist.</p>
</dd>
<dt>resume_download (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to delete incompletely received files. Attempt to resume the download if such a file
exists.</p>
</dd>
<dt>proxies (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>):</dt><dd><p>A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p>
</dd>
<dt>use_auth_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <cite>bool</cite>, <cite>optional</cite>):</dt><dd><p>The token to use as HTTP bearer authorization for remote files. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, will use the token
generated when running <code class="xref py py-obj docutils literal notranslate"><span class="pre">transformers-cli</span> <span class="pre">login</span></code> (stored in <code class="xref py py-obj docutils literal notranslate"><span class="pre">huggingface</span></code>).</p>
</dd>
<dt>revision(<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;main&quot;</span></code>):</dt><dd><p>The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p>
</dd>
<dt>subfolder (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>):</dt><dd><p>In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.</p>
</dd>
<dt>inputs (additional positional arguments, <cite>optional</cite>):</dt><dd><p>Will be passed along to the Tokenizer <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p>
</dd>
<dt>kwargs (additional keyword arguments, <cite>optional</cite>):</dt><dd><p>Will be passed to the Tokenizer <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method. Can be used to set special tokens like
<code class="docutils literal notranslate"><span class="pre">bos_token</span></code>, <code class="docutils literal notranslate"><span class="pre">eos_token</span></code>, <code class="docutils literal notranslate"><span class="pre">unk_token</span></code>, <code class="docutils literal notranslate"><span class="pre">sep_token</span></code>, <code class="docutils literal notranslate"><span class="pre">pad_token</span></code>, <code class="docutils literal notranslate"><span class="pre">cls_token</span></code>,
<code class="docutils literal notranslate"><span class="pre">mask_token</span></code>, <code class="docutils literal notranslate"><span class="pre">additional_special_tokens</span></code>. See parameters in the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> for more details.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">use_auth_token=True</span></code> is required when you want to use a private model.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># We can&#39;t instantiate directly the base class `PreTrainedTokenizerBase` so let&#39;s show our examples on a derived class: BertTokenizer</span>
<span class="c1"># Download vocabulary from huggingface.co and cache.</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="c1"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;dbmdz/bert-base-german-cased&#39;</span><span class="p">)</span>

<span class="c1"># If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained(&#39;./test/saved_model/&#39;)`)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./test/saved_model/&#39;</span><span class="p">)</span>

<span class="c1"># If the tokenizer uses a single vocabulary file, you can point directly to this file</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./test/saved_model/my_vocab.txt&#39;</span><span class="p">)</span>

<span class="c1"># You can link tokens to special vocabulary when instantiating</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">,</span> <span class="n">unk_token</span><span class="o">=</span><span class="s1">&#39;&lt;unk&gt;&#39;</span><span class="p">)</span>
<span class="c1"># You should be sure &#39;&lt;unk&gt;&#39; is in the vocabulary when doing that.</span>
<span class="c1"># Otherwise use tokenizer.add_special_tokens({&#39;unk_token&#39;: &#39;&lt;unk&gt;&#39;}) instead)</span>
<span class="k">assert</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">unk_token</span> <span class="o">==</span> <span class="s1">&#39;&lt;unk&gt;&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.get_special_tokens_mask">
<span class="sig-name descname"><span class="pre">get_special_tokens_mask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">token_ids_0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_ids_1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">already_has_special_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.get_special_tokens_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code class="docutils literal notranslate"><span class="pre">prepare_for_model</span></code> or <code class="docutils literal notranslate"><span class="pre">encode_plus</span></code> methods.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>token_ids_0 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>):</dt><dd><p>List of ids of the first sequence.</p>
</dd>
<dt>token_ids_1 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>):</dt><dd><p>List of ids of the second sequence.</p>
</dd>
<dt>already_has_special_tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not the token list is already formatted with special tokens for the model.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.get_vocab">
<span class="sig-name descname"><span class="pre">get_vocab</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.get_vocab" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the vocabulary as a dictionary of token to index.</p>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizer.get_vocab()[token]</span></code> is equivalent to <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizer.convert_tokens_to_ids(token)</span></code> when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">token</span></code> is in the vocab.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">int]</span></code>: The vocabulary.</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.max_len_sentences_pair">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">max_len_sentences_pair</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.max_len_sentences_pair" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>: The maximum combined length of a pair of sentences that can be fed to the model.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.max_len_single_sentence">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">max_len_single_sentence</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.max_len_single_sentence" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>: The maximum length of a sentence that can be fed to the model.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.max_model_input_sizes">
<span class="sig-name descname"><span class="pre">max_model_input_sizes</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{}</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.max_model_input_sizes" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.model_input_names">
<span class="sig-name descname"><span class="pre">model_input_names</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['input_ids',</span> <span class="pre">'token_type_ids',</span> <span class="pre">'attention_mask']</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.model_input_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.num_special_tokens_to_add">
<span class="sig-name descname"><span class="pre">num_special_tokens_to_add</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pair</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.num_special_tokens_to_add" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.pad">
<span class="sig-name descname"><span class="pre">pad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">encoded_inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.data.html#nlp_uncertainty_zoo.utils.data.BatchEncoding" title="transformers.tokenization_utils_base.BatchEncoding"><span class="pre">BatchEncoding</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.data.html#nlp_uncertainty_zoo.utils.data.BatchEncoding" title="transformers.tokenization_utils_base.BatchEncoding"><span class="pre">BatchEncoding</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">PaddingStrategy</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_to_multiple_of</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_attention_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">None</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">TensorType</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.data.html#nlp_uncertainty_zoo.utils.data.BatchEncoding" title="transformers.tokenization_utils_base.BatchEncoding"><span class="pre">BatchEncoding</span></a></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.pad" title="Permalink to this definition">¶</a></dt>
<dd><p>Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length
in the batch.</p>
<p>Padding side (left/right) padding token ids are defined at the tokenizer level (with <code class="docutils literal notranslate"><span class="pre">self.padding_side</span></code>,
<code class="docutils literal notranslate"><span class="pre">self.pad_token_id</span></code> and <code class="docutils literal notranslate"><span class="pre">self.pad_token_type_id</span></code>)</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">encoded_inputs</span></code> passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the
result will use the same type unless you provide a different tensor type with <code class="docutils literal notranslate"><span class="pre">return_tensors</span></code>. In the
case of PyTorch tensors, you will lose the specific device of your tensors however.</p>
</div>
<dl>
<dt>Args:</dt><dd><dl>
<dt>encoded_inputs (<code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code>, list of <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">List[int]]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">List[List[int]]</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">List[int]]]</span></code>):</dt><dd><p>Tokenized inputs. Can represent one input (<code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span>
<span class="pre">List[int]]</span></code>) or a batch of tokenized inputs (list of <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code>, <cite>Dict[str,
List[List[int]]]</cite> or <cite>List[Dict[str, List[int]]]</cite>) so you can use this method during preprocessing as
well as in a PyTorch Dataloader collate function.</p>
<p>Instead of <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code> you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),
see the note above for the return type.</p>
</dd>
<dt>padding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><blockquote>
<div><p>Select a strategy to pad the returned sequences (according to the model’s padding side and padding
index) among:</p>
</div></blockquote>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest'</span></code>: Pad to the longest sequence in the batch (or no padding if only a
single sequence if provided).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'max_length'</span></code>: Pad to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_pad'</span></code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</p></li>
</ul>
</dd>
<dt>max_length (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>Maximum length of the returned list and optionally padding length (see above).</p>
</dd>
<dt>pad_to_multiple_of (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>If set will pad the sequence to a multiple of the provided value.</p>
<p>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability
&gt;= 7.5 (Volta).</p>
</dd>
<dt>return_attention_mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>):</dt><dd><p>Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific tokenizer’s default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</dd>
<dt>return_tensors (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code>, <cite>optional</cite>):</dt><dd><p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'tf'</span></code>: Return TensorFlow <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.constant</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'pt'</span></code>: Return PyTorch <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'np'</span></code>: Return Numpy <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> objects.</p></li>
</ul>
</dd>
<dt>verbose (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to print more information and warnings.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.padding_side">
<span class="sig-name descname"><span class="pre">padding_side</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'right'</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.padding_side" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.prepare_for_model">
<span class="sig-name descname"><span class="pre">prepare_for_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pair_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_special_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">PaddingStrategy</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truncation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">TruncationStrategy</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_to_multiple_of</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">None</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">TensorType</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_token_type_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_attention_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_overflowing_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_special_tokens_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_offsets_mapping</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend_batch_axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.data.html#nlp_uncertainty_zoo.utils.data.BatchEncoding" title="transformers.tokenization_utils_base.BatchEncoding"><span class="pre">BatchEncoding</span></a></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.prepare_for_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It
adds special tokens, truncates sequences if overflowing while taking into account the special tokens and
manages a moving window (with user defined stride) for overflowing tokens</p>
<dl>
<dt>Args:</dt><dd><dl>
<dt>ids (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>):</dt><dd><p>Tokenized input ids of the first sequence. Can be obtained from a string by chaining the <code class="docutils literal notranslate"><span class="pre">tokenize</span></code>
and <code class="docutils literal notranslate"><span class="pre">convert_tokens_to_ids</span></code> methods.</p>
</dd>
<dt>pair_ids (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>):</dt><dd><p>Tokenized input ids of the second sequence. Can be obtained from a string by chaining the <code class="docutils literal notranslate"><span class="pre">tokenize</span></code>
and <code class="docutils literal notranslate"><span class="pre">convert_tokens_to_ids</span></code> methods.</p>
</dd>
<dt>add_special_tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to encode the sequences with the special tokens relative to their model.</p>
</dd>
<dt>padding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Activates and controls padding. Accepts the following values:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest'</span></code>: Pad to the longest sequence in the batch (or no padding if only a
single sequence if provided).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'max_length'</span></code>: Pad to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_pad'</span></code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</p></li>
</ul>
</dd>
<dt>truncation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TruncationStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Activates and controls truncation. Accepts the following values:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest_first'</span></code>: Truncate to a maximum length specified with the argument
<code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the maximum acceptable input length for the model if that argument is not
provided. This will truncate token by token, removing a token from the longest sequence in the pair
if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to
the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_second'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
to the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_truncate'</span></code> (default): No truncation (i.e., can output batch with
sequence lengths greater than the model maximum admissible input size).</p></li>
</ul>
</dd>
<dt>max_length (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this will use the predefined model maximum length if a maximum
length is required by one of the truncation/padding parameters. If the model has no specific maximum
input length (like XLNet) truncation/padding to a maximum length will be deactivated.</p>
</dd>
<dt>stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0):</dt><dd><p>If set to a number along with <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code>, the overflowing tokens returned when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code> will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.</p>
</dd>
<dt>is_split_into_words (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer
will skip the pre-tokenization step. This is useful for NER or token classification.</p>
</dd>
<dt>pad_to_multiple_of (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
the use of Tensor Cores on NVIDIA hardware with compute capability &gt;= 7.5 (Volta).</p>
</dd>
<dt>return_tensors (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code>, <cite>optional</cite>):</dt><dd><p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'tf'</span></code>: Return TensorFlow <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.constant</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'pt'</span></code>: Return PyTorch <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'np'</span></code>: Return Numpy <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> objects.</p></li>
</ul>
</dd>
<dt>return_token_type_ids (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>):</dt><dd><p>Whether to return token type IDs. If left to the default, will return the token type IDs according to
the specific tokenizer’s default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</dd>
<dt>return_attention_mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>):</dt><dd><p>Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific tokenizer’s default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</dd>
<dt>return_overflowing_tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return overflowing token sequences.</p>
</dd>
<dt>return_special_tokens_mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return special tokens mask information.</p>
</dd>
<dt>return_offsets_mapping (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return <code class="xref py py-obj docutils literal notranslate"><span class="pre">(char_start,</span> <span class="pre">char_end)</span></code> for each token.</p>
<p>This is only available on fast tokenizers inheriting from
<code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code>, if using Python’s tokenizer, this method will raise
<code class="xref py py-obj docutils literal notranslate"><span class="pre">NotImplementedError</span></code>.</p>
</dd>
<dt>return_length  (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return the lengths of the encoded inputs.</p>
</dd>
<dt>verbose (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to print more information and warnings.</p>
</dd>
</dl>
<p><a href="#id7"><span class="problematic" id="id8">**</span></a>kwargs: passed to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.tokenize()</span></code> method</p>
</dd>
<dt>Return:</dt><dd><p><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code>: A <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code> with the following fields:</p>
<ul>
<li><p><strong>input_ids</strong> – List of token ids to be fed to a model.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><p><strong>token_type_ids</strong> – List of token type ids to be fed to a model (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_token_type_ids=True</span></code>
or if <cite>“token_type_ids”</cite> is in <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.model_input_names</span></code>).</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</li>
<li><p><strong>attention_mask</strong> – List of indices specifying which tokens should be attended to by the model (when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_attention_mask=True</span></code> or if <cite>“attention_mask”</cite> is in <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.model_input_names</span></code>).</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</li>
<li><p><strong>overflowing_tokens</strong> – List of overflowing tokens sequences (when a <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> is specified and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code>).</p></li>
<li><p><strong>num_truncated_tokens</strong> – Number of tokens truncated (when a <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> is specified and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code>).</p></li>
<li><p><strong>special_tokens_mask</strong> – List of 0s and 1s, with 1 specifying added special tokens and 0 specifying
regular sequence tokens (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">add_special_tokens=True</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_special_tokens_mask=True</span></code>).</p></li>
<li><p><strong>length</strong> – The length of the inputs (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_length=True</span></code>)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.prepare_seq2seq_batch">
<span class="sig-name descname"><span class="pre">prepare_seq2seq_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src_texts</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_texts</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_target_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'longest'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truncation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.data.html#nlp_uncertainty_zoo.utils.data.BatchEncoding" title="transformers.tokenization_utils_base.BatchEncoding"><span class="pre">BatchEncoding</span></a></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.prepare_seq2seq_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepare model inputs for translation. For best performance, translate one sentence at a time.</p>
<dl>
<dt>Arguments:</dt><dd><dl class="simple">
<dt>src_texts (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>):</dt><dd><p>List of documents to summarize or source language texts.</p>
</dd>
<dt>tgt_texts (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>, <cite>optional</cite>):</dt><dd><p>List of summaries or target language texts.</p>
</dd>
<dt>max_length (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>Controls the maximum length for encoder inputs (documents to summarize or source language texts) If
left unset or set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.</p>
</dd>
<dt>max_target_length (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>Controls the maximum length of decoder inputs (target language texts or summaries) If left unset or set
to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this will use the max_length value.</p>
</dd>
<dt>padding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Activates and controls padding. Accepts the following values:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest'</span></code>: Pad to the longest sequence in the batch (or no padding if only a
single sequence if provided).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'max_length'</span></code>: Pad to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_pad'</span></code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</p></li>
</ul>
</dd>
<dt>return_tensors (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code>, <cite>optional</cite>):</dt><dd><p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'tf'</span></code>: Return TensorFlow <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.constant</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'pt'</span></code>: Return PyTorch <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'np'</span></code>: Return Numpy <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> objects.</p></li>
</ul>
</dd>
<dt>truncation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TruncationStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Activates and controls truncation. Accepts the following values:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest_first'</span></code>: Truncate to a maximum length specified with the argument
<code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the maximum acceptable input length for the model if that argument is not
provided. This will truncate token by token, removing a token from the longest sequence in the pair
if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to
the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_second'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
to the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_truncate'</span></code> (default): No truncation (i.e., can output batch with
sequence lengths greater than the model maximum admissible input size).</p></li>
</ul>
</dd>
<dt><a href="#id9"><span class="problematic" id="id10">**</span></a>kwargs:</dt><dd><p>Additional keyword arguments passed along to <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.__call__</span></code>.</p>
</dd>
</dl>
</dd>
<dt>Return:</dt><dd><p><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code>: A <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code> with the following fields:</p>
<ul class="simple">
<li><p><strong>input_ids</strong> – List of token ids to be fed to the encoder.</p></li>
<li><p><strong>attention_mask</strong> – List of indices specifying which tokens should be attended to by the model.</p></li>
<li><p><strong>labels</strong> – List of token ids for tgt_texts.</p></li>
</ul>
<p>The full set of keys <code class="docutils literal notranslate"><span class="pre">[input_ids,</span> <span class="pre">attention_mask,</span> <span class="pre">labels]</span></code>, will only be returned if tgt_texts is passed.
Otherwise, input_ids, attention_mask will be the only keys.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.pretrained_init_configuration">
<span class="sig-name descname"><span class="pre">pretrained_init_configuration</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{}</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.pretrained_init_configuration" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.pretrained_vocab_files_map">
<span class="sig-name descname"><span class="pre">pretrained_vocab_files_map</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{}</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.pretrained_vocab_files_map" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.save_pretrained">
<span class="sig-name descname"><span class="pre">save_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save_directory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">PathLike</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">legacy_format</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filename_prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.Tuple" title="typing.Tuple"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.save_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the full tokenizer state.</p>
<p>This method make sure the full tokenizer can then be re-loaded using the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code> class method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A “fast” tokenizer (instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizerFast</span></code>) saved with this method will
not be possible to load back in a “slow” tokenizer, i.e. in a <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer</span></code>
instance. It can only be loaded in a “fast” tokenizer, i.e. in a
<code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizerFast</span></code> instance.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This won’t save modifications you may have applied to the tokenizer after the instantiation (for instance,
modifying <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizer.do_lower_case</span></code> after creation).</p>
</div>
<dl>
<dt>Args:</dt><dd><p>save_directory (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>): The path to a directory where the tokenizer will be saved.
legacy_format (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</p>
<blockquote>
<div><p>Whether to save the tokenizer in legacy format (default), i.e. with tokenizer specific vocabulary and a
separate added_tokens files or in the unified JSON file format for the <cite>tokenizers</cite> library. It’s only
possible to save a Fast tokenizer in the unified JSON format and this format is incompatible with
“slow” tokenizers (not powered by the <cite>tokenizers</cite> library).</p>
</div></blockquote>
<dl class="simple">
<dt>filename_prefix: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>):</dt><dd><p>A prefix to add to the names of the files saved by the tokenizer.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>A tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>: The files saved.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.save_vocabulary">
<span class="sig-name descname"><span class="pre">save_vocabulary</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save_directory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filename_prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.Tuple" title="typing.Tuple"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.save_vocabulary" title="Permalink to this definition">¶</a></dt>
<dd><p>Save only the vocabulary of the tokenizer (vocabulary + added tokens).</p>
<p>This method won’t save the configuration and special token mappings of the tokenizer. Use
<code class="xref py py-meth docutils literal notranslate"><span class="pre">_save_pretrained()</span></code> to save the whole state of the tokenizer.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>save_directory (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>):</dt><dd><p>The directory in which to save the vocabulary.</p>
</dd>
<dt>filename_prefix (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>):</dt><dd><p>An optional prefix to add to the named of the saved files.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple(str)</span></code>: Paths to the files saved.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.slow_tokenizer_class">
<span class="sig-name descname"><span class="pre">slow_tokenizer_class</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.slow_tokenizer_class" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.tokenize">
<span class="sig-name descname"><span class="pre">tokenize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pair</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_special_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a string in a sequence of tokens, replacing unknown tokens with the <code class="xref py py-obj docutils literal notranslate"><span class="pre">unk_token</span></code>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>text (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>):</dt><dd><p>The sequence to be encoded.</p>
</dd>
<dt>pair (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>):</dt><dd><p>A second sequence to be encoded with the first.</p>
</dd>
<dt>add_special_tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to add the special tokens associated with the corresponding model.</p>
</dd>
<dt>kwargs (additional keyword arguments, <cite>optional</cite>):</dt><dd><p>Will be passed to the underlying model specific encode method. See details in
<code class="xref py py-meth docutils literal notranslate"><span class="pre">__call__()</span></code></p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>: The list of tokens.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.truncate_sequences">
<span class="sig-name descname"><span class="pre">truncate_sequences</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pair_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_tokens_to_remove</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truncation_strategy</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">TruncationStrategy</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'longest_first'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.Tuple" title="typing.Tuple"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.truncate_sequences" title="Permalink to this definition">¶</a></dt>
<dd><p>Truncates a sequence pair in-place following the strategy.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>ids (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>):</dt><dd><p>Tokenized input ids of the first sequence. Can be obtained from a string by chaining the <code class="docutils literal notranslate"><span class="pre">tokenize</span></code>
and <code class="docutils literal notranslate"><span class="pre">convert_tokens_to_ids</span></code> methods.</p>
</dd>
<dt>pair_ids (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>):</dt><dd><p>Tokenized input ids of the second sequence. Can be obtained from a string by chaining the <code class="docutils literal notranslate"><span class="pre">tokenize</span></code>
and <code class="docutils literal notranslate"><span class="pre">convert_tokens_to_ids</span></code> methods.</p>
</dd>
<dt>num_tokens_to_remove (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0):</dt><dd><p>Number of tokens to remove using the truncation strategy.</p>
</dd>
<dt>truncation_strategy (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TruncationStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>The strategy to follow for truncation. Can be:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
to the maximum acceptable input length for the model if that argument is not provided. This will
truncate token by token, removing a token from the longest sequence in the pair if a pair of
sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to
the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_second'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
to the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_truncate'</span></code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</p></li>
</ul>
</dd>
<dt>stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0):</dt><dd><p>If set to a positive number, the overflowing tokens returned will contain some tokens from the main
sequence returned. The value of this argument defines the number of additional tokens.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[List[int],</span> <span class="pre">List[int],</span> <span class="pre">List[int]]</span></code>: The truncated <code class="docutils literal notranslate"><span class="pre">ids</span></code>, the truncated <code class="docutils literal notranslate"><span class="pre">pair_ids</span></code> and the
list of overflowing tokens.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.vocab_files_names">
<span class="sig-name descname"><span class="pre">vocab_files_names</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{}</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase.vocab_files_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.accuracy_score">
<span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.task_eval.</span></span><span class="sig-name descname"><span class="pre">accuracy_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.accuracy_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Accuracy classification score.</p>
<p>In multilabel classification, this function computes subset accuracy:
the set of labels predicted for a sample must <em>exactly</em> match the
corresponding set of labels in y_true.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y_true</strong><span class="classifier">1d array-like, or label indicator array / sparse matrix</span></dt><dd><p>Ground truth (correct) labels.</p>
</dd>
<dt><strong>y_pred</strong><span class="classifier">1d array-like, or label indicator array / sparse matrix</span></dt><dd><p>Predicted labels, as returned by a classifier.</p>
</dd>
<dt><strong>normalize</strong><span class="classifier">bool, default=True</span></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">False</span></code>, return the number of correctly classified samples.
Otherwise, return the fraction of correctly classified samples.</p>
</dd>
<dt><strong>sample_weight</strong><span class="classifier">array-like of shape (n_samples,), default=None</span></dt><dd><p>Sample weights.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl>
<dt><strong>score</strong><span class="classifier">float</span></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">normalize</span> <span class="pre">==</span> <span class="pre">True</span></code>, return the fraction of correctly
classified samples (float), else returns the number of correctly
classified samples (int).</p>
<p>The best performance is 1 with <code class="docutils literal notranslate"><span class="pre">normalize</span> <span class="pre">==</span> <span class="pre">True</span></code> and the number
of samples with <code class="docutils literal notranslate"><span class="pre">normalize</span> <span class="pre">==</span> <span class="pre">False</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">jaccard_score</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">hamming_loss</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">zero_one_loss</span></code></dt><dd></dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>In binary and multiclass classification, this function is equal
to the <code class="docutils literal notranslate"><span class="pre">jaccard_score</span></code> function.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">2</span>
</pre></div>
</div>
<p>In the multilabel case with binary label indicators:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="go">0.5</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.defaultdict">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.task_eval.</span></span><span class="sig-name descname"><span class="pre">defaultdict</span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.defaultdict" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></p>
<p>defaultdict(default_factory[, …]) –&gt; dict with default factory</p>
<p>The default factory is called without arguments to produce
a new value when a key is not present, in __getitem__ only.
A defaultdict compares equal to a dict with the same items.
All remaining arguments are treated the same as if they were
passed to the dict constructor, including keyword arguments.</p>
<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.defaultdict.copy">
<span class="sig-name descname"><span class="pre">copy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">a</span> <span class="pre">shallow</span> <span class="pre">copy</span> <span class="pre">of</span> <span class="pre">D.</span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.defaultdict.copy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.defaultdict.default_factory">
<span class="sig-name descname"><span class="pre">default_factory</span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.defaultdict.default_factory" title="Permalink to this definition">¶</a></dt>
<dd><p>Factory for default value called by __missing__().</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.evaluate">
<span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.task_eval.</span></span><span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_split</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.DataLoader" title="torch.utils.data.dataloader.DataLoader"><span class="pre">DataLoader</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase"><span class="pre">PreTrainedTokenizerBase</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluate a model and save predictions (if applicable).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>model: Model</strong></dt><dd><p>Model to be evaluated.</p>
</dd>
<dt><strong>eval_split: DataSplit</strong></dt><dd><p>Data split the model is being evaluated on.</p>
</dd>
<dt><strong>tokenizer: PreTrainedTokenizerBase</strong></dt><dd><p>Tokenizer of the evaluated model.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>Dict[str, float]</dt><dd><p>Return score on test set.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.f1_score">
<span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.task_eval.</span></span><span class="sig-name descname"><span class="pre">f1_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">average</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'binary'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">zero_division</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'warn'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.f1_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the F1 score, also known as balanced F-score or F-measure.</p>
<p>The F1 score can be interpreted as a weighted average of the precision and
recall, where an F1 score reaches its best value at 1 and worst score at 0.
The relative contribution of precision and recall to the F1 score are
equal. The formula for the F1 score is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">F1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">precision</span> <span class="o">*</span> <span class="n">recall</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">precision</span> <span class="o">+</span> <span class="n">recall</span><span class="p">)</span>
</pre></div>
</div>
<p>In the multi-class and multi-label case, this is the average of
the F1 score of each class with weighting depending on the <code class="docutils literal notranslate"><span class="pre">average</span></code>
parameter.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>y_true</strong><span class="classifier">1d array-like, or label indicator array / sparse matrix</span></dt><dd><p>Ground truth (correct) target values.</p>
</dd>
<dt><strong>y_pred</strong><span class="classifier">1d array-like, or label indicator array / sparse matrix</span></dt><dd><p>Estimated targets as returned by a classifier.</p>
</dd>
<dt><strong>labels</strong><span class="classifier">array-like, default=None</span></dt><dd><p>The set of labels to include when <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code>, and their
order if <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">is</span> <span class="pre">None</span></code>. Labels present in the data can be
excluded, for example to calculate a multiclass average ignoring a
majority negative class, while labels not present in the data will
result in 0 components in a macro average. For multilabel targets,
labels are column indices. By default, all labels in <code class="docutils literal notranslate"><span class="pre">y_true</span></code> and
<code class="docutils literal notranslate"><span class="pre">y_pred</span></code> are used in sorted order.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.17: </span>Parameter <cite>labels</cite> improved for multiclass problem.</p>
</div>
</dd>
<dt><strong>pos_label</strong><span class="classifier">str or int, default=1</span></dt><dd><p>The class to report if <code class="docutils literal notranslate"><span class="pre">average='binary'</span></code> and the data is binary.
If the data are multiclass or multilabel, this will be ignored;
setting <code class="docutils literal notranslate"><span class="pre">labels=[pos_label]</span></code> and <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code> will report
scores for that label only.</p>
</dd>
<dt><strong>average</strong><span class="classifier">{‘micro’, ‘macro’, ‘samples’,’weighted’, ‘binary’} or None,             default=’binary’</span></dt><dd><p>This parameter is required for multiclass/multilabel targets.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the scores for each class are returned. Otherwise, this
determines the type of averaging performed on the data:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'binary'</span></code>:</dt><dd><p>Only report results for the class specified by <code class="docutils literal notranslate"><span class="pre">pos_label</span></code>.
This is applicable only if targets (<code class="docutils literal notranslate"><span class="pre">y_{true,pred}</span></code>) are binary.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'micro'</span></code>:</dt><dd><p>Calculate metrics globally by counting the total true positives,
false negatives and false positives.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'macro'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their unweighted
mean.  This does not take label imbalance into account.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'weighted'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their average weighted
by support (the number of true instances for each label). This
alters ‘macro’ to account for label imbalance; it can result in an
F-score that is not between precision and recall.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'samples'</span></code>:</dt><dd><p>Calculate metrics for each instance, and find their average (only
meaningful for multilabel classification where this differs from
<a class="reference internal" href="#nlp_uncertainty_zoo.utils.task_eval.accuracy_score" title="nlp_uncertainty_zoo.utils.task_eval.accuracy_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">accuracy_score()</span></code></a>).</p>
</dd>
</dl>
</dd>
<dt><strong>sample_weight</strong><span class="classifier">array-like of shape (n_samples,), default=None</span></dt><dd><p>Sample weights.</p>
</dd>
<dt><strong>zero_division</strong><span class="classifier">“warn”, 0 or 1, default=”warn”</span></dt><dd><p>Sets the value to return when there is a zero division, i.e. when all
predictions and labels are negative. If set to “warn”, this acts as 0,
but warnings are also raised.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>f1_score</strong><span class="classifier">float or array of float, shape = [n_unique_labels]</span></dt><dd><p>F1 score of the positive class in binary classification or weighted
average of the F1 scores of each class for the multiclass task.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">fbeta_score</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">precision_recall_fscore_support</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">jaccard_score</span></code></dt><dd></dd>
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">multilabel_confusion_matrix</span></code></dt><dd></dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>When <code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">positive</span> <span class="pre">==</span> <span class="pre">0</span></code>, precision is undefined.
When <code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">negative</span> <span class="pre">==</span> <span class="pre">0</span></code>, recall is undefined.
In such cases, by default the metric will be set to 0, as will f-score,
and <code class="docutils literal notranslate"><span class="pre">UndefinedMetricWarning</span></code> will be raised. This behavior can be
modified with <code class="docutils literal notranslate"><span class="pre">zero_division</span></code>.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="rc74bec891614-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/F1_score">Wikipedia entry for the F1-score</a>.</p>
</div>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>
<span class="go">0.33...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">array([0.8, 0. , 0. ])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">1.0...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.nll_loss">
<span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.task_eval.</span></span><span class="sig-name descname"><span class="pre">nll_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size_average</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.nll_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>The negative log likelihood loss.</p>
<p>See <code class="xref py py-class docutils literal notranslate"><span class="pre">NLLLoss</span></code> for details.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>input: <span class="math notranslate nohighlight">\((N, C)\)</span> where <cite>C = number of classes</cite> or <span class="math notranslate nohighlight">\((N, C, H, W)\)</span></dt><dd><p>in case of 2D Loss, or <span class="math notranslate nohighlight">\((N, C, d_1, d_2, ..., d_K)\)</span> where <span class="math notranslate nohighlight">\(K \geq 1\)</span>
in the case of K-dimensional loss.</p>
</dd>
<dt>target: <span class="math notranslate nohighlight">\((N)\)</span> where each value is <span class="math notranslate nohighlight">\(0 \leq \text{targets}[i] \leq C-1\)</span>,</dt><dd><p>or <span class="math notranslate nohighlight">\((N, d_1, d_2, ..., d_K)\)</span> where <span class="math notranslate nohighlight">\(K \geq 1\)</span> for
K-dimensional loss.</p>
</dd>
<dt>weight (Tensor, optional): a manual rescaling weight given to each</dt><dd><p>class. If given, has to be a Tensor of size <cite>C</cite></p>
</dd>
<dt>size_average (bool, optional): Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,</dt><dd><p>the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
</dd>
<dt>ignore_index (int, optional): Specifies a target value that is ignored</dt><dd><p>and does not contribute to the input gradient. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> is
<code class="docutils literal notranslate"><span class="pre">True</span></code>, the loss is averaged over non-ignored targets. Default: -100</p>
</dd>
<dt>reduce (bool, optional): Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the</dt><dd><p>losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
</dd>
<dt>reduction (string, optional): Specifies the reduction to apply to the output:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p>
</dd>
</dl>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># input is of size N x C = 3 x 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># each element in target has to have 0 &lt;= value &lt; C</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.task_eval.rearrange">
<span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.task_eval.</span></span><span class="sig-name descname"><span class="pre">rearrange</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pattern</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">axes_lengths</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.task_eval.rearrange" title="Permalink to this definition">¶</a></dt>
<dd><p>einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.
This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze,
stack, concatenate and other operations.</p>
<p>Examples for rearrange operation:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># suppose we have a set of 32 images in &quot;h w c&quot; format (height-width-channel)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">images</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">32</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># stack along first (batch) axis, output is a single array</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rearrange</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="s1">&#39;b h w c -&gt; b h w c&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(32, 30, 40, 3)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># concatenate images along height (vertical axis), 960 = 32 * 30</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rearrange</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="s1">&#39;b h w c -&gt; (b h) w c&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(960, 40, 3)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># concatenated images along horizontal axis, 1280 = 32 * 40</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rearrange</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="s1">&#39;b h w c -&gt; h (b w) c&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(30, 1280, 3)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># reordered axes to &quot;b c h w&quot; format for deep learning</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rearrange</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="s1">&#39;b h w c -&gt; b c h w&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(32, 3, 30, 40)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># flattened each image into a vector, 3600 = 30 * 40 * 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rearrange</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="s1">&#39;b h w c -&gt; b (c h w)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(32, 3600)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># split each image into 4 smaller (top-left, top-right, bottom-left, bottom-right), 128 = 32 * 2 * 2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rearrange</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="s1">&#39;b (h1 h) (w1 w) c -&gt; (b h1 w1) h w c&#39;</span><span class="p">,</span> <span class="n">h1</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">w1</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(128, 15, 20, 3)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># space-to-depth operation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rearrange</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="s1">&#39;b (h h1) (w w1) c -&gt; b h w (c h1 w1)&#39;</span><span class="p">,</span> <span class="n">h1</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">w1</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(32, 15, 20, 12)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch, mxnet.ndarray).
list of tensors is also accepted, those should be of the same type and shape</p></li>
<li><p><strong>pattern</strong> – string, rearrangement pattern</p></li>
<li><p><strong>axes_lengths</strong> – any additional specifications for dimensions</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>tensor of the same type as input. If possible, a view to the original tensor is returned.</p>
</dd>
</dl>
<p>When composing axes, C-order enumeration used (consecutive elements have different last axis)
Find more examples in einops tutorial.</p>
</dd></dl>

</section>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright 2022, Dennis Ulmer.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 5.3.0.<br/>
    </p>
  </div>
</footer>
  </body>
</html>