<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>nlp_uncertainty_zoo.utils.data &#8212; nlp-uncertainty-zoo 0.9.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/bootstrap-sphinx.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="nlp_uncertainty_zoo.utils.metrics" href="nlp_uncertainty_zoo.utils.metrics.html" />
    <link rel="prev" title="nlp_uncertainty_zoo.utils.custom_types" href="nlp_uncertainty_zoo.utils.custom_types.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="_static/js/jquery-1.12.4.min.js"></script>
<script type="text/javascript" src="_static/js/jquery-fix.js"></script>
<script type="text/javascript" src="_static/bootstrap-3.4.1/js/bootstrap.min.js"></script>
<script type="text/javascript" src="_static/bootstrap-sphinx.js"></script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="index.html">
          nlp-uncertainty-zoo</a>
        <span class="navbar-text navbar-version pull-left"><b>0.9.0</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.html">nlp_uncertainty_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.html">nlp_uncertainty_zoo.models</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.bayesian_lstm.html">nlp_uncertainty_zoo.models.bayesian_lstm</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.bert.html">nlp_uncertainty_zoo.models.bert</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.ddu_transformer.html">nlp_uncertainty_zoo.models.ddu_transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.dpp_transformer.html">nlp_uncertainty_zoo.models.dpp_transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.lstm.html">nlp_uncertainty_zoo.models.lstm</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.lstm_ensemble.html">nlp_uncertainty_zoo.models.lstm_ensemble</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.model.html">nlp_uncertainty_zoo.models.model</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.sngp_transformer.html">nlp_uncertainty_zoo.models.sngp_transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.spectral.html">nlp_uncertainty_zoo.models.spectral</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.st_tau_lstm.html">nlp_uncertainty_zoo.models.st_tau_lstm</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.transformer.html">nlp_uncertainty_zoo.models.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.st_tau_lstm.html">nlp_uncertainty_zoo.models.st_tau_lstm</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.transformer.html">nlp_uncertainty_zoo.models.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.variational_lstm.html">nlp_uncertainty_zoo.models.variational_lstm</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.models.variational_transformer.html">nlp_uncertainty_zoo.models.variational_transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.utils.html">nlp_uncertainty_zoo.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.utils.custom_types.html">nlp_uncertainty_zoo.utils.custom_types</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">nlp_uncertainty_zoo.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.utils.metrics.html">nlp_uncertainty_zoo.utils.metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html">nlp_uncertainty_zoo.utils.samplers</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html">nlp_uncertainty_zoo.utils.task_eval</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp_uncertainty_zoo.utils.uncertainty_eval.html">nlp_uncertainty_zoo.utils.uncertainty_eval</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">nlp_uncertainty_zoo.utils.data</a><ul>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.ABC"><code class="docutils literal notranslate"><span class="pre">ABC</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding"><code class="docutils literal notranslate"><span class="pre">BatchEncoding</span></code></a><ul>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.char_to_token"><code class="docutils literal notranslate"><span class="pre">BatchEncoding.char_to_token()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.char_to_word"><code class="docutils literal notranslate"><span class="pre">BatchEncoding.char_to_word()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.convert_to_tensors"><code class="docutils literal notranslate"><span class="pre">BatchEncoding.convert_to_tensors()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.encodings"><code class="docutils literal notranslate"><span class="pre">BatchEncoding.encodings</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.is_fast"><code class="docutils literal notranslate"><span class="pre">BatchEncoding.is_fast</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.items"><code class="docutils literal notranslate"><span class="pre">BatchEncoding.items()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.keys"><code class="docutils literal notranslate"><span class="pre">BatchEncoding.keys()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.n_sequences"><code class="docutils literal notranslate"><span class="pre">BatchEncoding.n_sequences</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.sequence_ids"><code class="docutils literal notranslate"><span class="pre">BatchEncoding.sequence_ids()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.to"><code class="docutils literal notranslate"><span class="pre">BatchEncoding.to()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.token_to_chars"><code class="docutils literal notranslate"><span class="pre">BatchEncoding.token_to_chars()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.token_to_sequence"><code class="docutils literal notranslate"><span class="pre">BatchEncoding.token_to_sequence()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.token_to_word"><code class="docutils literal notranslate"><span class="pre">BatchEncoding.token_to_word()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.tokens"><code class="docutils literal notranslate"><span class="pre">BatchEncoding.tokens()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.values"><code class="docutils literal notranslate"><span class="pre">BatchEncoding.values()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.word_ids"><code class="docutils literal notranslate"><span class="pre">BatchEncoding.word_ids()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.word_to_chars"><code class="docutils literal notranslate"><span class="pre">BatchEncoding.word_to_chars()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.word_to_tokens"><code class="docutils literal notranslate"><span class="pre">BatchEncoding.word_to_tokens()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.words"><code class="docutils literal notranslate"><span class="pre">BatchEncoding.words()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer"><code class="docutils literal notranslate"><span class="pre">BertTokenizer</span></code></a><ul>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer.build_inputs_with_special_tokens"><code class="docutils literal notranslate"><span class="pre">BertTokenizer.build_inputs_with_special_tokens()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer.convert_tokens_to_string"><code class="docutils literal notranslate"><span class="pre">BertTokenizer.convert_tokens_to_string()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer.create_token_type_ids_from_sequences"><code class="docutils literal notranslate"><span class="pre">BertTokenizer.create_token_type_ids_from_sequences()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer.do_lower_case"><code class="docutils literal notranslate"><span class="pre">BertTokenizer.do_lower_case</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer.get_special_tokens_mask"><code class="docutils literal notranslate"><span class="pre">BertTokenizer.get_special_tokens_mask()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer.get_vocab"><code class="docutils literal notranslate"><span class="pre">BertTokenizer.get_vocab()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer.max_model_input_sizes"><code class="docutils literal notranslate"><span class="pre">BertTokenizer.max_model_input_sizes</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer.pretrained_init_configuration"><code class="docutils literal notranslate"><span class="pre">BertTokenizer.pretrained_init_configuration</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer.pretrained_vocab_files_map"><code class="docutils literal notranslate"><span class="pre">BertTokenizer.pretrained_vocab_files_map</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer.save_vocabulary"><code class="docutils literal notranslate"><span class="pre">BertTokenizer.save_vocabulary()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer.vocab_files_names"><code class="docutils literal notranslate"><span class="pre">BertTokenizer.vocab_files_names</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer.vocab_size"><code class="docutils literal notranslate"><span class="pre">BertTokenizer.vocab_size</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BertTokenizerFast"><code class="docutils literal notranslate"><span class="pre">BertTokenizerFast</span></code></a><ul>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BertTokenizerFast.build_inputs_with_special_tokens"><code class="docutils literal notranslate"><span class="pre">BertTokenizerFast.build_inputs_with_special_tokens()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BertTokenizerFast.create_token_type_ids_from_sequences"><code class="docutils literal notranslate"><span class="pre">BertTokenizerFast.create_token_type_ids_from_sequences()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BertTokenizerFast.max_model_input_sizes"><code class="docutils literal notranslate"><span class="pre">BertTokenizerFast.max_model_input_sizes</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BertTokenizerFast.pretrained_init_configuration"><code class="docutils literal notranslate"><span class="pre">BertTokenizerFast.pretrained_init_configuration</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BertTokenizerFast.pretrained_vocab_files_map"><code class="docutils literal notranslate"><span class="pre">BertTokenizerFast.pretrained_vocab_files_map</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BertTokenizerFast.save_vocabulary"><code class="docutils literal notranslate"><span class="pre">BertTokenizerFast.save_vocabulary()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BertTokenizerFast.slow_tokenizer_class"><code class="docutils literal notranslate"><span class="pre">BertTokenizerFast.slow_tokenizer_class</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BertTokenizerFast.vocab_files_names"><code class="docutils literal notranslate"><span class="pre">BertTokenizerFast.vocab_files_names</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.ClassificationDatasetBuilder"><code class="docutils literal notranslate"><span class="pre">ClassificationDatasetBuilder</span></code></a><ul>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.ClassificationDatasetBuilder.build"><code class="docutils literal notranslate"><span class="pre">ClassificationDatasetBuilder.build()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataCollatorForLanguageModeling"><code class="docutils literal notranslate"><span class="pre">DataCollatorForLanguageModeling</span></code></a><ul>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataCollatorForLanguageModeling.mask_tokens"><code class="docutils literal notranslate"><span class="pre">DataCollatorForLanguageModeling.mask_tokens()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataCollatorForLanguageModeling.mlm"><code class="docutils literal notranslate"><span class="pre">DataCollatorForLanguageModeling.mlm</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataCollatorForLanguageModeling.mlm_probability"><code class="docutils literal notranslate"><span class="pre">DataCollatorForLanguageModeling.mlm_probability</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataCollatorForLanguageModeling.tokenizer"><code class="docutils literal notranslate"><span class="pre">DataCollatorForLanguageModeling.tokenizer</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataLoader"><code class="docutils literal notranslate"><span class="pre">DataLoader</span></code></a><ul>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataLoader.batch_size"><code class="docutils literal notranslate"><span class="pre">DataLoader.batch_size</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataLoader.check_worker_number_rationality"><code class="docutils literal notranslate"><span class="pre">DataLoader.check_worker_number_rationality()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataLoader.dataset"><code class="docutils literal notranslate"><span class="pre">DataLoader.dataset</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataLoader.drop_last"><code class="docutils literal notranslate"><span class="pre">DataLoader.drop_last</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataLoader.multiprocessing_context"><code class="docutils literal notranslate"><span class="pre">DataLoader.multiprocessing_context</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataLoader.num_workers"><code class="docutils literal notranslate"><span class="pre">DataLoader.num_workers</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataLoader.pin_memory"><code class="docutils literal notranslate"><span class="pre">DataLoader.pin_memory</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataLoader.prefetch_factor"><code class="docutils literal notranslate"><span class="pre">DataLoader.prefetch_factor</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataLoader.sampler"><code class="docutils literal notranslate"><span class="pre">DataLoader.sampler</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataLoader.timeout"><code class="docutils literal notranslate"><span class="pre">DataLoader.timeout</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DatasetBuilder"><code class="docutils literal notranslate"><span class="pre">DatasetBuilder</span></code></a><ul>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DatasetBuilder.build"><code class="docutils literal notranslate"><span class="pre">DatasetBuilder.build()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.Dict"><code class="docutils literal notranslate"><span class="pre">Dict</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.LabelEncoder"><code class="docutils literal notranslate"><span class="pre">LabelEncoder</span></code></a><ul>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.LabelEncoder.fit"><code class="docutils literal notranslate"><span class="pre">LabelEncoder.fit()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.LabelEncoder.fit_transform"><code class="docutils literal notranslate"><span class="pre">LabelEncoder.fit_transform()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.LabelEncoder.inverse_transform"><code class="docutils literal notranslate"><span class="pre">LabelEncoder.inverse_transform()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.LabelEncoder.transform"><code class="docutils literal notranslate"><span class="pre">LabelEncoder.transform()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.LanguageModellingDatasetBuilder"><code class="docutils literal notranslate"><span class="pre">LanguageModellingDatasetBuilder</span></code></a><ul>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.LanguageModellingDatasetBuilder.build"><code class="docutils literal notranslate"><span class="pre">LanguageModellingDatasetBuilder.build()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.List"><code class="docutils literal notranslate"><span class="pre">List</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.ModifiedDataCollatorForLanguageModeling"><code class="docutils literal notranslate"><span class="pre">ModifiedDataCollatorForLanguageModeling</span></code></a><ul>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.ModifiedDataCollatorForLanguageModeling.mlm"><code class="docutils literal notranslate"><span class="pre">ModifiedDataCollatorForLanguageModeling.mlm</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.ModifiedDataCollatorForLanguageModeling.mlm_probability"><code class="docutils literal notranslate"><span class="pre">ModifiedDataCollatorForLanguageModeling.mlm_probability</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.ModifiedDataCollatorForLanguageModeling.pad_to_multiple_of"><code class="docutils literal notranslate"><span class="pre">ModifiedDataCollatorForLanguageModeling.pad_to_multiple_of</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.ModifiedDataCollatorForLanguageModeling.tokenizer"><code class="docutils literal notranslate"><span class="pre">ModifiedDataCollatorForLanguageModeling.tokenizer</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase</span></code></a><ul>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.as_target_tokenizer"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.as_target_tokenizer()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.batch_decode"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.batch_decode()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.batch_encode_plus"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.batch_encode_plus()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.build_inputs_with_special_tokens"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.build_inputs_with_special_tokens()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.clean_up_tokenization"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.clean_up_tokenization()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.convert_tokens_to_string"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.convert_tokens_to_string()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.create_token_type_ids_from_sequences"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.create_token_type_ids_from_sequences()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.decode"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.decode()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.encode"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.encode()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.encode_plus"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.encode_plus()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.from_pretrained"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.from_pretrained()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.get_special_tokens_mask"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.get_special_tokens_mask()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.get_vocab"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.get_vocab()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.max_len_sentences_pair"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.max_len_sentences_pair</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.max_len_single_sentence"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.max_len_single_sentence</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.max_model_input_sizes"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.max_model_input_sizes</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.model_input_names"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.model_input_names</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.num_special_tokens_to_add"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.num_special_tokens_to_add()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.pad"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.pad()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.padding_side"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.padding_side</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.prepare_for_model"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.prepare_for_model()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.prepare_seq2seq_batch"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.prepare_seq2seq_batch()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.pretrained_init_configuration"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.pretrained_init_configuration</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.pretrained_vocab_files_map"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.pretrained_vocab_files_map</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.save_pretrained"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.save_pretrained()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.save_vocabulary"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.save_vocabulary()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.slow_tokenizer_class"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.slow_tokenizer_class</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.tokenize"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.tokenize()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.truncate_sequences"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.truncate_sequences()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.vocab_files_names"><code class="docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.vocab_files_names</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.Type"><code class="docutils literal notranslate"><span class="pre">Type</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.abstractmethod"><code class="docutils literal notranslate"><span class="pre">abstractmethod()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.defaultdict"><code class="docutils literal notranslate"><span class="pre">defaultdict</span></code></a><ul>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.defaultdict.copy"><code class="docutils literal notranslate"><span class="pre">defaultdict.copy()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.defaultdict.default_factory"><code class="docutils literal notranslate"><span class="pre">defaultdict.default_factory</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.load_dataset"><code class="docutils literal notranslate"><span class="pre">load_dataset()</span></code></a></li>
<li><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.reduce"><code class="docutils literal notranslate"><span class="pre">reduce()</span></code></a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
              
                
  <li>
    <a href="nlp_uncertainty_zoo.utils.custom_types.html" title="Previous Chapter: nlp_uncertainty_zoo.utils.custom_types"><span class="glyphicon glyphicon-chevron-left visible-sm"></span><span class="hidden-sm hidden-tablet">&laquo; nlp_uncertain...</span>
    </a>
  </li>
  <li>
    <a href="nlp_uncertainty_zoo.utils.metrics.html" title="Next Chapter: nlp_uncertainty_zoo.utils.metrics"><span class="glyphicon glyphicon-chevron-right visible-sm"></span><span class="hidden-sm hidden-tablet">nlp_uncertain... &raquo;</span>
    </a>
  </li>
              
            
            
            
            
              <li class="hidden-sm">
<div id="sourcelink">
  <a href="_sources/nlp_uncertainty_zoo.utils.data.rst.txt"
     rel="nofollow">Source</a>
</div></li>
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <section id="nlp-uncertainty-zoo-utils-data">
<h1>nlp_uncertainty_zoo.utils.data<a class="headerlink" href="#nlp-uncertainty-zoo-utils-data" title="Permalink to this heading">¶</a></h1>
<span class="target" id="module-nlp_uncertainty_zoo.utils.data"></span><p>Module to implement data reading and batching functionalities.</p>
<dl class="py class">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.ABC">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.data.</span></span><span class="sig-name descname"><span class="pre">ABC</span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.ABC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BatchEncoding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.data.</span></span><span class="sig-name descname"><span class="pre">BatchEncoding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Encoding</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Encoding</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">None</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">TensorType</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend_batch_axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_sequences</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">UserDict</span></code></p>
<p>Holds the output of the <code class="xref py py-meth docutils literal notranslate"><span class="pre">encode_plus()</span></code> and
<code class="xref py py-meth docutils literal notranslate"><span class="pre">batch_encode()</span></code> methods (tokens,
attention_masks, etc).</p>
<p>This class is derived from a python dictionary and can be used as a dictionary. In addition, this class exposes
utility methods to map from word/character space to token space.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>):</dt><dd><p>Dictionary of lists/arrays/tensors returned by the encode/batch_encode methods (‘input_ids’,
‘attention_mask’, etc.).</p>
</dd>
<dt>encoding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.Encoding</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[tokenizers.Encoding]</span></code>, <cite>optional</cite>):</dt><dd><p>If the tokenizer is a fast tokenizer which outputs additional information like mapping from word/character
space to token space the <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.Encoding</span></code> instance or list of instance (for batches) hold this
information.</p>
</dd>
<dt>tensor_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[None,</span> <span class="pre">str,</span> <span class="pre">TensorType]</span></code>, <cite>optional</cite>):</dt><dd><p>You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at
initialization.</p>
</dd>
<dt>prepend_batch_axis (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to add a batch axis when converting to tensors (see <code class="xref py py-obj docutils literal notranslate"><span class="pre">tensor_type</span></code> above).</p>
</dd>
<dt>n_sequences (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[int]</span></code>, <cite>optional</cite>):</dt><dd><p>You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at
initialization.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BatchEncoding.char_to_token">
<span class="sig-name descname"><span class="pre">char_to_token</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_or_char_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">char_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sequence_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.char_to_token" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the index of the token in the encoded output comprising a character in the original string for a sequence
of the batch.</p>
<p>Can be called as:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">self.char_to_token(char_index)</span></code> if batch size is 1</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.char_to_token(batch_index,</span> <span class="pre">char_index)</span></code> if batch size is greater or equal to 1</p></li>
</ul>
<p>This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e. words
are defined by the user). In this case it allows to easily associate encoded tokens with provided tokenized
words.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>batch_or_char_index (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>):</dt><dd><p>Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of
the word in the sequence</p>
</dd>
<dt>char_index (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>If a batch index is provided in <cite>batch_or_token_index</cite>, this can be the index of the word in the
sequence.</p>
</dd>
<dt>sequence_index (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0):</dt><dd><p>If pair of sequences are encoded in the batch this can be used to specify which sequence in the pair (0
or 1) the provided character index belongs to.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>: Index of the token.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BatchEncoding.char_to_word">
<span class="sig-name descname"><span class="pre">char_to_word</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_or_char_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">char_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sequence_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.char_to_word" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the word in the original string corresponding to a character in the original string of a sequence of the
batch.</p>
<p>Can be called as:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">self.char_to_word(char_index)</span></code> if batch size is 1</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.char_to_word(batch_index,</span> <span class="pre">char_index)</span></code> if batch size is greater than 1</p></li>
</ul>
<p>This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e. words
are defined by the user). In this case it allows to easily associate encoded tokens with provided tokenized
words.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>batch_or_char_index (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>):</dt><dd><p>Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of
the character in the original string.</p>
</dd>
<dt>char_index (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>If a batch index is provided in <cite>batch_or_token_index</cite>, this can be the index of the character in the
original string.</p>
</dd>
<dt>sequence_index (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0):</dt><dd><p>If pair of sequences are encoded in the batch this can be used to specify which sequence in the pair (0
or 1) the provided character index belongs to.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>: Index or indices of the associated encoded token(s).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BatchEncoding.convert_to_tensors">
<span class="sig-name descname"><span class="pre">convert_to_tensors</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">None</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">TensorType</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend_batch_axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.convert_to_tensors" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert the inner content to tensors.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>tensor_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code>, <cite>optional</cite>):</dt><dd><p>The type of tensors to use. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, should be one of the values of the enum
<code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code>. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, no modification is done.</p>
</dd>
<dt>prepend_batch_axis (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to add the batch dimension during the conversion.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BatchEncoding.encodings">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">encodings</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Encoding</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.encodings" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[List[tokenizers.Encoding]]</span></code>: The list all encodings from the tokenization process. Returns
<code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> if the input was tokenized through Python (i.e., not a fast) tokenizer.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BatchEncoding.is_fast">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">is_fast</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.is_fast" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>: Indicate whether this <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code> was generated from the result of a
<code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code> or not.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BatchEncoding.items">
<span class="sig-name descname"><span class="pre">items</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">a</span> <span class="pre">set-like</span> <span class="pre">object</span> <span class="pre">providing</span> <span class="pre">a</span> <span class="pre">view</span> <span class="pre">on</span> <span class="pre">D's</span> <span class="pre">items</span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.items" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BatchEncoding.keys">
<span class="sig-name descname"><span class="pre">keys</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">a</span> <span class="pre">set-like</span> <span class="pre">object</span> <span class="pre">providing</span> <span class="pre">a</span> <span class="pre">view</span> <span class="pre">on</span> <span class="pre">D's</span> <span class="pre">keys</span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.keys" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BatchEncoding.n_sequences">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">n_sequences</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.n_sequences" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[int]</span></code>: The number of sequences used to generate each sample from the batch encoded in this
<code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code>. Currently can be one of <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> (unknown), <code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> (a single
sentence) or <code class="xref py py-obj docutils literal notranslate"><span class="pre">2</span></code> (a pair of sentences)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BatchEncoding.sequence_ids">
<span class="sig-name descname"><span class="pre">sequence_ids</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.sequence_ids" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a list mapping the tokens to the id of their original sentences:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> for special tokens added around or between sequences,</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> for tokens corresponding to words in the first sequence,</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> for tokens corresponding to words in the second sequence when a pair of sequences was jointly
encoded.</p></li>
</ul>
</div></blockquote>
<dl class="simple">
<dt>Args:</dt><dd><p>batch_index (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0): The index to access in the batch.</p>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Optional[int]]</span></code>: A list indicating the sequence id corresponding to each token. Special tokens
added by the tokenizer are mapped to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> and other tokens are mapped to the index of their
corresponding sequence.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BatchEncoding.to">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.device</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding" title="nlp_uncertainty_zoo.utils.data.BatchEncoding"><span class="pre">BatchEncoding</span></a></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Send all values to device by calling <code class="xref py py-obj docutils literal notranslate"><span class="pre">v.to(device)</span></code> (PyTorch only).</p>
<dl class="simple">
<dt>Args:</dt><dd><p>device (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.device</span></code>): The device to put the tensors on.</p>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code>: The same instance after modification.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BatchEncoding.token_to_chars">
<span class="sig-name descname"><span class="pre">token_to_chars</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_or_token_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">CharSpan</span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.token_to_chars" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the character span corresponding to an encoded token in a sequence of the batch.</p>
<p>Character spans are returned as a <code class="xref py py-class docutils literal notranslate"><span class="pre">CharSpan</span></code> with:</p>
<ul class="simple">
<li><p><strong>start</strong> – Index of the first character in the original string associated to the token.</p></li>
<li><p><strong>end</strong> – Index of the character following the last character in the original string associated to the
token.</p></li>
</ul>
<p>Can be called as:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">self.token_to_chars(token_index)</span></code> if batch size is 1</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.token_to_chars(batch_index,</span> <span class="pre">token_index)</span></code> if batch size is greater or equal to 1</p></li>
</ul>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>batch_or_token_index (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>):</dt><dd><p>Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of
the token in the sequence.</p>
</dd>
<dt>token_index (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>If a batch index is provided in <cite>batch_or_token_index</cite>, this can be the index of the token or tokens in
the sequence.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-class docutils literal notranslate"><span class="pre">CharSpan</span></code>: Span of characters in the original string.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BatchEncoding.token_to_sequence">
<span class="sig-name descname"><span class="pre">token_to_sequence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_or_token_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.token_to_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the index of the sequence represented by the given token. In the general use case, this method returns
<code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> for a single sequence or the first sequence of a pair, and <code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> for the second sequence of a pair</p>
<p>Can be called as:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">self.token_to_sequence(token_index)</span></code> if batch size is 1</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.token_to_sequence(batch_index,</span> <span class="pre">token_index)</span></code> if batch size is greater than 1</p></li>
</ul>
<p>This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e.,
words are defined by the user). In this case it allows to easily associate encoded tokens with provided
tokenized words.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>batch_or_token_index (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>):</dt><dd><p>Index of the sequence in the batch. If the batch only comprises one sequence, this can be the index of
the token in the sequence.</p>
</dd>
<dt>token_index (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>If a batch index is provided in <cite>batch_or_token_index</cite>, this can be the index of the token in the
sequence.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>: Index of the word in the input sequence.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BatchEncoding.token_to_word">
<span class="sig-name descname"><span class="pre">token_to_word</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_or_token_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.token_to_word" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the index of the word corresponding (i.e. comprising) to an encoded token in a sequence of the batch.</p>
<p>Can be called as:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">self.token_to_word(token_index)</span></code> if batch size is 1</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.token_to_word(batch_index,</span> <span class="pre">token_index)</span></code> if batch size is greater than 1</p></li>
</ul>
<p>This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e.,
words are defined by the user). In this case it allows to easily associate encoded tokens with provided
tokenized words.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>batch_or_token_index (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>):</dt><dd><p>Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of
the token in the sequence.</p>
</dd>
<dt>token_index (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>If a batch index is provided in <cite>batch_or_token_index</cite>, this can be the index of the token in the
sequence.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>: Index of the word in the input sequence.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BatchEncoding.tokens">
<span class="sig-name descname"><span class="pre">tokens</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.tokens" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the list of tokens (sub-parts of the input strings after word/subword splitting and before conversion to
integer indices) at a given batch index (only works for the output of a fast tokenizer).</p>
<dl class="simple">
<dt>Args:</dt><dd><p>batch_index (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0): The index to access in the batch.</p>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>: The list of tokens at that index.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BatchEncoding.values">
<span class="sig-name descname"><span class="pre">values</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">an</span> <span class="pre">object</span> <span class="pre">providing</span> <span class="pre">a</span> <span class="pre">view</span> <span class="pre">on</span> <span class="pre">D's</span> <span class="pre">values</span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.values" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BatchEncoding.word_ids">
<span class="sig-name descname"><span class="pre">word_ids</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.word_ids" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a list mapping the tokens to their actual word in the initial sentence for a fast tokenizer.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>batch_index (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0): The index to access in the batch.</p>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Optional[int]]</span></code>: A list indicating the word corresponding to each token. Special tokens added by
the tokenizer are mapped to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> and other tokens are mapped to the index of their corresponding
word (several tokens will be mapped to the same word index if they are parts of that word).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BatchEncoding.word_to_chars">
<span class="sig-name descname"><span class="pre">word_to_chars</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_or_word_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">word_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sequence_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">CharSpan</span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.word_to_chars" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the character span in the original string corresponding to given word in a sequence of the batch.</p>
<p>Character spans are returned as a CharSpan NamedTuple with:</p>
<ul class="simple">
<li><p>start: index of the first character in the original string</p></li>
<li><p>end: index of the character following the last character in the original string</p></li>
</ul>
<p>Can be called as:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">self.word_to_chars(word_index)</span></code> if batch size is 1</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.word_to_chars(batch_index,</span> <span class="pre">word_index)</span></code> if batch size is greater or equal to 1</p></li>
</ul>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>batch_or_word_index (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>):</dt><dd><p>Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of
the word in the sequence</p>
</dd>
<dt>word_index (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>If a batch index is provided in <cite>batch_or_token_index</cite>, this can be the index of the word in the
sequence.</p>
</dd>
<dt>sequence_index (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0):</dt><dd><p>If pair of sequences are encoded in the batch this can be used to specify which sequence in the pair (0
or 1) the provided word index belongs to.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">CharSpan</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[CharSpan]</span></code>: Span(s) of the associated character or characters in the string.
CharSpan are NamedTuple with:</p>
<blockquote>
<div><ul class="simple">
<li><p>start: index of the first character associated to the token in the original string</p></li>
<li><p>end: index of the character following the last character associated to the token in the original
string</p></li>
</ul>
</div></blockquote>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BatchEncoding.word_to_tokens">
<span class="sig-name descname"><span class="pre">word_to_tokens</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_or_word_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">word_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sequence_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">TokenSpan</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.word_to_tokens" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the encoded token span corresponding to a word in a sequence of the batch.</p>
<p>Token spans are returned as a <code class="xref py py-class docutils literal notranslate"><span class="pre">TokenSpan</span></code> with:</p>
<ul class="simple">
<li><p><strong>start</strong> – Index of the first token.</p></li>
<li><p><strong>end</strong> – Index of the token following the last token.</p></li>
</ul>
<p>Can be called as:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">self.word_to_tokens(word_index,</span> <span class="pre">sequence_index:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">0)</span></code> if batch size is 1</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.word_to_tokens(batch_index,</span> <span class="pre">word_index,</span> <span class="pre">sequence_index:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">0)</span></code> if batch size is greater or equal
to 1</p></li>
</ul>
<p>This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e. words
are defined by the user). In this case it allows to easily associate encoded tokens with provided tokenized
words.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>batch_or_word_index (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>):</dt><dd><p>Index of the sequence in the batch. If the batch only comprises one sequence, this can be the index of
the word in the sequence.</p>
</dd>
<dt>word_index (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>If a batch index is provided in <cite>batch_or_token_index</cite>, this can be the index of the word in the
sequence.</p>
</dd>
<dt>sequence_index (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0):</dt><dd><p>If pair of sequences are encoded in the batch this can be used to specify which sequence in the pair (0
or 1) the provided word index belongs to.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>Optional <code class="xref py py-class docutils literal notranslate"><span class="pre">TokenSpan</span></code> Span of tokens in the encoded sequence.
Returns <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> if no tokens correspond to the word.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BatchEncoding.words">
<span class="sig-name descname"><span class="pre">words</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding.words" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a list mapping the tokens to their actual word in the initial sentence for a fast tokenizer.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>batch_index (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0): The index to access in the batch.</p>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Optional[int]]</span></code>: A list indicating the word corresponding to each token. Special tokens added by
the tokenizer are mapped to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> and other tokens are mapped to the index of their corresponding
word (several tokens will be mapped to the same word index if they are parts of that word).</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BertTokenizer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.data.</span></span><span class="sig-name descname"><span class="pre">BertTokenizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vocab_file</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">do_lower_case</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">do_basic_tokenize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">never_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unk_token</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'[UNK]'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sep_token</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'[SEP]'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_token</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'[PAD]'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cls_token</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'[CLS]'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_token</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'[MASK]'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenize_chinese_chars</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strip_accents</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code></p>
<p>Construct a BERT tokenizer. Based on WordPiece.</p>
<p>This tokenizer inherits from <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code> which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.</p>
<dl>
<dt>Args:</dt><dd><dl>
<dt>vocab_file (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>):</dt><dd><p>File containing the vocabulary.</p>
</dd>
<dt>do_lower_case (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to lowercase the input when tokenizing.</p>
</dd>
<dt>do_basic_tokenize (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to do basic tokenization before WordPiece.</p>
</dd>
<dt>never_split (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Iterable</span></code>, <cite>optional</cite>):</dt><dd><p>Collection of tokens which will never be split during tokenization. Only has an effect when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">do_basic_tokenize=True</span></code></p>
</dd>
<dt>unk_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;[UNK]&quot;</span></code>):</dt><dd><p>The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.</p>
</dd>
<dt>sep_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;[SEP]&quot;</span></code>):</dt><dd><p>The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.</p>
</dd>
<dt>pad_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;[PAD]&quot;</span></code>):</dt><dd><p>The token used for padding, for example when batching sequences of different lengths.</p>
</dd>
<dt>cls_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;[CLS]&quot;</span></code>):</dt><dd><p>The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.</p>
</dd>
<dt>mask_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;[MASK]&quot;</span></code>):</dt><dd><p>The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.</p>
</dd>
<dt>tokenize_chinese_chars (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to tokenize Chinese characters.</p>
<p>This should likely be deactivated for Japanese (see this <a class="reference external" href="https://github.com/huggingface/transformers/issues/328">issue</a>).</p>
</dd>
<dt>strip_accents: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>):</dt><dd><p>Whether or not to strip all accents. If this option is not specified, then it will be determined by the
value for <code class="xref py py-obj docutils literal notranslate"><span class="pre">lowercase</span></code> (as in the original BERT).</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BertTokenizer.build_inputs_with_special_tokens">
<span class="sig-name descname"><span class="pre">build_inputs_with_special_tokens</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">token_ids_0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_ids_1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer.build_inputs_with_special_tokens" title="Permalink to this definition">¶</a></dt>
<dd><p>Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:</p>
<ul class="simple">
<li><p>single sequence: <code class="docutils literal notranslate"><span class="pre">[CLS]</span> <span class="pre">X</span> <span class="pre">[SEP]</span></code></p></li>
<li><p>pair of sequences: <code class="docutils literal notranslate"><span class="pre">[CLS]</span> <span class="pre">A</span> <span class="pre">[SEP]</span> <span class="pre">B</span> <span class="pre">[SEP]</span></code></p></li>
</ul>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>token_ids_0 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>):</dt><dd><p>List of IDs to which the special tokens will be added.</p>
</dd>
<dt>token_ids_1 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>):</dt><dd><p>Optional second list of IDs for sequence pairs.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>: List of <a class="reference external" href="../glossary.html#input-ids">input IDs</a> with the appropriate special tokens.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BertTokenizer.convert_tokens_to_string">
<span class="sig-name descname"><span class="pre">convert_tokens_to_string</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokens</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer.convert_tokens_to_string" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a sequence of tokens (string) in a single string.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BertTokenizer.create_token_type_ids_from_sequences">
<span class="sig-name descname"><span class="pre">create_token_type_ids_from_sequences</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">token_ids_0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_ids_1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer.create_token_type_ids_from_sequences" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence
pair mask has the following format:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span>
<span class="o">|</span> <span class="n">first</span> <span class="n">sequence</span>    <span class="o">|</span> <span class="n">second</span> <span class="n">sequence</span> <span class="o">|</span>
</pre></div>
</div>
<p>If <code class="xref py py-obj docutils literal notranslate"><span class="pre">token_ids_1</span></code> is <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this method only returns the first portion of the mask (0s).</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>token_ids_0 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>):</dt><dd><p>List of IDs.</p>
</dd>
<dt>token_ids_1 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>):</dt><dd><p>Optional second list of IDs for sequence pairs.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>: List of <a class="reference external" href="../glossary.html#token-type-ids">token type IDs</a> according to the given
sequence(s).</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BertTokenizer.do_lower_case">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">do_lower_case</span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer.do_lower_case" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BertTokenizer.get_special_tokens_mask">
<span class="sig-name descname"><span class="pre">get_special_tokens_mask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">token_ids_0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_ids_1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">already_has_special_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer.get_special_tokens_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code class="docutils literal notranslate"><span class="pre">prepare_for_model</span></code> method.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>token_ids_0 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>):</dt><dd><p>List of IDs.</p>
</dd>
<dt>token_ids_1 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>):</dt><dd><p>Optional second list of IDs for sequence pairs.</p>
</dd>
<dt>already_has_special_tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not the token list is already formatted with special tokens for the model.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BertTokenizer.get_vocab">
<span class="sig-name descname"><span class="pre">get_vocab</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer.get_vocab" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the vocabulary as a dictionary of token to index.</p>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizer.get_vocab()[token]</span></code> is equivalent to <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizer.convert_tokens_to_ids(token)</span></code> when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">token</span></code> is in the vocab.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">int]</span></code>: The vocabulary.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BertTokenizer.max_model_input_sizes">
<span class="sig-name descname"><span class="pre">max_model_input_sizes</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'TurkuNLP/bert-base-finnish-cased-v1':</span> <span class="pre">512,</span> <span class="pre">'TurkuNLP/bert-base-finnish-uncased-v1':</span> <span class="pre">512,</span> <span class="pre">'bert-base-cased':</span> <span class="pre">512,</span> <span class="pre">'bert-base-cased-finetuned-mrpc':</span> <span class="pre">512,</span> <span class="pre">'bert-base-chinese':</span> <span class="pre">512,</span> <span class="pre">'bert-base-german-cased':</span> <span class="pre">512,</span> <span class="pre">'bert-base-german-dbmdz-cased':</span> <span class="pre">512,</span> <span class="pre">'bert-base-german-dbmdz-uncased':</span> <span class="pre">512,</span> <span class="pre">'bert-base-multilingual-cased':</span> <span class="pre">512,</span> <span class="pre">'bert-base-multilingual-uncased':</span> <span class="pre">512,</span> <span class="pre">'bert-base-uncased':</span> <span class="pre">512,</span> <span class="pre">'bert-large-cased':</span> <span class="pre">512,</span> <span class="pre">'bert-large-cased-whole-word-masking':</span> <span class="pre">512,</span> <span class="pre">'bert-large-cased-whole-word-masking-finetuned-squad':</span> <span class="pre">512,</span> <span class="pre">'bert-large-uncased':</span> <span class="pre">512,</span> <span class="pre">'bert-large-uncased-whole-word-masking':</span> <span class="pre">512,</span> <span class="pre">'bert-large-uncased-whole-word-masking-finetuned-squad':</span> <span class="pre">512,</span> <span class="pre">'wietsedv/bert-base-dutch-cased':</span> <span class="pre">512}</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer.max_model_input_sizes" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BertTokenizer.pretrained_init_configuration">
<span class="sig-name descname"><span class="pre">pretrained_init_configuration</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'TurkuNLP/bert-base-finnish-cased-v1':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">False},</span> <span class="pre">'TurkuNLP/bert-base-finnish-uncased-v1':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">True},</span> <span class="pre">'bert-base-cased':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">False},</span> <span class="pre">'bert-base-cased-finetuned-mrpc':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">False},</span> <span class="pre">'bert-base-chinese':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">False},</span> <span class="pre">'bert-base-german-cased':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">False},</span> <span class="pre">'bert-base-german-dbmdz-cased':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">False},</span> <span class="pre">'bert-base-german-dbmdz-uncased':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">True},</span> <span class="pre">'bert-base-multilingual-cased':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">False},</span> <span class="pre">'bert-base-multilingual-uncased':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">True},</span> <span class="pre">'bert-base-uncased':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">True},</span> <span class="pre">'bert-large-cased':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">False},</span> <span class="pre">'bert-large-cased-whole-word-masking':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">False},</span> <span class="pre">'bert-large-cased-whole-word-masking-finetuned-squad':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">False},</span> <span class="pre">'bert-large-uncased':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">True},</span> <span class="pre">'bert-large-uncased-whole-word-masking':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">True},</span> <span class="pre">'bert-large-uncased-whole-word-masking-finetuned-squad':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">True},</span> <span class="pre">'wietsedv/bert-base-dutch-cased':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">False}}</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer.pretrained_init_configuration" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BertTokenizer.pretrained_vocab_files_map">
<span class="sig-name descname"><span class="pre">pretrained_vocab_files_map</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'vocab_file':</span> <span class="pre">{'TurkuNLP/bert-base-finnish-cased-v1':</span> <span class="pre">'https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/vocab.txt',</span> <span class="pre">'TurkuNLP/bert-base-finnish-uncased-v1':</span> <span class="pre">'https://huggingface.co/TurkuNLP/bert-base-finnish-uncased-v1/resolve/main/vocab.txt',</span> <span class="pre">'bert-base-cased':</span> <span class="pre">'https://huggingface.co/bert-base-cased/resolve/main/vocab.txt',</span> <span class="pre">'bert-base-cased-finetuned-mrpc':</span> <span class="pre">'https://huggingface.co/bert-base-cased-finetuned-mrpc/resolve/main/vocab.txt',</span> <span class="pre">'bert-base-chinese':</span> <span class="pre">'https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt',</span> <span class="pre">'bert-base-german-cased':</span> <span class="pre">'https://huggingface.co/bert-base-german-cased/resolve/main/vocab.txt',</span> <span class="pre">'bert-base-german-dbmdz-cased':</span> <span class="pre">'https://huggingface.co/bert-base-german-dbmdz-cased/resolve/main/vocab.txt',</span> <span class="pre">'bert-base-german-dbmdz-uncased':</span> <span class="pre">'https://huggingface.co/bert-base-german-dbmdz-uncased/resolve/main/vocab.txt',</span> <span class="pre">'bert-base-multilingual-cased':</span> <span class="pre">'https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt',</span> <span class="pre">'bert-base-multilingual-uncased':</span> <span class="pre">'https://huggingface.co/bert-base-multilingual-uncased/resolve/main/vocab.txt',</span> <span class="pre">'bert-base-uncased':</span> <span class="pre">'https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt',</span> <span class="pre">'bert-large-cased':</span> <span class="pre">'https://huggingface.co/bert-large-cased/resolve/main/vocab.txt',</span> <span class="pre">'bert-large-cased-whole-word-masking':</span> <span class="pre">'https://huggingface.co/bert-large-cased-whole-word-masking/resolve/main/vocab.txt',</span> <span class="pre">'bert-large-cased-whole-word-masking-finetuned-squad':</span> <span class="pre">'https://huggingface.co/bert-large-cased-whole-word-masking-finetuned-squad/resolve/main/vocab.txt',</span> <span class="pre">'bert-large-uncased':</span> <span class="pre">'https://huggingface.co/bert-large-uncased/resolve/main/vocab.txt',</span> <span class="pre">'bert-large-uncased-whole-word-masking':</span> <span class="pre">'https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/vocab.txt',</span> <span class="pre">'bert-large-uncased-whole-word-masking-finetuned-squad':</span> <span class="pre">'https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/vocab.txt',</span> <span class="pre">'wietsedv/bert-base-dutch-cased':</span> <span class="pre">'https://huggingface.co/wietsedv/bert-base-dutch-cased/resolve/main/vocab.txt'}}</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer.pretrained_vocab_files_map" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BertTokenizer.save_vocabulary">
<span class="sig-name descname"><span class="pre">save_vocabulary</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save_directory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filename_prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.Tuple" title="typing.Tuple"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer.save_vocabulary" title="Permalink to this definition">¶</a></dt>
<dd><p>Save only the vocabulary of the tokenizer (vocabulary + added tokens).</p>
<p>This method won’t save the configuration and special token mappings of the tokenizer. Use
<code class="xref py py-meth docutils literal notranslate"><span class="pre">_save_pretrained()</span></code> to save the whole state of the tokenizer.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>save_directory (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>):</dt><dd><p>The directory in which to save the vocabulary.</p>
</dd>
<dt>filename_prefix (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>):</dt><dd><p>An optional prefix to add to the named of the saved files.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple(str)</span></code>: Paths to the files saved.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BertTokenizer.vocab_files_names">
<span class="sig-name descname"><span class="pre">vocab_files_names</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'vocab_file':</span> <span class="pre">'vocab.txt'}</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer.vocab_files_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BertTokenizer.vocab_size">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">vocab_size</span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer.vocab_size" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>: Size of the base vocabulary (without the added tokens).</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BertTokenizerFast">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.data.</span></span><span class="sig-name descname"><span class="pre">BertTokenizerFast</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vocab_file</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">do_lower_case</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unk_token</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'[UNK]'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sep_token</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'[SEP]'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_token</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'[PAD]'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cls_token</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'[CLS]'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_token</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'[MASK]'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenize_chinese_chars</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strip_accents</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BertTokenizerFast" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code></p>
<p>Construct a “fast” BERT tokenizer (backed by HuggingFace’s <cite>tokenizers</cite> library). Based on WordPiece.</p>
<p>This tokenizer inherits from <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code> which contains most of the main
methods. Users should refer to this superclass for more information regarding those methods.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>vocab_file (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>):</dt><dd><p>File containing the vocabulary.</p>
</dd>
<dt>do_lower_case (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to lowercase the input when tokenizing.</p>
</dd>
<dt>unk_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;[UNK]&quot;</span></code>):</dt><dd><p>The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.</p>
</dd>
<dt>sep_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;[SEP]&quot;</span></code>):</dt><dd><p>The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.</p>
</dd>
<dt>pad_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;[PAD]&quot;</span></code>):</dt><dd><p>The token used for padding, for example when batching sequences of different lengths.</p>
</dd>
<dt>cls_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;[CLS]&quot;</span></code>):</dt><dd><p>The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.</p>
</dd>
<dt>mask_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;[MASK]&quot;</span></code>):</dt><dd><p>The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.</p>
</dd>
<dt>clean_text (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to clean the text before tokenization by removing any control characters and replacing all
whitespaces by the classic one.</p>
</dd>
<dt>tokenize_chinese_chars (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see <a class="reference external" href="https://github.com/huggingface/transformers/issues/328">this
issue</a>).</p>
</dd>
<dt>strip_accents: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>):</dt><dd><p>Whether or not to strip all accents. If this option is not specified, then it will be determined by the
value for <code class="xref py py-obj docutils literal notranslate"><span class="pre">lowercase</span></code> (as in the original BERT).</p>
</dd>
<dt>wordpieces_prefix: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;##&quot;</span></code>):</dt><dd><p>The prefix for subwords.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BertTokenizerFast.build_inputs_with_special_tokens">
<span class="sig-name descname"><span class="pre">build_inputs_with_special_tokens</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">token_ids_0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_ids_1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BertTokenizerFast.build_inputs_with_special_tokens" title="Permalink to this definition">¶</a></dt>
<dd><p>Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:</p>
<ul class="simple">
<li><p>single sequence: <code class="docutils literal notranslate"><span class="pre">[CLS]</span> <span class="pre">X</span> <span class="pre">[SEP]</span></code></p></li>
<li><p>pair of sequences: <code class="docutils literal notranslate"><span class="pre">[CLS]</span> <span class="pre">A</span> <span class="pre">[SEP]</span> <span class="pre">B</span> <span class="pre">[SEP]</span></code></p></li>
</ul>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>token_ids_0 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>):</dt><dd><p>List of IDs to which the special tokens will be added.</p>
</dd>
<dt>token_ids_1 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>):</dt><dd><p>Optional second list of IDs for sequence pairs.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>: List of <a class="reference external" href="../glossary.html#input-ids">input IDs</a> with the appropriate special tokens.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BertTokenizerFast.create_token_type_ids_from_sequences">
<span class="sig-name descname"><span class="pre">create_token_type_ids_from_sequences</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">token_ids_0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_ids_1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BertTokenizerFast.create_token_type_ids_from_sequences" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence
pair mask has the following format:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span>
<span class="o">|</span> <span class="n">first</span> <span class="n">sequence</span>    <span class="o">|</span> <span class="n">second</span> <span class="n">sequence</span> <span class="o">|</span>
</pre></div>
</div>
<p>If <code class="xref py py-obj docutils literal notranslate"><span class="pre">token_ids_1</span></code> is <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this method only returns the first portion of the mask (0s).</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>token_ids_0 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>):</dt><dd><p>List of IDs.</p>
</dd>
<dt>token_ids_1 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>):</dt><dd><p>Optional second list of IDs for sequence pairs.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>: List of <a class="reference external" href="../glossary.html#token-type-ids">token type IDs</a> according to the given
sequence(s).</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BertTokenizerFast.max_model_input_sizes">
<span class="sig-name descname"><span class="pre">max_model_input_sizes</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.Dict" title="nlp_uncertainty_zoo.utils.data.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'TurkuNLP/bert-base-finnish-cased-v1':</span> <span class="pre">512,</span> <span class="pre">'TurkuNLP/bert-base-finnish-uncased-v1':</span> <span class="pre">512,</span> <span class="pre">'bert-base-cased':</span> <span class="pre">512,</span> <span class="pre">'bert-base-cased-finetuned-mrpc':</span> <span class="pre">512,</span> <span class="pre">'bert-base-chinese':</span> <span class="pre">512,</span> <span class="pre">'bert-base-german-cased':</span> <span class="pre">512,</span> <span class="pre">'bert-base-german-dbmdz-cased':</span> <span class="pre">512,</span> <span class="pre">'bert-base-german-dbmdz-uncased':</span> <span class="pre">512,</span> <span class="pre">'bert-base-multilingual-cased':</span> <span class="pre">512,</span> <span class="pre">'bert-base-multilingual-uncased':</span> <span class="pre">512,</span> <span class="pre">'bert-base-uncased':</span> <span class="pre">512,</span> <span class="pre">'bert-large-cased':</span> <span class="pre">512,</span> <span class="pre">'bert-large-cased-whole-word-masking':</span> <span class="pre">512,</span> <span class="pre">'bert-large-cased-whole-word-masking-finetuned-squad':</span> <span class="pre">512,</span> <span class="pre">'bert-large-uncased':</span> <span class="pre">512,</span> <span class="pre">'bert-large-uncased-whole-word-masking':</span> <span class="pre">512,</span> <span class="pre">'bert-large-uncased-whole-word-masking-finetuned-squad':</span> <span class="pre">512,</span> <span class="pre">'wietsedv/bert-base-dutch-cased':</span> <span class="pre">512}</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BertTokenizerFast.max_model_input_sizes" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BertTokenizerFast.pretrained_init_configuration">
<span class="sig-name descname"><span class="pre">pretrained_init_configuration</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.Dict" title="nlp_uncertainty_zoo.utils.data.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.Dict" title="nlp_uncertainty_zoo.utils.data.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'TurkuNLP/bert-base-finnish-cased-v1':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">False},</span> <span class="pre">'TurkuNLP/bert-base-finnish-uncased-v1':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">True},</span> <span class="pre">'bert-base-cased':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">False},</span> <span class="pre">'bert-base-cased-finetuned-mrpc':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">False},</span> <span class="pre">'bert-base-chinese':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">False},</span> <span class="pre">'bert-base-german-cased':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">False},</span> <span class="pre">'bert-base-german-dbmdz-cased':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">False},</span> <span class="pre">'bert-base-german-dbmdz-uncased':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">True},</span> <span class="pre">'bert-base-multilingual-cased':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">False},</span> <span class="pre">'bert-base-multilingual-uncased':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">True},</span> <span class="pre">'bert-base-uncased':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">True},</span> <span class="pre">'bert-large-cased':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">False},</span> <span class="pre">'bert-large-cased-whole-word-masking':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">False},</span> <span class="pre">'bert-large-cased-whole-word-masking-finetuned-squad':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">False},</span> <span class="pre">'bert-large-uncased':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">True},</span> <span class="pre">'bert-large-uncased-whole-word-masking':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">True},</span> <span class="pre">'bert-large-uncased-whole-word-masking-finetuned-squad':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">True},</span> <span class="pre">'wietsedv/bert-base-dutch-cased':</span> <span class="pre">{'do_lower_case':</span> <span class="pre">False}}</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BertTokenizerFast.pretrained_init_configuration" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BertTokenizerFast.pretrained_vocab_files_map">
<span class="sig-name descname"><span class="pre">pretrained_vocab_files_map</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.Dict" title="nlp_uncertainty_zoo.utils.data.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.Dict" title="nlp_uncertainty_zoo.utils.data.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'tokenizer_file':</span> <span class="pre">{'TurkuNLP/bert-base-finnish-cased-v1':</span> <span class="pre">'https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/tokenizer.json',</span> <span class="pre">'TurkuNLP/bert-base-finnish-uncased-v1':</span> <span class="pre">'https://huggingface.co/TurkuNLP/bert-base-finnish-uncased-v1/resolve/main/tokenizer.json',</span> <span class="pre">'bert-base-cased':</span> <span class="pre">'https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json',</span> <span class="pre">'bert-base-cased-finetuned-mrpc':</span> <span class="pre">'https://huggingface.co/bert-base-cased-finetuned-mrpc/resolve/main/tokenizer.json',</span> <span class="pre">'bert-base-chinese':</span> <span class="pre">'https://huggingface.co/bert-base-chinese/resolve/main/tokenizer.json',</span> <span class="pre">'bert-base-german-cased':</span> <span class="pre">'https://huggingface.co/bert-base-german-cased/resolve/main/tokenizer.json',</span> <span class="pre">'bert-base-german-dbmdz-cased':</span> <span class="pre">'https://huggingface.co/bert-base-german-dbmdz-cased/resolve/main/tokenizer.json',</span> <span class="pre">'bert-base-german-dbmdz-uncased':</span> <span class="pre">'https://huggingface.co/bert-base-german-dbmdz-uncased/resolve/main/tokenizer.json',</span> <span class="pre">'bert-base-multilingual-cased':</span> <span class="pre">'https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json',</span> <span class="pre">'bert-base-multilingual-uncased':</span> <span class="pre">'https://huggingface.co/bert-base-multilingual-uncased/resolve/main/tokenizer.json',</span> <span class="pre">'bert-base-uncased':</span> <span class="pre">'https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json',</span> <span class="pre">'bert-large-cased':</span> <span class="pre">'https://huggingface.co/bert-large-cased/resolve/main/tokenizer.json',</span> <span class="pre">'bert-large-cased-whole-word-masking':</span> <span class="pre">'https://huggingface.co/bert-large-cased-whole-word-masking/resolve/main/tokenizer.json',</span> <span class="pre">'bert-large-cased-whole-word-masking-finetuned-squad':</span> <span class="pre">'https://huggingface.co/bert-large-cased-whole-word-masking-finetuned-squad/resolve/main/tokenizer.json',</span> <span class="pre">'bert-large-uncased':</span> <span class="pre">'https://huggingface.co/bert-large-uncased/resolve/main/tokenizer.json',</span> <span class="pre">'bert-large-uncased-whole-word-masking':</span> <span class="pre">'https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer.json',</span> <span class="pre">'bert-large-uncased-whole-word-masking-finetuned-squad':</span> <span class="pre">'https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/tokenizer.json',</span> <span class="pre">'wietsedv/bert-base-dutch-cased':</span> <span class="pre">'https://huggingface.co/wietsedv/bert-base-dutch-cased/resolve/main/tokenizer.json'},</span> <span class="pre">'vocab_file':</span> <span class="pre">{'TurkuNLP/bert-base-finnish-cased-v1':</span> <span class="pre">'https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/vocab.txt',</span> <span class="pre">'TurkuNLP/bert-base-finnish-uncased-v1':</span> <span class="pre">'https://huggingface.co/TurkuNLP/bert-base-finnish-uncased-v1/resolve/main/vocab.txt',</span> <span class="pre">'bert-base-cased':</span> <span class="pre">'https://huggingface.co/bert-base-cased/resolve/main/vocab.txt',</span> <span class="pre">'bert-base-cased-finetuned-mrpc':</span> <span class="pre">'https://huggingface.co/bert-base-cased-finetuned-mrpc/resolve/main/vocab.txt',</span> <span class="pre">'bert-base-chinese':</span> <span class="pre">'https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt',</span> <span class="pre">'bert-base-german-cased':</span> <span class="pre">'https://huggingface.co/bert-base-german-cased/resolve/main/vocab.txt',</span> <span class="pre">'bert-base-german-dbmdz-cased':</span> <span class="pre">'https://huggingface.co/bert-base-german-dbmdz-cased/resolve/main/vocab.txt',</span> <span class="pre">'bert-base-german-dbmdz-uncased':</span> <span class="pre">'https://huggingface.co/bert-base-german-dbmdz-uncased/resolve/main/vocab.txt',</span> <span class="pre">'bert-base-multilingual-cased':</span> <span class="pre">'https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt',</span> <span class="pre">'bert-base-multilingual-uncased':</span> <span class="pre">'https://huggingface.co/bert-base-multilingual-uncased/resolve/main/vocab.txt',</span> <span class="pre">'bert-base-uncased':</span> <span class="pre">'https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt',</span> <span class="pre">'bert-large-cased':</span> <span class="pre">'https://huggingface.co/bert-large-cased/resolve/main/vocab.txt',</span> <span class="pre">'bert-large-cased-whole-word-masking':</span> <span class="pre">'https://huggingface.co/bert-large-cased-whole-word-masking/resolve/main/vocab.txt',</span> <span class="pre">'bert-large-cased-whole-word-masking-finetuned-squad':</span> <span class="pre">'https://huggingface.co/bert-large-cased-whole-word-masking-finetuned-squad/resolve/main/vocab.txt',</span> <span class="pre">'bert-large-uncased':</span> <span class="pre">'https://huggingface.co/bert-large-uncased/resolve/main/vocab.txt',</span> <span class="pre">'bert-large-uncased-whole-word-masking':</span> <span class="pre">'https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/vocab.txt',</span> <span class="pre">'bert-large-uncased-whole-word-masking-finetuned-squad':</span> <span class="pre">'https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/vocab.txt',</span> <span class="pre">'wietsedv/bert-base-dutch-cased':</span> <span class="pre">'https://huggingface.co/wietsedv/bert-base-dutch-cased/resolve/main/vocab.txt'}}</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BertTokenizerFast.pretrained_vocab_files_map" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BertTokenizerFast.save_vocabulary">
<span class="sig-name descname"><span class="pre">save_vocabulary</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save_directory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filename_prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.Tuple" title="typing.Tuple"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BertTokenizerFast.save_vocabulary" title="Permalink to this definition">¶</a></dt>
<dd><p>Save only the vocabulary of the tokenizer (vocabulary + added tokens).</p>
<p>This method won’t save the configuration and special token mappings of the tokenizer. Use
<code class="xref py py-meth docutils literal notranslate"><span class="pre">_save_pretrained()</span></code> to save the whole state of the tokenizer.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>save_directory (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>):</dt><dd><p>The directory in which to save the vocabulary.</p>
</dd>
<dt>filename_prefix (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>):</dt><dd><p>An optional prefix to add to the named of the saved files.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple(str)</span></code>: Paths to the files saved.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BertTokenizerFast.slow_tokenizer_class">
<span class="sig-name descname"><span class="pre">slow_tokenizer_class</span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BertTokenizerFast.slow_tokenizer_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BertTokenizer" title="transformers.models.bert.tokenization_bert.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertTokenizer</span></code></a></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.BertTokenizerFast.vocab_files_names">
<span class="sig-name descname"><span class="pre">vocab_files_names</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.Dict" title="nlp_uncertainty_zoo.utils.data.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'tokenizer_file':</span> <span class="pre">'tokenizer.json',</span> <span class="pre">'vocab_file':</span> <span class="pre">'vocab.txt'}</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.BertTokenizerFast.vocab_files_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.ClassificationDatasetBuilder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.data.</span></span><span class="sig-name descname"><span class="pre">ClassificationDatasetBuilder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">splits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">type_</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase"><span class="pre">PreTrainedTokenizerBase</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampler_class</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.Type" title="typing.Type"><span class="pre">Type</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampler_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_jobs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.ClassificationDatasetBuilder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DatasetBuilder" title="nlp_uncertainty_zoo.utils.data.DatasetBuilder"><code class="xref py py-class docutils literal notranslate"><span class="pre">DatasetBuilder</span></code></a></p>
<p>DatasetBuilder for classification datasets. This includes sequence classification and token classification /
sequence labelling.</p>
<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.ClassificationDatasetBuilder.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">dataloader_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.DataLoader" title="torch.utils.data.dataloader.DataLoader"><span class="pre">DataLoader</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.ClassificationDatasetBuilder.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Build a language modelling dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>batch_size: int</strong></dt><dd><p>The desired batch size.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>Dict[str, DataLoader]</dt><dd><p>Dictionary of DataLoaders for every given split.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.DataCollatorForLanguageModeling">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.data.</span></span><span class="sig-name descname"><span class="pre">DataCollatorForLanguageModeling</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase"><span class="pre">PreTrainedTokenizerBase</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlm_probability</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.15</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.DataCollatorForLanguageModeling" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Data collator used for language modeling. Inputs are dynamically padded to the maximum length of a batch if they
are not all of the same length.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>tokenizer (<code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code>):</dt><dd><p>The tokenizer used for encoding the data.</p>
</dd>
<dt>mlm (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to use masked language modeling. If set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>, the labels are the same as the
inputs with the padding tokens ignored (by setting them to -100). Otherwise, the labels are -100 for
non-masked tokens and the value to predict for the masked token.</p>
</dd>
<dt>mlm_probability (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.15):</dt><dd><p>The probability with which to (randomly) mask tokens in the input, when <a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataCollatorForLanguageModeling.mlm" title="nlp_uncertainty_zoo.utils.data.DataCollatorForLanguageModeling.mlm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mlm</span></code></a> is set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best performance, this data collator should be used with a dataset having items that are dictionaries or
BatchEncoding, with the <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;special_tokens_mask&quot;</span></code> key, as returned by a
<code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code> or a <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code> with the
argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_special_tokens_mask=True</span></code>.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.DataCollatorForLanguageModeling.mask_tokens">
<span class="sig-name descname"><span class="pre">mask_tokens</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">special_tokens_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.Tuple" title="typing.Tuple"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.DataCollatorForLanguageModeling.mask_tokens" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.DataCollatorForLanguageModeling.mlm">
<span class="sig-name descname"><span class="pre">mlm</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.DataCollatorForLanguageModeling.mlm" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.DataCollatorForLanguageModeling.mlm_probability">
<span class="sig-name descname"><span class="pre">mlm_probability</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0.15</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.DataCollatorForLanguageModeling.mlm_probability" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.DataCollatorForLanguageModeling.tokenizer">
<span class="sig-name descname"><span class="pre">tokenizer</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase"><span class="pre">PreTrainedTokenizerBase</span></a></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.DataCollatorForLanguageModeling.tokenizer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.DataLoader">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.data.</span></span><span class="sig-name descname"><span class="pre">DataLoader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dataset</span><span class="p"><span class="pre">[</span></span><span class="pre">T_co</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampler</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.Sampler" title="torch.utils.data.sampler.Sampler"><span class="pre">Sampler</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_sampler</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.Sampler" title="torch.utils.data.sampler.Sampler"><span class="pre">Sampler</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">collate_fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">T</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_last</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">worker_init_fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multiprocessing_context</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefetch_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">persistent_workers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.DataLoader" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Generic</span></code>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">T_co</span></code>]</p>
<p>Data loader. Combines a dataset and a sampler, and provides an iterable over
the given dataset.</p>
<p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code> supports both map-style and
iterable-style datasets with single- or multi-process loading, customizing
loading order and optional automatic batching (collation) and memory pinning.</p>
<p>See <code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.utils.data</span></code> documentation page for more details.</p>
<dl>
<dt>Args:</dt><dd><p>dataset (Dataset): dataset from which to load the data.
batch_size (int, optional): how many samples per batch to load</p>
<blockquote>
<div><p>(default: <code class="docutils literal notranslate"><span class="pre">1</span></code>).</p>
</div></blockquote>
<dl class="simple">
<dt>shuffle (bool, optional): set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to have the data reshuffled</dt><dd><p>at every epoch (default: <code class="docutils literal notranslate"><span class="pre">False</span></code>).</p>
</dd>
<dt>sampler (Sampler or Iterable, optional): defines the strategy to draw</dt><dd><p>samples from the dataset. Can be any <code class="docutils literal notranslate"><span class="pre">Iterable</span></code> with <code class="docutils literal notranslate"><span class="pre">__len__</span></code>
implemented. If specified, <code class="xref py py-attr docutils literal notranslate"><span class="pre">shuffle</span></code> must not be specified.</p>
</dd>
<dt>batch_sampler (Sampler or Iterable, optional): like <a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataLoader.sampler" title="nlp_uncertainty_zoo.utils.data.DataLoader.sampler"><code class="xref py py-attr docutils literal notranslate"><span class="pre">sampler</span></code></a>, but</dt><dd><p>returns a batch of indices at a time. Mutually exclusive with
<a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataLoader.batch_size" title="nlp_uncertainty_zoo.utils.data.DataLoader.batch_size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">batch_size</span></code></a>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">shuffle</span></code>, <a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataLoader.sampler" title="nlp_uncertainty_zoo.utils.data.DataLoader.sampler"><code class="xref py py-attr docutils literal notranslate"><span class="pre">sampler</span></code></a>,
and <a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataLoader.drop_last" title="nlp_uncertainty_zoo.utils.data.DataLoader.drop_last"><code class="xref py py-attr docutils literal notranslate"><span class="pre">drop_last</span></code></a>.</p>
</dd>
<dt>num_workers (int, optional): how many subprocesses to use for data</dt><dd><p>loading. <code class="docutils literal notranslate"><span class="pre">0</span></code> means that the data will be loaded in the main process.
(default: <code class="docutils literal notranslate"><span class="pre">0</span></code>)</p>
</dd>
<dt>collate_fn (callable, optional): merges a list of samples to form a</dt><dd><p>mini-batch of Tensor(s).  Used when using batched loading from a
map-style dataset.</p>
</dd>
<dt>pin_memory (bool, optional): If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the data loader will copy Tensors</dt><dd><p>into CUDA pinned memory before returning them.  If your data elements
are a custom type, or your <code class="xref py py-attr docutils literal notranslate"><span class="pre">collate_fn</span></code> returns a batch that is a custom type,
see the example below.</p>
</dd>
<dt>drop_last (bool, optional): set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to drop the last incomplete batch,</dt><dd><p>if the dataset size is not divisible by the batch size. If <code class="docutils literal notranslate"><span class="pre">False</span></code> and
the size of dataset is not divisible by the batch size, then the last batch
will be smaller. (default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p>
</dd>
<dt>timeout (numeric, optional): if positive, the timeout value for collecting a batch</dt><dd><p>from workers. Should always be non-negative. (default: <code class="docutils literal notranslate"><span class="pre">0</span></code>)</p>
</dd>
<dt>worker_init_fn (callable, optional): If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, this will be called on each</dt><dd><p>worker subprocess with the worker id (an int in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">num_workers</span> <span class="pre">-</span> <span class="pre">1]</span></code>) as
input, after seeding and before data loading. (default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p>
</dd>
<dt>generator (torch.Generator, optional): If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, this RNG will be used</dt><dd><p>by RandomSampler to generate random indexes and multiprocessing to generate
<cite>base_seed</cite> for workers. (default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p>
</dd>
<dt>prefetch_factor (int, optional, keyword-only arg): Number of samples loaded</dt><dd><p>in advance by each worker. <code class="docutils literal notranslate"><span class="pre">2</span></code> means there will be a total of
2 * num_workers samples prefetched across all workers. (default: <code class="docutils literal notranslate"><span class="pre">2</span></code>)</p>
</dd>
<dt>persistent_workers (bool, optional): If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the data loader will not shutdown</dt><dd><p>the worker processes after a dataset has been consumed once. This allows to
maintain the workers <cite>Dataset</cite> instances alive. (default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">spawn</span></code> start method is used, <code class="xref py py-attr docutils literal notranslate"><span class="pre">worker_init_fn</span></code>
cannot be an unpicklable object, e.g., a lambda function. See
<span class="xref std std-ref">multiprocessing-best-practices</span> on more details related
to multiprocessing in PyTorch.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">len(dataloader)</span></code> heuristic is based on the length of the sampler used.
When <a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataLoader.dataset" title="nlp_uncertainty_zoo.utils.data.DataLoader.dataset"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dataset</span></code></a> is an <code class="xref py py-class docutils literal notranslate"><span class="pre">IterableDataset</span></code>,
it instead returns an estimate based on <code class="docutils literal notranslate"><span class="pre">len(dataset)</span> <span class="pre">/</span> <span class="pre">batch_size</span></code>, with proper
rounding depending on <a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataLoader.drop_last" title="nlp_uncertainty_zoo.utils.data.DataLoader.drop_last"><code class="xref py py-attr docutils literal notranslate"><span class="pre">drop_last</span></code></a>, regardless of multi-process loading
configurations. This represents the best guess PyTorch can make because PyTorch
trusts user <a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataLoader.dataset" title="nlp_uncertainty_zoo.utils.data.DataLoader.dataset"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dataset</span></code></a> code in correctly handling multi-process
loading to avoid duplicate data.</p>
<p>However, if sharding results in multiple workers having incomplete last batches,
this estimate can still be inaccurate, because (1) an otherwise complete batch can
be broken into multiple ones and (2) more than one batch worth of samples can be
dropped when <a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataLoader.drop_last" title="nlp_uncertainty_zoo.utils.data.DataLoader.drop_last"><code class="xref py py-attr docutils literal notranslate"><span class="pre">drop_last</span></code></a> is set. Unfortunately, PyTorch can not detect such
cases in general.</p>
<p>See <a href="#id14"><span class="problematic" id="id15">`Dataset Types`_</span></a> for more details on these two types of datasets and how
<code class="xref py py-class docutils literal notranslate"><span class="pre">IterableDataset</span></code> interacts with
<a href="#id16"><span class="problematic" id="id17">`Multi-process data loading`_</span></a>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>See <span class="xref std std-ref">reproducibility</span>, and <span class="xref std std-ref">dataloader-workers-random-seed</span>, and
<span class="xref std std-ref">data-loading-randomness</span> notes for random seed related questions.</p>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.DataLoader.batch_size">
<span class="sig-name descname"><span class="pre">batch_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.DataLoader.batch_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.DataLoader.check_worker_number_rationality">
<span class="sig-name descname"><span class="pre">check_worker_number_rationality</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.DataLoader.check_worker_number_rationality" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.DataLoader.dataset">
<span class="sig-name descname"><span class="pre">dataset</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dataset</span><span class="p"><span class="pre">[</span></span><span class="pre">T_co</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.DataLoader.dataset" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.DataLoader.drop_last">
<span class="sig-name descname"><span class="pre">drop_last</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.DataLoader.drop_last" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.DataLoader.multiprocessing_context">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">multiprocessing_context</span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.DataLoader.multiprocessing_context" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.DataLoader.num_workers">
<span class="sig-name descname"><span class="pre">num_workers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.DataLoader.num_workers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.DataLoader.pin_memory">
<span class="sig-name descname"><span class="pre">pin_memory</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.DataLoader.pin_memory" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.DataLoader.prefetch_factor">
<span class="sig-name descname"><span class="pre">prefetch_factor</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.DataLoader.prefetch_factor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.DataLoader.sampler">
<span class="sig-name descname"><span class="pre">sampler</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.Sampler" title="torch.utils.data.sampler.Sampler"><span class="pre">Sampler</span></a></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.DataLoader.sampler" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.DataLoader.timeout">
<span class="sig-name descname"><span class="pre">timeout</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.DataLoader.timeout" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.DatasetBuilder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.data.</span></span><span class="sig-name descname"><span class="pre">DatasetBuilder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">splits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">type_</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase"><span class="pre">PreTrainedTokenizerBase</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampler_class</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.Type" title="typing.Type"><span class="pre">Type</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampler_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_jobs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.DatasetBuilder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.ABC" title="abc.ABC"><code class="xref py py-class docutils literal notranslate"><span class="pre">ABC</span></code></a></p>
<p>Abstract dataset builder class used to create a variety of different dataset types, including sequence prediction,
token prediction, next-token-prediction language modelling and masked language modelling.</p>
<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.DatasetBuilder.build">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">dataloader_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.DataLoader" title="torch.utils.data.dataloader.DataLoader"><span class="pre">DataLoader</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.DatasetBuilder.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Build a dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>batch_size: int</strong></dt><dd><p>The desired batch size.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>Dict[str, DataLoader]</dt><dd><p>Dictionary of DataLoaders for every given split.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.Dict">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.data.</span></span><span class="sig-name descname"><span class="pre">Dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">)</span> <span class="pre">-&gt;</span> <span class="pre">new</span> <span class="pre">empty</span> <span class="pre">dictionary</span> <span class="pre">dict(mapping)</span> <span class="pre">-&gt;</span> <span class="pre">new</span> <span class="pre">dictionary</span> <span class="pre">initialized</span> <span class="pre">from</span> <span class="pre">a</span> <span class="pre">mapping</span> <span class="pre">object's</span> <span class="pre">(key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value)</span> <span class="pre">pairs</span> <span class="pre">dict(iterable)</span> <span class="pre">-&gt;</span> <span class="pre">new</span> <span class="pre">dictionary</span> <span class="pre">initialized</span> <span class="pre">as</span> <span class="pre">if</span> <span class="pre">via:</span> <span class="pre">d</span> <span class="pre">=</span> <span class="pre">{}</span> <span class="pre">for</span> <span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span> <span class="pre">in</span> <span class="pre">iterable:</span> <span class="pre">d[k]</span> <span class="pre">=</span> <span class="pre">v</span> <span class="pre">dict(**kwargs)</span> <span class="pre">-&gt;</span> <span class="pre">new</span> <span class="pre">dictionary</span> <span class="pre">initialized</span> <span class="pre">with</span> <span class="pre">the</span> <span class="pre">name=value</span> <span class="pre">pairs</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">keyword</span> <span class="pre">argument</span> <span class="pre">list.</span>&#160; <span class="pre">For</span> <span class="pre">example:</span>&#160; <span class="pre">dict(one=1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">two=2</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.Dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">MutableMapping</span></code>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">KT</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">VT</span></code>]</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.LabelEncoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.data.</span></span><span class="sig-name descname"><span class="pre">LabelEncoder</span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.LabelEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">TransformerMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseEstimator</span></code></p>
<p>Encode target labels with value between 0 and n_classes-1.</p>
<p>This transformer should be used to encode target values, <em>i.e.</em> <cite>y</cite>, and
not the input <cite>X</cite>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.12.</span></p>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">OrdinalEncoder</span></code></dt><dd><p>Encode categorical features using an ordinal encoding scheme.</p>
</dd>
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">OneHotEncoder</span></code></dt><dd><p>Encode categorical features as a one-hot numeric array.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<p><cite>LabelEncoder</cite> can be used to normalize labels.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelEncoder</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="go">LabelEncoder()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">classes_</span>
<span class="go">array([1, 2, 6])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="go">array([0, 0, 1, 2]...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">array([1, 1, 2, 6])</span>
</pre></div>
</div>
<p>It can also be used to transform non-numerical labels (as long as they are
hashable and comparable) to numerical labels.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">le</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelEncoder</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="s2">&quot;paris&quot;</span><span class="p">,</span> <span class="s2">&quot;paris&quot;</span><span class="p">,</span> <span class="s2">&quot;tokyo&quot;</span><span class="p">,</span> <span class="s2">&quot;amsterdam&quot;</span><span class="p">])</span>
<span class="go">LabelEncoder()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">le</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="go">[&#39;amsterdam&#39;, &#39;paris&#39;, &#39;tokyo&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="s2">&quot;tokyo&quot;</span><span class="p">,</span> <span class="s2">&quot;tokyo&quot;</span><span class="p">,</span> <span class="s2">&quot;paris&quot;</span><span class="p">])</span>
<span class="go">array([2, 2, 1]...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">le</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="go">[&#39;tokyo&#39;, &#39;tokyo&#39;, &#39;paris&#39;]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>classes_</strong><span class="classifier">ndarray of shape (n_classes,)</span></dt><dd><p>Holds the label for each class.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.LabelEncoder.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.LabelEncoder.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit label encoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y</strong><span class="classifier">array-like of shape (n_samples,)</span></dt><dd><p>Target values.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>self</strong><span class="classifier">returns an instance of self.</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.LabelEncoder.fit_transform">
<span class="sig-name descname"><span class="pre">fit_transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.LabelEncoder.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit label encoder and return encoded labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y</strong><span class="classifier">array-like of shape (n_samples,)</span></dt><dd><p>Target values.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>y</strong><span class="classifier">array-like of shape (n_samples,)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.LabelEncoder.inverse_transform">
<span class="sig-name descname"><span class="pre">inverse_transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.LabelEncoder.inverse_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform labels back to original encoding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y</strong><span class="classifier">ndarray of shape (n_samples,)</span></dt><dd><p>Target values.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>y</strong><span class="classifier">ndarray of shape (n_samples,)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.LabelEncoder.transform">
<span class="sig-name descname"><span class="pre">transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.LabelEncoder.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform labels to normalized encoding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>y</strong><span class="classifier">array-like of shape (n_samples,)</span></dt><dd><p>Target values.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>y</strong><span class="classifier">array-like of shape (n_samples,)</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.LanguageModellingDatasetBuilder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.data.</span></span><span class="sig-name descname"><span class="pre">LanguageModellingDatasetBuilder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">splits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">type_</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase"><span class="pre">PreTrainedTokenizerBase</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampler_class</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.Type" title="typing.Type"><span class="pre">Type</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampler_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_jobs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.LanguageModellingDatasetBuilder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DatasetBuilder" title="nlp_uncertainty_zoo.utils.data.DatasetBuilder"><code class="xref py py-class docutils literal notranslate"><span class="pre">DatasetBuilder</span></code></a></p>
<p>DatasetBuilder for language modelling datasets. This includes “classic” language modelling (aka next token
prediction) as well as masked language modelling.</p>
<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.LanguageModellingDatasetBuilder.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">dataloader_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.DataLoader" title="torch.utils.data.dataloader.DataLoader"><span class="pre">DataLoader</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.LanguageModellingDatasetBuilder.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Build a language modelling dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>batch_size: int</strong></dt><dd><p>The desired batch size.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>Dict[str, DataLoader]</dt><dd><p>Dictionary of DataLoaders for every given split.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.List">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.data.</span></span><span class="sig-name descname"><span class="pre">List</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwds</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.List" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">MutableSequence</span></code>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">T</span></code>]</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.ModifiedDataCollatorForLanguageModeling">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.data.</span></span><span class="sig-name descname"><span class="pre">ModifiedDataCollatorForLanguageModeling</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase"><span class="pre">PreTrainedTokenizerBase</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlm_probability</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.15</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.ModifiedDataCollatorForLanguageModeling" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DataCollatorForLanguageModeling" title="transformers.data.data_collator.DataCollatorForLanguageModeling"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataCollatorForLanguageModeling</span></code></a></p>
<p>Modified version of the DataCollatorForLanguageModelling. The only change introduced is to the __call__ function,
where an offset between input_ids and labels for next token prediction language modelling in order to be consistent
with the rest of the code base.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.ModifiedDataCollatorForLanguageModeling.mlm">
<span class="sig-name descname"><span class="pre">mlm</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.ModifiedDataCollatorForLanguageModeling.mlm" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.ModifiedDataCollatorForLanguageModeling.mlm_probability">
<span class="sig-name descname"><span class="pre">mlm_probability</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0.15</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.ModifiedDataCollatorForLanguageModeling.mlm_probability" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.ModifiedDataCollatorForLanguageModeling.pad_to_multiple_of">
<span class="sig-name descname"><span class="pre">pad_to_multiple_of</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.ModifiedDataCollatorForLanguageModeling.pad_to_multiple_of" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.ModifiedDataCollatorForLanguageModeling.tokenizer">
<span class="sig-name descname"><span class="pre">tokenizer</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase"><span class="pre">PreTrainedTokenizerBase</span></a></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.ModifiedDataCollatorForLanguageModeling.tokenizer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.data.</span></span><span class="sig-name descname"><span class="pre">PreTrainedTokenizerBase</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">SpecialTokensMixin</span></code></p>
<p>Base class for <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code>.</p>
<p>Handles shared (mostly boiler plate) methods for those two classes.</p>
<p>Class attributes (overridden by derived classes)</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>vocab_files_names</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str]</span></code>) – A dictionary with, as keys, the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> keyword name of
each vocabulary file required by the model, and as associated values, the filename for saving the associated
file (string).</p></li>
<li><p><strong>pretrained_vocab_files_map</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Dict[str,</span> <span class="pre">str]]</span></code>) – A dictionary of dictionaries, with the
high-level keys being the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> keyword name of each vocabulary file required by the model, the
low-level being the <code class="xref py py-obj docutils literal notranslate"><span class="pre">short-cut-names</span></code> of the pretrained models with, as associated values, the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">url</span></code> to the associated pretrained vocabulary file.</p></li>
<li><p><strong>max_model_input_sizes</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Optinal[int]]</span></code>) – A dictionary with, as keys, the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">short-cut-names</span></code> of the pretrained models, and as associated values, the maximum length of the sequence
inputs of this model, or <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> if the model has no maximum input size.</p></li>
<li><p><strong>pretrained_init_configuration</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Dict[str,</span> <span class="pre">Any]]</span></code>) – A dictionary with, as keys, the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">short-cut-names</span></code> of the pretrained models, and as associated values, a dictionary of specific arguments
to pass to the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method of the tokenizer class for this pretrained model when loading the
tokenizer with the <code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code>
method.</p></li>
<li><p><strong>model_input_names</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>) – A list of inputs expected in the forward pass of the model.</p></li>
<li><p><strong>padding_side</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) – The default value for the side on which the model should have padding
applied. Should be <code class="xref py py-obj docutils literal notranslate"><span class="pre">'right'</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'left'</span></code>.</p></li>
</ul>
</div></blockquote>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>model_max_length (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>The maximum length (in number of tokens) for the inputs to the transformer model. When the tokenizer is
loaded with <code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code>, this
will be set to the value stored for the associated model in <code class="docutils literal notranslate"><span class="pre">max_model_input_sizes</span></code> (see above). If no
value is provided, will default to VERY_LARGE_INTEGER (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int(1e30)</span></code>).</p>
</dd>
<dt>padding_side: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>):</dt><dd><p>The side on which the model should have padding applied. Should be selected between [‘right’, ‘left’].
Default value is picked from the class attribute of the same name.</p>
</dd>
<dt>model_input_names (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[string]</span></code>, <cite>optional</cite>):</dt><dd><p>The list of inputs accepted by the forward pass of the model (like <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;token_type_ids&quot;</span></code> or
<code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;attention_mask&quot;</span></code>). Default value is picked from the class attribute of the same name.</p>
</dd>
<dt>bos_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>):</dt><dd><p>A special token representing the beginning of a sentence. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.bos_token</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.bos_token_id</span></code>.</p>
</dd>
<dt>eos_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>):</dt><dd><p>A special token representing the end of a sentence. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.eos_token</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.eos_token_id</span></code>.</p>
</dd>
<dt>unk_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>):</dt><dd><p>A special token representing an out-of-vocabulary token. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.unk_token</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.unk_token_id</span></code>.</p>
</dd>
<dt>sep_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>):</dt><dd><p>A special token separating two different sentences in the same input (used by BERT for instance). Will be
associated to <code class="docutils literal notranslate"><span class="pre">self.sep_token</span></code> and <code class="docutils literal notranslate"><span class="pre">self.sep_token_id</span></code>.</p>
</dd>
<dt>pad_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>):</dt><dd><p>A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by
attention mechanisms or loss computation. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.pad_token</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.pad_token_id</span></code>.</p>
</dd>
<dt>cls_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>):</dt><dd><p>A special token representing the class of the input (used by BERT for instance). Will be associated to
<code class="docutils literal notranslate"><span class="pre">self.cls_token</span></code> and <code class="docutils literal notranslate"><span class="pre">self.cls_token_id</span></code>.</p>
</dd>
<dt>mask_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>):</dt><dd><p>A special token representing a masked token (used by masked-language modeling pretraining objectives, like
BERT). Will be associated to <code class="docutils literal notranslate"><span class="pre">self.mask_token</span></code> and <code class="docutils literal notranslate"><span class="pre">self.mask_token_id</span></code>.</p>
</dd>
<dt>additional_special_tokens (tuple or list of <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>):</dt><dd><p>A tuple or a list of additional special tokens. Add them here to ensure they won’t be split by the
tokenization process. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.additional_special_tokens</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.additional_special_tokens_ids</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.as_target_tokenizer">
<span class="sig-name descname"><span class="pre">as_target_tokenizer</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.as_target_tokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to
sequence-to-sequence models that need a slightly different processing for the labels.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.batch_decode">
<span class="sig-name descname"><span class="pre">batch_decode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sequences</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">np.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_special_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clean_up_tokenization_spaces</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.batch_decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert a list of lists of token ids into a list of strings by calling decode.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>sequences (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[List[int],</span> <span class="pre">List[List[int]],</span> <span class="pre">np.ndarray,</span> <span class="pre">torch.Tensor,</span> <span class="pre">tf.Tensor]</span></code>):</dt><dd><p>List of tokenized input ids. Can be obtained using the <code class="docutils literal notranslate"><span class="pre">__call__</span></code> method.</p>
</dd>
<dt>skip_special_tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to remove special tokens in the decoding.</p>
</dd>
<dt>clean_up_tokenization_spaces (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to clean up the tokenization spaces.</p>
</dd>
<dt>kwargs (additional keyword arguments, <cite>optional</cite>):</dt><dd><p>Will be passed to the underlying model specific decode method.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>: The list of decoded sentences.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.batch_encode_plus">
<span class="sig-name descname"><span class="pre">batch_encode_plus</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_text_or_text_pairs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.Tuple" title="typing.Tuple"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.Tuple" title="typing.Tuple"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.Tuple" title="typing.Tuple"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_special_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">PaddingStrategy</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truncation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">TruncationStrategy</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_split_into_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_to_multiple_of</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">None</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">TensorType</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_token_type_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_attention_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_overflowing_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_special_tokens_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_offsets_mapping</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding" title="transformers.tokenization_utils_base.BatchEncoding"><span class="pre">BatchEncoding</span></a></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.batch_encode_plus" title="Permalink to this definition">¶</a></dt>
<dd><p>Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This method is deprecated, <code class="docutils literal notranslate"><span class="pre">__call__</span></code> should be used instead.</p>
</div>
<dl>
<dt>Args:</dt><dd><dl>
<dt>batch_text_or_text_pairs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Tuple[str,</span> <span class="pre">str]]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[List[str]]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Tuple[List[str],</span> <span class="pre">List[str]]]</span></code>, and for not-fast tokenizers, also <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[List[int]]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Tuple[List[int],</span> <span class="pre">List[int]]]</span></code>):</dt><dd><p>Batch of sequences or pair of sequences to be encoded. This can be a list of
string/string-sequences/int-sequences or a list of pair of string/string-sequences/int-sequence (see
details in <code class="docutils literal notranslate"><span class="pre">encode_plus</span></code>).</p>
</dd>
<dt>add_special_tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to encode the sequences with the special tokens relative to their model.</p>
</dd>
<dt>padding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Activates and controls padding. Accepts the following values:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest'</span></code>: Pad to the longest sequence in the batch (or no padding if only a
single sequence if provided).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'max_length'</span></code>: Pad to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_pad'</span></code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</p></li>
</ul>
</dd>
<dt>truncation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TruncationStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Activates and controls truncation. Accepts the following values:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest_first'</span></code>: Truncate to a maximum length specified with the argument
<code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the maximum acceptable input length for the model if that argument is not
provided. This will truncate token by token, removing a token from the longest sequence in the pair
if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to
the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_second'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
to the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_truncate'</span></code> (default): No truncation (i.e., can output batch with
sequence lengths greater than the model maximum admissible input size).</p></li>
</ul>
</dd>
<dt>max_length (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this will use the predefined model maximum length if a maximum
length is required by one of the truncation/padding parameters. If the model has no specific maximum
input length (like XLNet) truncation/padding to a maximum length will be deactivated.</p>
</dd>
<dt>stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0):</dt><dd><p>If set to a number along with <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code>, the overflowing tokens returned when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code> will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.</p>
</dd>
<dt>is_split_into_words (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer
will skip the pre-tokenization step. This is useful for NER or token classification.</p>
</dd>
<dt>pad_to_multiple_of (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
the use of Tensor Cores on NVIDIA hardware with compute capability &gt;= 7.5 (Volta).</p>
</dd>
<dt>return_tensors (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code>, <cite>optional</cite>):</dt><dd><p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'tf'</span></code>: Return TensorFlow <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.constant</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'pt'</span></code>: Return PyTorch <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'np'</span></code>: Return Numpy <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> objects.</p></li>
</ul>
</dd>
<dt>return_token_type_ids (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>):</dt><dd><p>Whether to return token type IDs. If left to the default, will return the token type IDs according to
the specific tokenizer’s default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</dd>
<dt>return_attention_mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>):</dt><dd><p>Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific tokenizer’s default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</dd>
<dt>return_overflowing_tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return overflowing token sequences.</p>
</dd>
<dt>return_special_tokens_mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return special tokens mask information.</p>
</dd>
<dt>return_offsets_mapping (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return <code class="xref py py-obj docutils literal notranslate"><span class="pre">(char_start,</span> <span class="pre">char_end)</span></code> for each token.</p>
<p>This is only available on fast tokenizers inheriting from
<code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code>, if using Python’s tokenizer, this method will raise
<code class="xref py py-obj docutils literal notranslate"><span class="pre">NotImplementedError</span></code>.</p>
</dd>
<dt>return_length  (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return the lengths of the encoded inputs.</p>
</dd>
<dt>verbose (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to print more information and warnings.</p>
</dd>
</dl>
<p><a href="#id2"><span class="problematic" id="id3">**</span></a>kwargs: passed to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.tokenize()</span></code> method</p>
</dd>
<dt>Return:</dt><dd><p><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code>: A <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code> with the following fields:</p>
<ul>
<li><p><strong>input_ids</strong> – List of token ids to be fed to a model.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><p><strong>token_type_ids</strong> – List of token type ids to be fed to a model (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_token_type_ids=True</span></code>
or if <cite>“token_type_ids”</cite> is in <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.model_input_names</span></code>).</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</li>
<li><p><strong>attention_mask</strong> – List of indices specifying which tokens should be attended to by the model (when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_attention_mask=True</span></code> or if <cite>“attention_mask”</cite> is in <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.model_input_names</span></code>).</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</li>
<li><p><strong>overflowing_tokens</strong> – List of overflowing tokens sequences (when a <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> is specified and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code>).</p></li>
<li><p><strong>num_truncated_tokens</strong> – Number of tokens truncated (when a <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> is specified and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code>).</p></li>
<li><p><strong>special_tokens_mask</strong> – List of 0s and 1s, with 1 specifying added special tokens and 0 specifying
regular sequence tokens (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">add_special_tokens=True</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_special_tokens_mask=True</span></code>).</p></li>
<li><p><strong>length</strong> – The length of the inputs (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_length=True</span></code>)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.build_inputs_with_special_tokens">
<span class="sig-name descname"><span class="pre">build_inputs_with_special_tokens</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">token_ids_0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_ids_1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.build_inputs_with_special_tokens" title="Permalink to this definition">¶</a></dt>
<dd><p>Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens.</p>
<p>This implementation does not add special tokens and this method should be overridden in a subclass.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>token_ids_0 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>): The first tokenized sequence.
token_ids_1 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>): The second tokenized sequence.</p>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>: The model input with special tokens.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.clean_up_tokenization">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">clean_up_tokenization</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">out_string</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.clean_up_tokenization" title="Permalink to this definition">¶</a></dt>
<dd><p>Clean up a list of simple English tokenization artifacts like spaces before punctuations and abbreviated forms.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>out_string (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): The text to clean up.</p>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>: The cleaned-up string.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.convert_tokens_to_string">
<span class="sig-name descname"><span class="pre">convert_tokens_to_string</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.convert_tokens_to_string" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a sequence of tokens in a single string. The most simple way to do it is <code class="docutils literal notranslate"><span class="pre">&quot;</span> <span class="pre">&quot;.join(tokens)</span></code> but we
often want to remove sub-word tokenization artifacts at the same time.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): The token to join in a string.</p>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>: The joined tokens.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.create_token_type_ids_from_sequences">
<span class="sig-name descname"><span class="pre">create_token_type_ids_from_sequences</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">token_ids_0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_ids_1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.create_token_type_ids_from_sequences" title="Permalink to this definition">¶</a></dt>
<dd><p>Create the token type IDs corresponding to the sequences passed. <a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
<p>Should be overridden in a subclass if the model has a special way of building those.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>token_ids_0 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>): The first tokenized sequence.
token_ids_1 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>): The second tokenized sequence.</p>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>: The token type ids.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.decode">
<span class="sig-name descname"><span class="pre">decode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">token_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">np.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_special_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clean_up_tokenization_spaces</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special
tokens and clean up tokenization spaces.</p>
<p>Similar to doing <code class="docutils literal notranslate"><span class="pre">self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))</span></code>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>token_ids (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[int,</span> <span class="pre">List[int],</span> <span class="pre">np.ndarray,</span> <span class="pre">torch.Tensor,</span> <span class="pre">tf.Tensor]</span></code>):</dt><dd><p>List of tokenized input ids. Can be obtained using the <code class="docutils literal notranslate"><span class="pre">__call__</span></code> method.</p>
</dd>
<dt>skip_special_tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to remove special tokens in the decoding.</p>
</dd>
<dt>clean_up_tokenization_spaces (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to clean up the tokenization spaces.</p>
</dd>
<dt>kwargs (additional keyword arguments, <cite>optional</cite>):</dt><dd><p>Will be passed to the underlying model specific decode method.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>: The decoded sentence.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.encode">
<span class="sig-name descname"><span class="pre">encode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">text_pair</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_special_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">PaddingStrategy</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truncation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">TruncationStrategy</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">None</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">TensorType</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.encode" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.</p>
<p>Same as doing <code class="docutils literal notranslate"><span class="pre">self.convert_tokens_to_ids(self.tokenize(text))</span></code>.</p>
<dl>
<dt>Args:</dt><dd><dl>
<dt>text (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>):</dt><dd><p>The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the
<code class="docutils literal notranslate"><span class="pre">tokenize</span></code> method) or a list of integers (tokenized string ids using the <code class="docutils literal notranslate"><span class="pre">convert_tokens_to_ids</span></code>
method).</p>
</dd>
<dt>text_pair (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>):</dt><dd><p>Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using
the <code class="docutils literal notranslate"><span class="pre">tokenize</span></code> method) or a list of integers (tokenized string ids using the
<code class="docutils literal notranslate"><span class="pre">convert_tokens_to_ids</span></code> method).</p>
</dd>
<dt>add_special_tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to encode the sequences with the special tokens relative to their model.</p>
</dd>
<dt>padding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Activates and controls padding. Accepts the following values:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest'</span></code>: Pad to the longest sequence in the batch (or no padding if only a
single sequence if provided).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'max_length'</span></code>: Pad to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_pad'</span></code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</p></li>
</ul>
</dd>
<dt>truncation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TruncationStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Activates and controls truncation. Accepts the following values:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest_first'</span></code>: Truncate to a maximum length specified with the argument
<code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the maximum acceptable input length for the model if that argument is not
provided. This will truncate token by token, removing a token from the longest sequence in the pair
if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to
the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_second'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
to the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_truncate'</span></code> (default): No truncation (i.e., can output batch with
sequence lengths greater than the model maximum admissible input size).</p></li>
</ul>
</dd>
<dt>max_length (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this will use the predefined model maximum length if a maximum
length is required by one of the truncation/padding parameters. If the model has no specific maximum
input length (like XLNet) truncation/padding to a maximum length will be deactivated.</p>
</dd>
<dt>stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0):</dt><dd><p>If set to a number along with <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code>, the overflowing tokens returned when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code> will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.</p>
</dd>
<dt>is_split_into_words (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer
will skip the pre-tokenization step. This is useful for NER or token classification.</p>
</dd>
<dt>pad_to_multiple_of (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
the use of Tensor Cores on NVIDIA hardware with compute capability &gt;= 7.5 (Volta).</p>
</dd>
<dt>return_tensors (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code>, <cite>optional</cite>):</dt><dd><p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'tf'</span></code>: Return TensorFlow <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.constant</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'pt'</span></code>: Return PyTorch <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'np'</span></code>: Return Numpy <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> objects.</p></li>
</ul>
</dd>
</dl>
<p><a href="#id4"><span class="problematic" id="id5">**</span></a>kwargs: Passed along to the <cite>.tokenize()</cite> method.</p>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code>: The tokenized ids of the
text.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.encode_plus">
<span class="sig-name descname"><span class="pre">encode_plus</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">text_pair</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_special_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">PaddingStrategy</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truncation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">TruncationStrategy</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_split_into_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_to_multiple_of</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">None</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">TensorType</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_token_type_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_attention_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_overflowing_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_special_tokens_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_offsets_mapping</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding" title="transformers.tokenization_utils_base.BatchEncoding"><span class="pre">BatchEncoding</span></a></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.encode_plus" title="Permalink to this definition">¶</a></dt>
<dd><p>Tokenize and prepare for the model a sequence or a pair of sequences.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This method is deprecated, <code class="docutils literal notranslate"><span class="pre">__call__</span></code> should be used instead.</p>
</div>
<dl>
<dt>Args:</dt><dd><dl>
<dt>text (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code> (the latter only for not-fast tokenizers)):</dt><dd><p>The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the
<code class="docutils literal notranslate"><span class="pre">tokenize</span></code> method) or a list of integers (tokenized string ids using the <code class="docutils literal notranslate"><span class="pre">convert_tokens_to_ids</span></code>
method).</p>
</dd>
<dt>text_pair (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>):</dt><dd><p>Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using
the <code class="docutils literal notranslate"><span class="pre">tokenize</span></code> method) or a list of integers (tokenized string ids using the
<code class="docutils literal notranslate"><span class="pre">convert_tokens_to_ids</span></code> method).</p>
</dd>
<dt>add_special_tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to encode the sequences with the special tokens relative to their model.</p>
</dd>
<dt>padding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Activates and controls padding. Accepts the following values:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest'</span></code>: Pad to the longest sequence in the batch (or no padding if only a
single sequence if provided).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'max_length'</span></code>: Pad to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_pad'</span></code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</p></li>
</ul>
</dd>
<dt>truncation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TruncationStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Activates and controls truncation. Accepts the following values:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest_first'</span></code>: Truncate to a maximum length specified with the argument
<code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the maximum acceptable input length for the model if that argument is not
provided. This will truncate token by token, removing a token from the longest sequence in the pair
if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to
the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_second'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
to the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_truncate'</span></code> (default): No truncation (i.e., can output batch with
sequence lengths greater than the model maximum admissible input size).</p></li>
</ul>
</dd>
<dt>max_length (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this will use the predefined model maximum length if a maximum
length is required by one of the truncation/padding parameters. If the model has no specific maximum
input length (like XLNet) truncation/padding to a maximum length will be deactivated.</p>
</dd>
<dt>stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0):</dt><dd><p>If set to a number along with <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code>, the overflowing tokens returned when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code> will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.</p>
</dd>
<dt>is_split_into_words (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer
will skip the pre-tokenization step. This is useful for NER or token classification.</p>
</dd>
<dt>pad_to_multiple_of (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
the use of Tensor Cores on NVIDIA hardware with compute capability &gt;= 7.5 (Volta).</p>
</dd>
<dt>return_tensors (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code>, <cite>optional</cite>):</dt><dd><p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'tf'</span></code>: Return TensorFlow <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.constant</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'pt'</span></code>: Return PyTorch <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'np'</span></code>: Return Numpy <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> objects.</p></li>
</ul>
</dd>
<dt>return_token_type_ids (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>):</dt><dd><p>Whether to return token type IDs. If left to the default, will return the token type IDs according to
the specific tokenizer’s default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</dd>
<dt>return_attention_mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>):</dt><dd><p>Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific tokenizer’s default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</dd>
<dt>return_overflowing_tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return overflowing token sequences.</p>
</dd>
<dt>return_special_tokens_mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return special tokens mask information.</p>
</dd>
<dt>return_offsets_mapping (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return <code class="xref py py-obj docutils literal notranslate"><span class="pre">(char_start,</span> <span class="pre">char_end)</span></code> for each token.</p>
<p>This is only available on fast tokenizers inheriting from
<code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code>, if using Python’s tokenizer, this method will raise
<code class="xref py py-obj docutils literal notranslate"><span class="pre">NotImplementedError</span></code>.</p>
</dd>
<dt>return_length  (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return the lengths of the encoded inputs.</p>
</dd>
<dt>verbose (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to print more information and warnings.</p>
</dd>
</dl>
<p><a href="#id6"><span class="problematic" id="id7">**</span></a>kwargs: passed to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.tokenize()</span></code> method</p>
</dd>
<dt>Return:</dt><dd><p><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code>: A <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code> with the following fields:</p>
<ul>
<li><p><strong>input_ids</strong> – List of token ids to be fed to a model.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><p><strong>token_type_ids</strong> – List of token type ids to be fed to a model (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_token_type_ids=True</span></code>
or if <cite>“token_type_ids”</cite> is in <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.model_input_names</span></code>).</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</li>
<li><p><strong>attention_mask</strong> – List of indices specifying which tokens should be attended to by the model (when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_attention_mask=True</span></code> or if <cite>“attention_mask”</cite> is in <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.model_input_names</span></code>).</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</li>
<li><p><strong>overflowing_tokens</strong> – List of overflowing tokens sequences (when a <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> is specified and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code>).</p></li>
<li><p><strong>num_truncated_tokens</strong> – Number of tokens truncated (when a <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> is specified and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code>).</p></li>
<li><p><strong>special_tokens_mask</strong> – List of 0s and 1s, with 1 specifying added special tokens and 0 specifying
regular sequence tokens (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">add_special_tokens=True</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_special_tokens_mask=True</span></code>).</p></li>
<li><p><strong>length</strong> – The length of the inputs (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_length=True</span></code>)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.from_pretrained">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pretrained_model_name_or_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">PathLike</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">init_inputs</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.from_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiate a <a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.PreTrainedTokenizerBase" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase</span></code></a> (or a derived class) from
a predefined tokenizer.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>pretrained_model_name_or_path (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>):</dt><dd><p>Can be either:</p>
<ul class="simple">
<li><p>A string, the <cite>model id</cite> of a predefined tokenizer hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under a
user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing vocabulary files required by the tokenizer, for instance saved
using the <code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code>
method, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>(<strong>Deprecated</strong>, not applicable to all derived classes) A path or url to a single saved vocabulary
file (if and only if the tokenizer only requires a single vocabulary file like Bert or XLNet), e.g.,
<code class="docutils literal notranslate"><span class="pre">./my_model_directory/vocab.txt</span></code>.</p></li>
</ul>
</dd>
<dt>cache_dir (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>, <cite>optional</cite>):</dt><dd><p>Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the
standard cache should not be used.</p>
</dd>
<dt>force_download (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to force the (re-)download the vocabulary files and override the cached versions if they
exist.</p>
</dd>
<dt>resume_download (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to delete incompletely received files. Attempt to resume the download if such a file
exists.</p>
</dd>
<dt>proxies (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>):</dt><dd><p>A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p>
</dd>
<dt>use_auth_token (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <cite>bool</cite>, <cite>optional</cite>):</dt><dd><p>The token to use as HTTP bearer authorization for remote files. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, will use the token
generated when running <code class="xref py py-obj docutils literal notranslate"><span class="pre">transformers-cli</span> <span class="pre">login</span></code> (stored in <code class="xref py py-obj docutils literal notranslate"><span class="pre">huggingface</span></code>).</p>
</dd>
<dt>revision(<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;main&quot;</span></code>):</dt><dd><p>The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p>
</dd>
<dt>subfolder (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>):</dt><dd><p>In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.</p>
</dd>
<dt>inputs (additional positional arguments, <cite>optional</cite>):</dt><dd><p>Will be passed along to the Tokenizer <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p>
</dd>
<dt>kwargs (additional keyword arguments, <cite>optional</cite>):</dt><dd><p>Will be passed to the Tokenizer <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method. Can be used to set special tokens like
<code class="docutils literal notranslate"><span class="pre">bos_token</span></code>, <code class="docutils literal notranslate"><span class="pre">eos_token</span></code>, <code class="docutils literal notranslate"><span class="pre">unk_token</span></code>, <code class="docutils literal notranslate"><span class="pre">sep_token</span></code>, <code class="docutils literal notranslate"><span class="pre">pad_token</span></code>, <code class="docutils literal notranslate"><span class="pre">cls_token</span></code>,
<code class="docutils literal notranslate"><span class="pre">mask_token</span></code>, <code class="docutils literal notranslate"><span class="pre">additional_special_tokens</span></code>. See parameters in the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> for more details.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">use_auth_token=True</span></code> is required when you want to use a private model.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># We can&#39;t instantiate directly the base class `PreTrainedTokenizerBase` so let&#39;s show our examples on a derived class: BertTokenizer</span>
<span class="c1"># Download vocabulary from huggingface.co and cache.</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="c1"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;dbmdz/bert-base-german-cased&#39;</span><span class="p">)</span>

<span class="c1"># If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained(&#39;./test/saved_model/&#39;)`)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./test/saved_model/&#39;</span><span class="p">)</span>

<span class="c1"># If the tokenizer uses a single vocabulary file, you can point directly to this file</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./test/saved_model/my_vocab.txt&#39;</span><span class="p">)</span>

<span class="c1"># You can link tokens to special vocabulary when instantiating</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">,</span> <span class="n">unk_token</span><span class="o">=</span><span class="s1">&#39;&lt;unk&gt;&#39;</span><span class="p">)</span>
<span class="c1"># You should be sure &#39;&lt;unk&gt;&#39; is in the vocabulary when doing that.</span>
<span class="c1"># Otherwise use tokenizer.add_special_tokens({&#39;unk_token&#39;: &#39;&lt;unk&gt;&#39;}) instead)</span>
<span class="k">assert</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">unk_token</span> <span class="o">==</span> <span class="s1">&#39;&lt;unk&gt;&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.get_special_tokens_mask">
<span class="sig-name descname"><span class="pre">get_special_tokens_mask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">token_ids_0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_ids_1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">already_has_special_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.get_special_tokens_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code class="docutils literal notranslate"><span class="pre">prepare_for_model</span></code> or <code class="docutils literal notranslate"><span class="pre">encode_plus</span></code> methods.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>token_ids_0 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>):</dt><dd><p>List of ids of the first sequence.</p>
</dd>
<dt>token_ids_1 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>):</dt><dd><p>List of ids of the second sequence.</p>
</dd>
<dt>already_has_special_tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not the token list is already formatted with special tokens for the model.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.get_vocab">
<span class="sig-name descname"><span class="pre">get_vocab</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.get_vocab" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the vocabulary as a dictionary of token to index.</p>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizer.get_vocab()[token]</span></code> is equivalent to <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizer.convert_tokens_to_ids(token)</span></code> when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">token</span></code> is in the vocab.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">int]</span></code>: The vocabulary.</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.max_len_sentences_pair">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">max_len_sentences_pair</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.max_len_sentences_pair" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>: The maximum combined length of a pair of sentences that can be fed to the model.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.max_len_single_sentence">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">max_len_single_sentence</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.max_len_single_sentence" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>: The maximum length of a sentence that can be fed to the model.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.max_model_input_sizes">
<span class="sig-name descname"><span class="pre">max_model_input_sizes</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{}</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.max_model_input_sizes" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.model_input_names">
<span class="sig-name descname"><span class="pre">model_input_names</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['input_ids',</span> <span class="pre">'token_type_ids',</span> <span class="pre">'attention_mask']</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.model_input_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.num_special_tokens_to_add">
<span class="sig-name descname"><span class="pre">num_special_tokens_to_add</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pair</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.num_special_tokens_to_add" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.pad">
<span class="sig-name descname"><span class="pre">pad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">encoded_inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding" title="transformers.tokenization_utils_base.BatchEncoding"><span class="pre">BatchEncoding</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding" title="transformers.tokenization_utils_base.BatchEncoding"><span class="pre">BatchEncoding</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">PaddingStrategy</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_to_multiple_of</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_attention_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">None</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">TensorType</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding" title="transformers.tokenization_utils_base.BatchEncoding"><span class="pre">BatchEncoding</span></a></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.pad" title="Permalink to this definition">¶</a></dt>
<dd><p>Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length
in the batch.</p>
<p>Padding side (left/right) padding token ids are defined at the tokenizer level (with <code class="docutils literal notranslate"><span class="pre">self.padding_side</span></code>,
<code class="docutils literal notranslate"><span class="pre">self.pad_token_id</span></code> and <code class="docutils literal notranslate"><span class="pre">self.pad_token_type_id</span></code>)</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">encoded_inputs</span></code> passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the
result will use the same type unless you provide a different tensor type with <code class="docutils literal notranslate"><span class="pre">return_tensors</span></code>. In the
case of PyTorch tensors, you will lose the specific device of your tensors however.</p>
</div>
<dl>
<dt>Args:</dt><dd><dl>
<dt>encoded_inputs (<code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code>, list of <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">List[int]]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">List[List[int]]</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">List[int]]]</span></code>):</dt><dd><p>Tokenized inputs. Can represent one input (<code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span>
<span class="pre">List[int]]</span></code>) or a batch of tokenized inputs (list of <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code>, <cite>Dict[str,
List[List[int]]]</cite> or <cite>List[Dict[str, List[int]]]</cite>) so you can use this method during preprocessing as
well as in a PyTorch Dataloader collate function.</p>
<p>Instead of <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code> you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),
see the note above for the return type.</p>
</dd>
<dt>padding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><blockquote>
<div><p>Select a strategy to pad the returned sequences (according to the model’s padding side and padding
index) among:</p>
</div></blockquote>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest'</span></code>: Pad to the longest sequence in the batch (or no padding if only a
single sequence if provided).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'max_length'</span></code>: Pad to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_pad'</span></code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</p></li>
</ul>
</dd>
<dt>max_length (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>Maximum length of the returned list and optionally padding length (see above).</p>
</dd>
<dt>pad_to_multiple_of (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>If set will pad the sequence to a multiple of the provided value.</p>
<p>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability
&gt;= 7.5 (Volta).</p>
</dd>
<dt>return_attention_mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>):</dt><dd><p>Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific tokenizer’s default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</dd>
<dt>return_tensors (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code>, <cite>optional</cite>):</dt><dd><p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'tf'</span></code>: Return TensorFlow <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.constant</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'pt'</span></code>: Return PyTorch <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'np'</span></code>: Return Numpy <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> objects.</p></li>
</ul>
</dd>
<dt>verbose (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to print more information and warnings.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.padding_side">
<span class="sig-name descname"><span class="pre">padding_side</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'right'</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.padding_side" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.prepare_for_model">
<span class="sig-name descname"><span class="pre">prepare_for_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pair_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_special_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">PaddingStrategy</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truncation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">TruncationStrategy</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_to_multiple_of</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">None</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">TensorType</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_token_type_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_attention_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_overflowing_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_special_tokens_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_offsets_mapping</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend_batch_axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding" title="transformers.tokenization_utils_base.BatchEncoding"><span class="pre">BatchEncoding</span></a></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.prepare_for_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It
adds special tokens, truncates sequences if overflowing while taking into account the special tokens and
manages a moving window (with user defined stride) for overflowing tokens</p>
<dl>
<dt>Args:</dt><dd><dl>
<dt>ids (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>):</dt><dd><p>Tokenized input ids of the first sequence. Can be obtained from a string by chaining the <code class="docutils literal notranslate"><span class="pre">tokenize</span></code>
and <code class="docutils literal notranslate"><span class="pre">convert_tokens_to_ids</span></code> methods.</p>
</dd>
<dt>pair_ids (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>):</dt><dd><p>Tokenized input ids of the second sequence. Can be obtained from a string by chaining the <code class="docutils literal notranslate"><span class="pre">tokenize</span></code>
and <code class="docutils literal notranslate"><span class="pre">convert_tokens_to_ids</span></code> methods.</p>
</dd>
<dt>add_special_tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to encode the sequences with the special tokens relative to their model.</p>
</dd>
<dt>padding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Activates and controls padding. Accepts the following values:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest'</span></code>: Pad to the longest sequence in the batch (or no padding if only a
single sequence if provided).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'max_length'</span></code>: Pad to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_pad'</span></code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</p></li>
</ul>
</dd>
<dt>truncation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TruncationStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Activates and controls truncation. Accepts the following values:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest_first'</span></code>: Truncate to a maximum length specified with the argument
<code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the maximum acceptable input length for the model if that argument is not
provided. This will truncate token by token, removing a token from the longest sequence in the pair
if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to
the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_second'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
to the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_truncate'</span></code> (default): No truncation (i.e., can output batch with
sequence lengths greater than the model maximum admissible input size).</p></li>
</ul>
</dd>
<dt>max_length (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this will use the predefined model maximum length if a maximum
length is required by one of the truncation/padding parameters. If the model has no specific maximum
input length (like XLNet) truncation/padding to a maximum length will be deactivated.</p>
</dd>
<dt>stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0):</dt><dd><p>If set to a number along with <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code>, the overflowing tokens returned when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code> will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.</p>
</dd>
<dt>is_split_into_words (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer
will skip the pre-tokenization step. This is useful for NER or token classification.</p>
</dd>
<dt>pad_to_multiple_of (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
the use of Tensor Cores on NVIDIA hardware with compute capability &gt;= 7.5 (Volta).</p>
</dd>
<dt>return_tensors (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code>, <cite>optional</cite>):</dt><dd><p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'tf'</span></code>: Return TensorFlow <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.constant</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'pt'</span></code>: Return PyTorch <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'np'</span></code>: Return Numpy <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> objects.</p></li>
</ul>
</dd>
<dt>return_token_type_ids (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>):</dt><dd><p>Whether to return token type IDs. If left to the default, will return the token type IDs according to
the specific tokenizer’s default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</dd>
<dt>return_attention_mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>):</dt><dd><p>Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific tokenizer’s default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</dd>
<dt>return_overflowing_tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return overflowing token sequences.</p>
</dd>
<dt>return_special_tokens_mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return special tokens mask information.</p>
</dd>
<dt>return_offsets_mapping (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return <code class="xref py py-obj docutils literal notranslate"><span class="pre">(char_start,</span> <span class="pre">char_end)</span></code> for each token.</p>
<p>This is only available on fast tokenizers inheriting from
<code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code>, if using Python’s tokenizer, this method will raise
<code class="xref py py-obj docutils literal notranslate"><span class="pre">NotImplementedError</span></code>.</p>
</dd>
<dt>return_length  (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return the lengths of the encoded inputs.</p>
</dd>
<dt>verbose (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to print more information and warnings.</p>
</dd>
</dl>
<p><a href="#id8"><span class="problematic" id="id9">**</span></a>kwargs: passed to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.tokenize()</span></code> method</p>
</dd>
<dt>Return:</dt><dd><p><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code>: A <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code> with the following fields:</p>
<ul>
<li><p><strong>input_ids</strong> – List of token ids to be fed to a model.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><p><strong>token_type_ids</strong> – List of token type ids to be fed to a model (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_token_type_ids=True</span></code>
or if <cite>“token_type_ids”</cite> is in <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.model_input_names</span></code>).</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</li>
<li><p><strong>attention_mask</strong> – List of indices specifying which tokens should be attended to by the model (when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_attention_mask=True</span></code> or if <cite>“attention_mask”</cite> is in <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.model_input_names</span></code>).</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</li>
<li><p><strong>overflowing_tokens</strong> – List of overflowing tokens sequences (when a <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> is specified and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code>).</p></li>
<li><p><strong>num_truncated_tokens</strong> – Number of tokens truncated (when a <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> is specified and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code>).</p></li>
<li><p><strong>special_tokens_mask</strong> – List of 0s and 1s, with 1 specifying added special tokens and 0 specifying
regular sequence tokens (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">add_special_tokens=True</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_special_tokens_mask=True</span></code>).</p></li>
<li><p><strong>length</strong> – The length of the inputs (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_length=True</span></code>)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.prepare_seq2seq_batch">
<span class="sig-name descname"><span class="pre">prepare_seq2seq_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src_texts</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_texts</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_target_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'longest'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truncation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.BatchEncoding" title="transformers.tokenization_utils_base.BatchEncoding"><span class="pre">BatchEncoding</span></a></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.prepare_seq2seq_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepare model inputs for translation. For best performance, translate one sentence at a time.</p>
<dl>
<dt>Arguments:</dt><dd><dl class="simple">
<dt>src_texts (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>):</dt><dd><p>List of documents to summarize or source language texts.</p>
</dd>
<dt>tgt_texts (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>, <cite>optional</cite>):</dt><dd><p>List of summaries or target language texts.</p>
</dd>
<dt>max_length (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>Controls the maximum length for encoder inputs (documents to summarize or source language texts) If
left unset or set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.</p>
</dd>
<dt>max_target_length (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>):</dt><dd><p>Controls the maximum length of decoder inputs (target language texts or summaries) If left unset or set
to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this will use the max_length value.</p>
</dd>
<dt>padding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Activates and controls padding. Accepts the following values:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest'</span></code>: Pad to the longest sequence in the batch (or no padding if only a
single sequence if provided).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'max_length'</span></code>: Pad to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_pad'</span></code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</p></li>
</ul>
</dd>
<dt>return_tensors (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code>, <cite>optional</cite>):</dt><dd><p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'tf'</span></code>: Return TensorFlow <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.constant</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'pt'</span></code>: Return PyTorch <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'np'</span></code>: Return Numpy <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> objects.</p></li>
</ul>
</dd>
<dt>truncation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TruncationStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Activates and controls truncation. Accepts the following values:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest_first'</span></code>: Truncate to a maximum length specified with the argument
<code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the maximum acceptable input length for the model if that argument is not
provided. This will truncate token by token, removing a token from the longest sequence in the pair
if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to
the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_second'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
to the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_truncate'</span></code> (default): No truncation (i.e., can output batch with
sequence lengths greater than the model maximum admissible input size).</p></li>
</ul>
</dd>
<dt><a href="#id10"><span class="problematic" id="id11">**</span></a>kwargs:</dt><dd><p>Additional keyword arguments passed along to <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.__call__</span></code>.</p>
</dd>
</dl>
</dd>
<dt>Return:</dt><dd><p><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code>: A <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code> with the following fields:</p>
<ul class="simple">
<li><p><strong>input_ids</strong> – List of token ids to be fed to the encoder.</p></li>
<li><p><strong>attention_mask</strong> – List of indices specifying which tokens should be attended to by the model.</p></li>
<li><p><strong>labels</strong> – List of token ids for tgt_texts.</p></li>
</ul>
<p>The full set of keys <code class="docutils literal notranslate"><span class="pre">[input_ids,</span> <span class="pre">attention_mask,</span> <span class="pre">labels]</span></code>, will only be returned if tgt_texts is passed.
Otherwise, input_ids, attention_mask will be the only keys.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.pretrained_init_configuration">
<span class="sig-name descname"><span class="pre">pretrained_init_configuration</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{}</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.pretrained_init_configuration" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.pretrained_vocab_files_map">
<span class="sig-name descname"><span class="pre">pretrained_vocab_files_map</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{}</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.pretrained_vocab_files_map" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.save_pretrained">
<span class="sig-name descname"><span class="pre">save_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save_directory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">PathLike</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">legacy_format</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filename_prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.Tuple" title="typing.Tuple"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.save_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the full tokenizer state.</p>
<p>This method make sure the full tokenizer can then be re-loaded using the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code> class method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A “fast” tokenizer (instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizerFast</span></code>) saved with this method will
not be possible to load back in a “slow” tokenizer, i.e. in a <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer</span></code>
instance. It can only be loaded in a “fast” tokenizer, i.e. in a
<code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizerFast</span></code> instance.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This won’t save modifications you may have applied to the tokenizer after the instantiation (for instance,
modifying <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizer.do_lower_case</span></code> after creation).</p>
</div>
<dl>
<dt>Args:</dt><dd><p>save_directory (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>): The path to a directory where the tokenizer will be saved.
legacy_format (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</p>
<blockquote>
<div><p>Whether to save the tokenizer in legacy format (default), i.e. with tokenizer specific vocabulary and a
separate added_tokens files or in the unified JSON file format for the <cite>tokenizers</cite> library. It’s only
possible to save a Fast tokenizer in the unified JSON format and this format is incompatible with
“slow” tokenizers (not powered by the <cite>tokenizers</cite> library).</p>
</div></blockquote>
<dl class="simple">
<dt>filename_prefix: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>):</dt><dd><p>A prefix to add to the names of the files saved by the tokenizer.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>A tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>: The files saved.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.save_vocabulary">
<span class="sig-name descname"><span class="pre">save_vocabulary</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save_directory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filename_prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.Tuple" title="typing.Tuple"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.save_vocabulary" title="Permalink to this definition">¶</a></dt>
<dd><p>Save only the vocabulary of the tokenizer (vocabulary + added tokens).</p>
<p>This method won’t save the configuration and special token mappings of the tokenizer. Use
<code class="xref py py-meth docutils literal notranslate"><span class="pre">_save_pretrained()</span></code> to save the whole state of the tokenizer.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>save_directory (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>):</dt><dd><p>The directory in which to save the vocabulary.</p>
</dd>
<dt>filename_prefix (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>):</dt><dd><p>An optional prefix to add to the named of the saved files.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple(str)</span></code>: Paths to the files saved.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.slow_tokenizer_class">
<span class="sig-name descname"><span class="pre">slow_tokenizer_class</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.slow_tokenizer_class" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.tokenize">
<span class="sig-name descname"><span class="pre">tokenize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pair</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_special_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a string in a sequence of tokens, replacing unknown tokens with the <code class="xref py py-obj docutils literal notranslate"><span class="pre">unk_token</span></code>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>text (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>):</dt><dd><p>The sequence to be encoded.</p>
</dd>
<dt>pair (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>):</dt><dd><p>A second sequence to be encoded with the first.</p>
</dd>
<dt>add_special_tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to add the special tokens associated with the corresponding model.</p>
</dd>
<dt>kwargs (additional keyword arguments, <cite>optional</cite>):</dt><dd><p>Will be passed to the underlying model specific encode method. See details in
<code class="xref py py-meth docutils literal notranslate"><span class="pre">__call__()</span></code></p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>: The list of tokens.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.truncate_sequences">
<span class="sig-name descname"><span class="pre">truncate_sequences</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pair_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_tokens_to_remove</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truncation_strategy</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">TruncationStrategy</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'longest_first'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.Tuple" title="typing.Tuple"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.truncate_sequences" title="Permalink to this definition">¶</a></dt>
<dd><p>Truncates a sequence pair in-place following the strategy.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>ids (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>):</dt><dd><p>Tokenized input ids of the first sequence. Can be obtained from a string by chaining the <code class="docutils literal notranslate"><span class="pre">tokenize</span></code>
and <code class="docutils literal notranslate"><span class="pre">convert_tokens_to_ids</span></code> methods.</p>
</dd>
<dt>pair_ids (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>):</dt><dd><p>Tokenized input ids of the second sequence. Can be obtained from a string by chaining the <code class="docutils literal notranslate"><span class="pre">tokenize</span></code>
and <code class="docutils literal notranslate"><span class="pre">convert_tokens_to_ids</span></code> methods.</p>
</dd>
<dt>num_tokens_to_remove (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0):</dt><dd><p>Number of tokens to remove using the truncation strategy.</p>
</dd>
<dt>truncation_strategy (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TruncationStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>The strategy to follow for truncation. Can be:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
to the maximum acceptable input length for the model if that argument is not provided. This will
truncate token by token, removing a token from the longest sequence in the pair if a pair of
sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to
the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_second'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
to the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_truncate'</span></code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</p></li>
</ul>
</dd>
<dt>stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0):</dt><dd><p>If set to a positive number, the overflowing tokens returned will contain some tokens from the main
sequence returned. The value of this argument defines the number of additional tokens.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[List[int],</span> <span class="pre">List[int],</span> <span class="pre">List[int]]</span></code>: The truncated <code class="docutils literal notranslate"><span class="pre">ids</span></code>, the truncated <code class="docutils literal notranslate"><span class="pre">pair_ids</span></code> and the
list of overflowing tokens.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.vocab_files_names">
<span class="sig-name descname"><span class="pre">vocab_files_names</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{}</span></em><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.PreTrainedTokenizerBase.vocab_files_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.Type">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.data.</span></span><span class="sig-name descname"><span class="pre">Type</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwds</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.Type" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Generic</span></code>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">CT_co</span></code>]</p>
<p>A special construct usable to annotate class objects.</p>
<p>For example, suppose we have the following classes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">User</span><span class="p">:</span> <span class="o">...</span>  <span class="c1"># Abstract base for User classes</span>
<span class="k">class</span> <span class="nc">BasicUser</span><span class="p">(</span><span class="n">User</span><span class="p">):</span> <span class="o">...</span>
<span class="k">class</span> <span class="nc">ProUser</span><span class="p">(</span><span class="n">User</span><span class="p">):</span> <span class="o">...</span>
<span class="k">class</span> <span class="nc">TeamUser</span><span class="p">(</span><span class="n">User</span><span class="p">):</span> <span class="o">...</span>
</pre></div>
</div>
<p>And a function that takes a class argument that’s a subclass of
User and returns an instance of the corresponding class:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">U</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s1">&#39;U&#39;</span><span class="p">,</span> <span class="n">bound</span><span class="o">=</span><span class="n">User</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">new_user</span><span class="p">(</span><span class="n">user_class</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">U</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">U</span><span class="p">:</span>
    <span class="n">user</span> <span class="o">=</span> <span class="n">user_class</span><span class="p">()</span>
    <span class="c1"># (Here we could write the user object to a database)</span>
    <span class="k">return</span> <span class="n">user</span>

<span class="n">joe</span> <span class="o">=</span> <span class="n">new_user</span><span class="p">(</span><span class="n">BasicUser</span><span class="p">)</span>
</pre></div>
</div>
<p>At this point the type checker knows that joe has type BasicUser.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.abstractmethod">
<span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.data.</span></span><span class="sig-name descname"><span class="pre">abstractmethod</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">funcobj</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.abstractmethod" title="Permalink to this definition">¶</a></dt>
<dd><p>A decorator indicating abstract methods.</p>
<p>Requires that the metaclass is ABCMeta or derived from it.  A
class that has a metaclass derived from ABCMeta cannot be
instantiated unless all of its abstract methods are overridden.
The abstract methods can be called using any of the normal
‘super’ call mechanisms.</p>
<p>Usage:</p>
<blockquote>
<div><dl>
<dt>class C(metaclass=ABCMeta):</dt><dd><p>&#64;abstractmethod
def my_abstract_method(self, …):</p>
<blockquote>
<div><p>…</p>
</div></blockquote>
</dd>
</dl>
</div></blockquote>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.defaultdict">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.data.</span></span><span class="sig-name descname"><span class="pre">defaultdict</span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.defaultdict" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></p>
<p>defaultdict(default_factory[, …]) –&gt; dict with default factory</p>
<p>The default factory is called without arguments to produce
a new value when a key is not present, in __getitem__ only.
A defaultdict compares equal to a dict with the same items.
All remaining arguments are treated the same as if they were
passed to the dict constructor, including keyword arguments.</p>
<dl class="py method">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.defaultdict.copy">
<span class="sig-name descname"><span class="pre">copy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">a</span> <span class="pre">shallow</span> <span class="pre">copy</span> <span class="pre">of</span> <span class="pre">D.</span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.defaultdict.copy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.defaultdict.default_factory">
<span class="sig-name descname"><span class="pre">default_factory</span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.defaultdict.default_factory" title="Permalink to this definition">¶</a></dt>
<dd><p>Factory for default value called by __missing__().</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.load_dataset">
<span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.data.</span></span><span class="sig-name descname"><span class="pre">load_dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_files</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="nlp_uncertainty_zoo.utils.task_eval.html#nlp_uncertainty_zoo.utils.task_eval.Dict" title="typing.Dict"><span class="pre">Dict</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="nlp_uncertainty_zoo.utils.samplers.html#nlp_uncertainty_zoo.utils.samplers.List" title="typing.List"><span class="pre">List</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Split</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Features</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">download_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">DownloadConfig</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">download_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">GenerateMode</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_verifications</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_in_memory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_infos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">script_version</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Version</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_auth_token</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">config_kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">DatasetDict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Dataset</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.load_dataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a dataset.</p>
<p>This method does the following under the hood:</p>
<blockquote>
<div><ol class="arabic">
<li><p>Download and import in the library the dataset loading script from <code class="docutils literal notranslate"><span class="pre">path</span></code> if it’s not already cached inside the library.</p>
<blockquote>
<div><p>Processing scripts are small python scripts that define the citation, info and format of the dataset,
contain the URL to the original data files and the code to load examples from the original data files.</p>
<p>You can find some of the scripts here: <a class="reference external" href="https://github.com/huggingface/datasets/datasets">https://github.com/huggingface/datasets/datasets</a>
and easily upload yours to share them using the CLI <code class="docutils literal notranslate"><span class="pre">huggingface-cli</span></code>.
You can find the complete list of datasets in the Datasets Hub at <a class="reference external" href="https://huggingface.co/datasets">https://huggingface.co/datasets</a></p>
</div></blockquote>
</li>
<li><p>Run the dataset loading script which will:</p>
<blockquote>
<div><ul>
<li><p>Download the dataset file from the original URL (see the script) if it’s not already downloaded and cached.</p></li>
<li><p>Process and cache the dataset in typed Arrow tables for caching.</p>
<blockquote>
<div><p>Arrow table are arbitrarily long, typed tables which can store nested objects and be mapped to numpy/pandas/python standard types.
They can be directly access from drive, loaded in RAM or even streamed over the web.</p>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
<li><p>Return a dataset built from the requested splits in <code class="docutils literal notranslate"><span class="pre">split</span></code> (default: all).</p></li>
</ol>
</div></blockquote>
<p>Args:</p>
<blockquote>
<div><p>path (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): Path to the dataset processing script with the dataset builder. Can be either:</p>
<blockquote>
<div><ul class="simple">
<li><p>a local path to processing script or the directory containing the script (if the script has the same name as the directory),
e.g. <code class="docutils literal notranslate"><span class="pre">'./dataset/squad'</span></code> or <code class="docutils literal notranslate"><span class="pre">'./dataset/squad/squad.py'</span></code>.</p></li>
<li><p>a dataset identifier in the HuggingFace Datasets Hub (list all available datasets and ids with <code class="docutils literal notranslate"><span class="pre">datasets.list_datasets()</span></code>)
e.g. <code class="docutils literal notranslate"><span class="pre">'squad'</span></code>, <code class="docutils literal notranslate"><span class="pre">'glue'</span></code> or <code class="docutils literal notranslate"><span class="pre">'openai/webtext'</span></code>.</p></li>
</ul>
</div></blockquote>
<p>name (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, optional): Defining the name of the dataset configuration.
data_files (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, optional): Defining the data_files of the dataset configuration.
data_dir (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, optional): Defining the data_dir of the dataset configuration.
split (<code class="xref py py-class docutils literal notranslate"><span class="pre">Split</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): Which split of the data to load.</p>
<blockquote>
<div><p>If None, will return a <cite>dict</cite> with all splits (typically <cite>datasets.Split.TRAIN</cite> and <cite>datasets.Split.TEST</cite>).
If given, will return a single Dataset.
Splits can be combined and specified like in tensorflow-datasets.</p>
</div></blockquote>
<p>cache_dir (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, optional): Directory to read/write data. Defaults to “~/datasets”.
features (<code class="xref py py-class docutils literal notranslate"><span class="pre">Features</span></code>, optional): Set the features type to use for this dataset.
download_config (<code class="xref py py-class docutils literal notranslate"><span class="pre">DownloadConfig</span></code>, optional): Specific download configuration parameters.
download_mode (<code class="xref py py-class docutils literal notranslate"><span class="pre">GenerateMode</span></code>, optional): Select the download/generate mode - Default to REUSE_DATASET_IF_EXISTS
ignore_verifications (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, default <code class="docutils literal notranslate"><span class="pre">False</span></code>): Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/…).
keep_in_memory (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, default <code class="docutils literal notranslate"><span class="pre">None</span></code>): Whether to copy the dataset in-memory. If <cite>None</cite>, the</p>
<blockquote>
<div><p>dataset will be copied in-memory if its size is smaller than
<cite>datasets.config.MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES</cite> (default <cite>250 MiB</cite>). This behavior can be disabled by
setting <code class="docutils literal notranslate"><span class="pre">datasets.config.MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES</span> <span class="pre">=</span> <span class="pre">None</span></code>, and in this case the dataset is not
loaded in memory.</p>
</div></blockquote>
<p>save_infos (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, default <code class="docutils literal notranslate"><span class="pre">False</span></code>): Save the dataset information (checksums/size/splits/…).
script_version (<code class="xref py py-class docutils literal notranslate"><span class="pre">Version</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, optional): Version of the dataset script to load:</p>
<blockquote>
<div><ul class="simple">
<li><p>For canonical datasets in the <cite>huggingface/datasets</cite> library like “squad”, the default version of the module is the local version fo the lib.
You can specify a different version from your local version of the lib (e.g. “master” or “1.2.0”) but it might cause compatibility issues.</p></li>
<li><p>For community provided datasets like “lhoestq/squad” that have their own git repository on the Datasets Hub, the default version “main” corresponds to the “main” branch.
You can specify a different version that the default “main” by using a commit sha or a git tag of the dataset repository.</p></li>
</ul>
</div></blockquote>
<dl class="simple">
<dt>use_auth_token (<code class="docutils literal notranslate"><span class="pre">str</span></code> or <code class="docutils literal notranslate"><span class="pre">bool</span></code>, optional): Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.</dt><dd><p>If True, will get token from <cite>“~/.huggingface”</cite>.</p>
</dd>
</dl>
<p><a href="#id12"><span class="problematic" id="id13">**</span></a>config_kwargs: Keyword arguments to be passed to the <code class="xref py py-class docutils literal notranslate"><span class="pre">BuilderConfig</span></code> and used in the <a class="reference internal" href="#nlp_uncertainty_zoo.utils.data.DatasetBuilder" title="nlp_uncertainty_zoo.utils.data.DatasetBuilder"><code class="xref py py-class docutils literal notranslate"><span class="pre">DatasetBuilder</span></code></a>.</p>
</div></blockquote>
<dl class="simple">
<dt>Returns:</dt><dd><dl class="simple">
<dt><code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">DatasetDict</span></code>:</dt><dd><p>if <cite>split</cite> is not None: the dataset requested,
if <cite>split</cite> is None, a <code class="docutils literal notranslate"><span class="pre">datasets.DatasetDict</span></code> with each split.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="nlp_uncertainty_zoo.utils.data.reduce">
<span class="sig-prename descclassname"><span class="pre">nlp_uncertainty_zoo.utils.data.</span></span><span class="sig-name descname"><span class="pre">reduce</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">function</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sequence</span></span></em><span class="optional">[</span>, <em class="sig-param"><span class="n"><span class="pre">initial</span></span></em><span class="optional">]</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">value</span></span></span><a class="headerlink" href="#nlp_uncertainty_zoo.utils.data.reduce" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply a function of two arguments cumulatively to the items of a sequence,
from left to right, so as to reduce the sequence to a single value.
For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates
((((1+2)+3)+4)+5).  If initial is present, it is placed before the items
of the sequence in the calculation, and serves as a default when the
sequence is empty.</p>
</dd></dl>

</section>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright 2022, Dennis Ulmer.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 5.3.0.<br/>
    </p>
  </div>
</footer>
  </body>
</html>