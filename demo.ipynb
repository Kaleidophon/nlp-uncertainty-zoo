{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0b06f40",
   "metadata": {},
   "source": [
    "# ü§ñüí¨‚ùìnlp-uncertainty-zoo Demo\n",
    "\n",
    "This is a quick demo for the nlp-uncertainty-zoo, detailing how to jump in quickly with package. We will do this by training two different models on the Rotten Tomatoes sentiment analysis dataset, where want to classify where a movie review is positive or negative. \n",
    "\n",
    "For that purpose, we first start by importing all necessary packages as well as loading and preprocessing the dataset. Even though the first model we are using is LSTM-based, we will still use the BERT tokenizer here for the sake of simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971bc512",
   "metadata": {},
   "source": [
    "## Loading the dataset & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3bcf1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from string import ascii_lowercase\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from nlp_uncertainty_zoo.models import LSTMEnsemble, VariationalBert  # We will test these two models in this demo!\n",
    "\n",
    "# CONST\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6ae10d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset rotten_tomatoes_movie_review (/Users/deul/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/9c411f7ecd9f3045389de0d9ce984061a1056507703d2e3183b1ac1a90816e4d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7645d62961ed42af9789825a243b55fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_with(tokenizer):\n",
    "    def preprocess(input_):\n",
    "        return tokenizer(\n",
    "            input_[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=50\n",
    "        )\n",
    "    \n",
    "    return preprocess\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_set = dataset[\"train\"].map(preprocess_with(tokenizer), batched=True)\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_set = dataset[\"train\"].map(preprocess_with(tokenizer), batched=True)\n",
    "test_loader = DataLoader(train_set, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad3fdaf",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d52c7f",
   "metadata": {},
   "source": [
    "We now start by training an ensemble of LSTMs. Due to fact that all members of an ensemble are randomly initialized, models tend to converge to different solutions, making the ensemble very robust to unseen data points (see paper TODO). This is also a very useful property for uncertainty quantification, as we will see later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "259f2cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbbcef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Training code for LSTM Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ffd8fa",
   "metadata": {},
   "source": [
    "Next up, we will fine-tune a BERT model. For uncertainty quantification, we will use Monte Carlo Dropout (TODO: Citations): By using multiple different dropout masks during inference, we can create different predictions for the same data point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "068ede00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Training code for Variational BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805e664c",
   "metadata": {},
   "source": [
    "## Evaluating task performance & calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ce5006",
   "metadata": {},
   "source": [
    "Before we continue, let us first evaluate the models to reassure ourselves that the training was successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b21de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c0c40e",
   "metadata": {},
   "source": [
    "We can also evaluate to what extend the probability of a predicted class actually corresponds to the chance of the model actually predicting the correct class, also called *calibration* (Guo et al., 2017). One way to evaluate this propery is the expected calibration error (ECE): By binning predictions with similar confidence scores, we can evaluate if the mean confidence per bin corresponds to the accuracy on the binned samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acaa4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement calibration with ECE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91320321",
   "metadata": {},
   "source": [
    "Another approach is evaluation using *prediction sets* (TODO: Citation). The idea here is to sort predictings descendingly and add classes to a set until a certain amount of probability mass - for instance 90 % in the example below - is reached. If the model is well calibrated, these prediction sets should be small and contain the correct class (on average). Using the functions implemented in the package, we evaluate these properties below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b49bb6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement prediction set evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f634a0",
   "metadata": {},
   "source": [
    "## Uncertainty quantification\n",
    "\n",
    "Next, we want to use the model to actually quantify their uncertainty in a prediction. For this purpose, we manually define some sequences which should seem suspicious to the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b2f1bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\n"
     ]
    }
   ],
   "source": [
    "original_sentence = train_set[1][\"text\"]\n",
    "print(original_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "907d2416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of vision expanded of lord of . \" huge a . the tolkien's is words describe peter so middle-earth cannot \" the of gorgeously column adequately r co-writer/director j rings . r that continuation trilogy . the jackson's elaborate\n",
      "the gorgeously elaborate continuationpn of \" the lsord ofthe rings \" trilogy is sohuge that a column of words cannort adequaely describek co-riter/dzijrector pteorn ackson's expanded vsion of jm . r .r  tolkien's middleearth .\n"
     ]
    }
   ],
   "source": [
    "# The model hasn't been finetuned on German, so this should be weird\n",
    "sentence1 = (\n",
    "    \"Die umwerfend aufwendige Fortsetzung der ‚ÄûDer Herr der Ringe‚Äú-Trilogie ist so umfangreich,\"\n",
    "    \"dass eine Kolonne von Worten die erweiterte Vision von Co-Autor/Regisseur Peter Jackson \"\n",
    "    \"von j. r . r . Tolkiens Mittelerde nicht angemessen beschreiben kann.\"\n",
    ").lower()\n",
    "# Now we scramble the contents of the sentence randomly\n",
    "tokens = original_sentence.split(\" \")\n",
    "sentence2 = \" \".join(random.sample(tokens, len(tokens)))\n",
    "print(sentence2)\n",
    "\n",
    "# Add noise to the sentence\n",
    "delete_chars = 10\n",
    "add_noise_chars = 10\n",
    "\n",
    "sentence3 = str(original_sentence)\n",
    "\n",
    "for _ in range(delete_chars):\n",
    "    idx = random.choice(range(len(sentence3)))\n",
    "    sentence3 = sentence3[:idx] + sentence3[idx + 1:]\n",
    "    \n",
    "for _ in range(add_noise_chars):\n",
    "    idx = random.choice(range(len(sentence3)))\n",
    "    char = random.choice(ascii_lowercase)\n",
    "    \n",
    "    sentence3 = sentence3[:idx] + char + sentence3[idx:]\n",
    "    \n",
    "print(sentence3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f512a12",
   "metadata": {},
   "source": [
    "We first check the predictions for the sentence above. The original sentence had a positive sentiment, so we first whether our model come to the same conclusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f2d19730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f07ec96",
   "metadata": {},
   "source": [
    "Since the sentences are very different from the training sentences, we now measure the uncertainty. Since the inputs above are pretty different from the inputs the models were trained on, we would hope the models to be more uncertain on the noisy sentences. \n",
    "\n",
    "In this demo, we will explore three different uncertainty matrix: Maximum softmax probability, predictive entropy, and mutual information. Depending on the model, there might be different metrics available. You can check that by inspecting the ``available_uncertainty_metrics`` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c60ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement functionality and use here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426e8b3e",
   "metadata": {},
   "source": [
    "But back to metrics here. An easy and intuitive metric is the maximum softmax probability (TODO: Citation)\n",
    "\n",
    "$$1 - \\max_k p_{\\theta}(y=k|x)$$\n",
    "\n",
    "Intuitively, when the model is uncertain, the distribution over classes should be uniform, thus yielding a low maximum probability over classes. We substract the value from 1 here in order to have small values correspond to high certainty. \n",
    "\n",
    "Another way to measure uncertainty is to use the Shannon entropy of the predictive distribution: For a uniform distribution, the entropy will be maximal:\n",
    "\n",
    "$$-\\sum_{k=1}^K p_{\\theta}(y=k|x) \\log p_{\\theta}(y=k|x)$$\n",
    "\n",
    "Lastly, Smith & Gal (2017) propose mutual information as a way to exlusively measure the *model uncertainty*:\n",
    "\n",
    "$$\\text{H}\\bigg[\\mathbb{E}_{q(\\theta)}\\Big[p_{\\theta}(y|x)\\Big]\\bigg] - \\mathbb{E}_{q(\\theta)}\\bigg[\\text{H}\\Big[p_{\\theta}(y|x)\\Big]\\bigg]$$\n",
    "\n",
    "Here, the first term denotes the total uncertainty, from which the second term, the *data uncertainty*, is subtracted, leaving only the model uncertainty. Usually, the expectation in both terms would over the weight posterior $p(\\theta|\\mathcal{D})$ of the model, which is generally intractable to evaluate for neural networks, which is why we model an approximate posterior $q(\\theta)$ instead. To evaluate this expectation, we use monte carlo sampling, by simply averaging the predictions coming from different sets of weights - in the case of the LSTM ensemble, these come from different ensemble members, for the Variational BERT, this corresponds to predictions using different dropout masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "156aefeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Demonstrate usage of uncertainty metrics, measure uncertainty on noisy sentences compared to original one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31e8d90",
   "metadata": {},
   "source": [
    "## Evaluating the quality of uncertainty estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfce6eaa",
   "metadata": {},
   "source": [
    "As we have done before with the raw probalities, we also want to know how reliable the uncertainty estimates for our models are. The package also provides several ways to do this: Firstly, we can evaluate them using an OOD detection task - the model should be more uncertain on data points that are unlike the ones in the training set. By using the uncertainty scores, we can use binary classification metrics like the area under the precision-recall curve (AUPR) and the area under the receiver-operator characteristic (AUROC) to evaluate this. In our Rotten tomatoes example, we will add noise to the sentences in our test set and use these sentences as an OOD data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba637cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d213ef",
   "metadata": {},
   "source": [
    "The other way introduced by Ulmer et al. (2022) is to measure how much high uncertainty corresponds to the model making wrong predictions. This is quantified by collecting the model loss and uncertainty for all points in the test set, and measuring their correlation using the [Kendall's $\\tau$ correlation coefficient](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient). The values range from -1 to 1, which 1 indicating that high uncertainty perfectly correlates with high model loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb64b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate using Kendal's tau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96041512",
   "metadata": {},
   "source": [
    "## Visualizing sentence representations\n",
    "\n",
    "Part of the interface of the model implementations also allows us to create representation of input sequences and to visualize the latent space of the models. Below we visualize the representations for the original and corrupted sentences below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b96cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement functions to extract representations and visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b05b61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot representations for Variational BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff28d006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot representations for LSTM Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498c53a4",
   "metadata": {},
   "source": [
    "Thanks for reading through this demo! We only showcase the most useful functionalities here that people might want to use when applying the implemented models. If you would like to know more about the different models and functionalities in the package, consult [the documentation](http://dennisulmer.eu/nlp-uncertainty-zoo/). If you find any bugs or have requests for missing features, please [open an issue on the Github repository](https://github.com/Kaleidophon/nlp-uncertainty-zoo/issues). Below you can find the papers that were referenced in this demo:\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88ff06c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
